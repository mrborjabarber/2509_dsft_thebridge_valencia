{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "### Ejemplos de PySpark ML (Machine Learning)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "bbca5a5c-0e53-412d-b818-ea7d26f6dc9d",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### INICIAMOS SESIÓN DE SPARK",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4d0fd368-844b-4fde-8f4f-f3e635bd52b2",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Importamos SparkSession, que es el punto de entrada para trabajar con Spark\nfrom pyspark.sql import SparkSession\n\n# Creamos una sesión de Spark con el nombre 'dataframe'\n# getOrCreate() crea una nueva sesión o devuelve una existente si ya hay una\nspark = SparkSession.builder.appName('dataframe').getOrCreate()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5f770538-16f9-4c93-aaac-26ea1370d077",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Listamos los archivos disponibles en el directorio de FileStore de Databricks\n# Esta es una ubicación donde puedes subir tus propios archivos\ndisplay(dbutils.fs.ls(\"dbfs:/FileStore/tables/\"))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "026131a4-1a02-4a3c-b1e9-6b677969824b",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cargamos el archivo CSV de entrenamiento\n# inferSchema=True permite a Spark detectar automáticamente los tipos de datos\n# header=True indica que la primera fila contiene los nombres de las columnas\ntraining = spark.read.csv(\"dbfs:/FileStore/tables/test1.csv\", inferSchema=True, header=True)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "656c3d96-5f19-478b-95be-0dd136f6a96d",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Mostramos información sobre la sesión de Spark\n# Esto nos da detalles sobre la configuración y el estado de la sesión\nspark",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c7b3aa39-3f7a-43a8-9dd0-80a9229ddd73",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "CARGAMOS EL ARCHIVO CSV",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d6eb166f-bbeb-4453-8864-373489fdfc33",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "training = spark.read.csv('data/test1.csv', header=True, inferSchema=True)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4232a1b7-46de-4c35-a75f-d88417af2cf5",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6fca4b16-40a1-496c-8844-bdd05791ab62",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "source": "# Cargamos los datos de entrenamiento desde un archivo CSV local\n# Este es un método alternativo cuando trabajas en un entorno local (no Databricks)\ntraining = spark.read.csv('data/test1.csv', header=True, inferSchema=True)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "895876c4-c9ad-4648-8160-7b03ae3367e2",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Mostramos el contenido del DataFrame de entrenamiento\n# Podemos ver que tenemos datos sobre nombres, edad, experiencia y salario\ntraining.show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3e3c81b4-789e-4124-9417-95ace9e7ed48",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "En PySpark se trabaja de forma diferente a otros frameworks de ML. Tendremos que agrupar nuestras variables independientes de forma que queden todas en una columna y dentro de un vector, por lo que crearemos un **vector assembler** (ensamblador de vectores), de tal modo que queden así esas variables independientes:\n- [Age, Experience]\n\nLo que haremos con estas dos variables, será tratarlas como una nueva variable independiente:\n- [Age, Experience] ----> nueva_variable_independiente\n\nEsto es un requisito específico de PySpark ML para el procesamiento de características.",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "295063a6-cc59-42c4-83eb-3d7dbfaea6bd",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Importamos VectorAssembler, que es una herramienta de transformación de características\n# Esta clase nos permite combinar múltiples columnas en un único vector de características\nfrom pyspark.ml.feature import VectorAssembler",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "cf4f8e2d-5561-4b27-925d-eadb49a1f385",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Creamos un VectorAssembler que combinará nuestras variables independientes\n# inputCols: lista de columnas que queremos usar como características (age y Experience)\n# outputCol: nombre de la nueva columna que contendrá el vector de características\nfeature_assembler = VectorAssembler(inputCols=['age', 'Experience'], outputCol='Independent features')",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "306b05a1-9461-42f3-abbc-ea4ac1aa6a43",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Aplicamos la transformación al DataFrame de entrenamiento\n# transform() crea una nueva columna con el vector de características combinadas\noutput = feature_assembler.transform(training)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1d76d404-0b0d-42fe-afec-c5718a0cb449",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Veremos que se crea una nueva columna cuyos valores se corresponden a vectores (arrays) con el contenido de aquellas variables independientes que hemos agrupado. Esta columna será nuestro **input feature** o lo que solíamos definir como X (variables predictoras) en otros frameworks de Machine Learning.",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "03663ae5-414c-40c5-918f-d71c481e6001",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Mostramos el DataFrame transformado\n# Observa la nueva columna \"Independent features\" que contiene vectores [age, Experience]\noutput.show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "658780d7-2ca7-4362-85b4-7ac0f8c8311a",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Seleccionamos las columnas que nos interesan para nuestro modelo: \n- **Independent features**: nuestras variables predictoras (X)\n- **Salary**: nuestra variable objetivo (y)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d6778dad-f457-429e-9a36-e8823f09f425",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Seleccionamos solo las columnas relevantes para el modelo de ML\n# Eliminamos columnas como \"Name\" que no aportan valor predictivo\nfinalized_data = output.select('Independent features', 'Salary')\n\n# Mostramos el DataFrame final listo para entrenar el modelo\nfinalized_data.show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b23802d1-895f-4f5b-a4ab-46f00bd3fff7",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "A continuación, entrenaremos un **modelo de regresión lineal** para predecir el salario basándonos en la edad y la experiencia.",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "841861f9-2a5e-43f4-9874-6a9f392e4725",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Importamos el algoritmo de regresión lineal desde el módulo de ML de PySpark\nfrom pyspark.ml.regression import LinearRegression",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e66a1a99-f3ed-4f08-a857-2541c2662600",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Dividimos los datos en conjuntos de entrenamiento (75%) y prueba (25%)\n# randomSplit([0.75, 0.25]) divide aleatoriamente el DataFrame en dos partes\n(train, test) = finalized_data.randomSplit([0.75, 0.25])\n\n# Creamos el modelo de regresión lineal\n# featuresCol: columna que contiene las variables predictoras (X)\n# labelCol: columna que contiene la variable objetivo (y)\nregressor = LinearRegression(featuresCol='Independent features', labelCol='Salary')\n\n# Entrenamos el modelo con los datos de entrenamiento\n# fit() ajusta el modelo a los datos\nregressor = regressor.fit(train)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ba776c60-c540-4409-8be6-26fcfcbe9639",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Obtenemos los coeficientes del modelo entrenado\n# Estos representan el peso de cada variable independiente en la predicción\n# En regresión lineal: y = β₀ + β₁*x₁ + β₂*x₂\n# coefficients contiene [β₁, β₂] (los pesos para age y Experience)\nregressor.coefficients",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4bafb0ae-f467-457a-93f5-10319cb5bec9",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Obtenemos el intercepto (término independiente) del modelo\n# En regresión lineal: y = β₀ + β₁*x₁ + β₂*x₂\n# intercept es β₀ (el valor base cuando todas las variables son 0)\nregressor.intercept",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ed572138-7566-4457-957b-4527f61519ab",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Evaluamos el modelo con el conjunto de prueba\n# evaluate() genera predicciones y calcula métricas de rendimiento\nprediction = regressor.evaluate(test)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0709acdf-0676-450a-b74f-d73a653364a8",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Mostramos las predicciones del modelo\n# Compara los valores reales (Salary) con los valores predichos (prediction)\n# Esto nos permite ver qué tan bien está prediciendo el modelo\nprediction.predictions.show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5a58ce7a-c5b6-44aa-a676-26e482f8346d",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Calculamos las métricas de error del modelo\n\n# MAE (Mean Absolute Error) - Error Absoluto Medio\n# Mide la diferencia promedio absoluta entre predicciones y valores reales\n# Valores más bajos indican mejor rendimiento\n\n# MSE (Mean Squared Error) - Error Cuadrático Medio\n# Penaliza más los errores grandes al elevarlos al cuadrado\n# Valores más bajos indican mejor rendimiento\nprediction.meanAbsoluteError, prediction.meanSquaredError",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b5cbb02b-9862-4772-b0c3-04aa99de8215",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5bd1639a-cc3f-4200-9500-5ae1f84f9556",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": 0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.10",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "6-pyspark_ml_example",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 2959605645862986
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}