{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase de PySpark para Big Data\n",
    "\n",
    "## √çndice\n",
    "1. Introducci√≥n a PySpark\n",
    "2. Configuraci√≥n e Inicializaci√≥n\n",
    "3. Creaci√≥n de DataFrames\n",
    "4. Operaciones B√°sicas con DataFrames\n",
    "5. Transformaciones y Acciones\n",
    "6. SQL en PySpark\n",
    "7. An√°lisis de Datos Reales\n",
    "8. Ejercicios Pr√°cticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 1. Introducci√≥n a PySpark\n\n### ¬øQu√© es PySpark?\n\n**PySpark** es la interfaz de Python para **Apache Spark**, uno de los frameworks de procesamiento distribuido m√°s potentes y populares para trabajar con Big Data. Apache Spark fue desarrollado originalmente en la Universidad de California, Berkeley, y actualmente es mantenido por la Apache Software Foundation.\n\n#### Caracter√≠sticas principales de PySpark:\n\n- üöÄ **Procesamiento de grandes vol√∫menes de datos**: Capacidad para manejar datasets que van desde gigabytes hasta petabytes de informaci√≥n\n- ‚ö° **Computaci√≥n distribuida y paralela**: Distribuye autom√°ticamente el procesamiento entre m√∫ltiples nodos (computadoras) en un cluster\n- üíæ **M√∫ltiples fuentes de datos**: Integraci√≥n nativa con CSV, JSON, Parquet, Avro, ORC, bases de datos SQL, NoSQL, y sistemas de almacenamiento distribuido como HDFS y S3\n- üîÑ **Operaciones en memoria (In-Memory Computing)**: Procesa datos en la RAM, lo que lo hace hasta 100x m√°s r√°pido que sistemas tradicionales como Hadoop MapReduce\n- üîó **Integraci√≥n con Python**: Aprovecha todo el ecosistema de Python (NumPy, Pandas, scikit-learn, etc.)\n\n### ¬øPor qu√© usar PySpark?\n\n#### 1. **Escalabilidad horizontal**\nPySpark puede escalar desde tu laptop local hasta clusters con cientos o miles de nodos. El mismo c√≥digo que escribes para procesar 1GB en tu m√°quina puede procesar 1TB en un cluster sin modificaciones.\n\n#### 2. **Velocidad excepcional**\n- **In-memory computing**: Los datos se mantienen en RAM entre operaciones\n- **Optimizaci√≥n autom√°tica**: El motor Catalyst optimiza las consultas autom√°ticamente\n- **Procesamiento paralelo**: Divide y procesa datos simult√°neamente en m√∫ltiples n√∫cleos/nodos\n\n#### 3. **Facilidad de uso**\n- API de alto nivel similar a Pandas\n- Soporte para SQL est√°ndar\n- Documentaci√≥n extensa y comunidad activa\n- Sintaxis intuitiva y Pythonica\n\n#### 4. **Ecosistema completo**\n- **Spark SQL**: Para consultas SQL y trabajo con datos estructurados\n- **Spark Streaming**: Para procesamiento de datos en tiempo real\n- **MLlib**: Biblioteca de Machine Learning distribuido\n- **GraphX**: Para procesamiento de grafos\n\n#### 5. **Compatibilidad empresarial**\n- Soportado por todas las plataformas de cloud (AWS, Azure, Google Cloud)\n- Integraci√≥n con Hadoop, Hive, HBase\n- Usado por empresas como Netflix, Uber, Airbnb, NASA, CERN\n\n### Casos de uso t√≠picos:\n\n- üìä **An√°lisis de datos a gran escala**: ETL (Extract, Transform, Load) de terabytes de datos\n- ü§ñ **Machine Learning distribuido**: Entrenar modelos con datasets masivos\n- üìà **Business Intelligence**: An√°lisis de m√©tricas empresariales en tiempo real\n- üîç **Procesamiento de logs**: An√°lisis de logs de servidores, aplicaciones, IoT\n- üí≥ **Detecci√≥n de fraude**: An√°lisis de transacciones en tiempo real\n- üéØ **Sistemas de recomendaci√≥n**: Procesamiento de comportamiento de usuarios\n\n### Comparaci√≥n: PySpark vs Pandas\n\n| Caracter√≠stica | Pandas | PySpark |\n|---------------|--------|----------|\n| **Tama√±o de datos** | Hasta ~10GB (RAM local) | Petabytes (distribuido) |\n| **Procesamiento** | Single-node (1 m√°quina) | Multi-node (cluster) |\n| **Velocidad** | R√°pido para datos peque√±os | √ìptimo para datos grandes |\n| **Sintaxis** | M√°s simple | Similar pero distribuida |\n| **Uso t√≠pico** | An√°lisis exploratorio local | Producci√≥n a gran escala |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Configuraci√≥n e Inicializaci√≥n\n\n### ¬øQu√© es una SparkSession?\n\nLa **SparkSession** es el punto de entrada unificado para todas las funcionalidades de Spark desde la versi√≥n 2.0. Es el objeto principal que necesitas crear para comenzar a trabajar con PySpark. \n\nAntes de Spark 2.0, exist√≠an m√∫ltiples contextos (SparkContext, SQLContext, HiveContext), pero ahora todos est√°n unificados en SparkSession, simplificando enormemente el trabajo.\n\n### Componentes de la arquitectura Spark:\n\n1. **Driver Program**: Tu programa principal que contiene la SparkSession\n2. **Cluster Manager**: Coordina los recursos (YARN, Mesos, Kubernetes, o Standalone)\n3. **Worker Nodes**: Nodos que ejecutan las tareas\n4. **Executors**: Procesos en los workers que ejecutan el c√≥digo\n\n### Instalaci√≥n de PySpark\n\nPySpark se puede instalar f√°cilmente usando pip. Incluye todo lo necesario para ejecutar Spark localmente en tu m√°quina."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de PySpark (ejecutar solo una vez)\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Importar bibliotecas necesarias\n\nAntes de comenzar, necesitamos importar las bibliotecas y m√≥dulos principales de PySpark:\n\n- **SparkSession**: Punto de entrada principal para trabajar con Spark\n- **pyspark.sql.functions**: Funciones para manipular y transformar columnas (similar a funciones de agregaci√≥n en SQL)\n- **pyspark.sql.types**: Tipos de datos para definir esquemas expl√≠citos (StringType, IntegerType, FloatType, etc.)\n\nEstas importaciones nos dan acceso a todas las herramientas necesarias para trabajar con DataFrames distribuidos."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# IMPORTACIONES B√ÅSICAS DE PYSPARK\n# ============================================\n\n# Importar SparkSession: clase principal para iniciar Spark\nfrom pyspark.sql import SparkSession\n\n# Importar funciones de columna: para manipular y transformar datos\nfrom pyspark.sql.functions import col, count, avg, sum, max, min, desc, asc\n# col() - referencia a columnas del DataFrame\n# count() - contar valores\n# avg() - calcular promedio\n# sum() - sumar valores\n# max(), min() - valores m√°ximo y m√≠nimo\n# desc(), asc() - ordenar descendente o ascendente\n\n# Importar funciones de fecha y condicionales\nfrom pyspark.sql.functions import year, month, dayofmonth, when, lit\n# year(), month(), dayofmonth() - extraer partes de fechas\n# when() - condicional (similar a IF en SQL)\n# lit() - crear columna con valor literal/constante\n\n# Importar tipos de datos para definir esquemas\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType\n# StructType - define la estructura completa del DataFrame\n# StructField - define cada campo/columna individual\n# StringType, IntegerType, FloatType, DateType - tipos de datos\n\nprint(\"‚úÖ Bibliotecas importadas correctamente\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Crear una SparkSession\n\nLa `SparkSession` es el objeto m√°s importante en PySpark. Es el punto de entrada unificado que nos permite:\n\n- Crear DataFrames\n- Leer datos de diferentes fuentes\n- Ejecutar consultas SQL\n- Configurar par√°metros de Spark\n- Acceder al SparkContext (para operaciones de bajo nivel)\n\n#### Explicaci√≥n de los par√°metros:\n\n- **`.appName(\"nombre\")`**: Nombre descriptivo de tu aplicaci√≥n (aparecer√° en la UI de Spark)\n- **`.master(\"local[*]\")`**: Modo de ejecuci√≥n\n  - `local`: Ejecuta en 1 solo hilo local\n  - `local[4]`: Ejecuta con 4 hilos locales\n  - `local[*]`: Usa todos los cores disponibles en tu CPU\n  - En producci√≥n, aqu√≠ ir√≠a la URL del cluster (ej: `spark://master:7077`)\n- **`.config()`**: Configuraciones adicionales\n  - `spark.driver.memory`: Memoria RAM asignada al driver (4g = 4 gigabytes)\n  - `spark.executor.memory`: Memoria para cada executor\n  - `spark.sql.shuffle.partitions`: N√∫mero de particiones (default: 200)\n  \n#### ¬øQu√© hace `.getOrCreate()`?\n\nEste m√©todo inteligente:\n- Crea una nueva SparkSession si no existe ninguna\n- Retorna la SparkSession existente si ya hay una activa\n- Evita crear m√∫ltiples sesiones innecesariamente"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CREAR SPARKSESSION - PUNTO DE ENTRADA A SPARK\n# ============================================\n\n# Usar el patr√≥n builder para configurar la sesi√≥n\nspark = SparkSession.builder \\\n    .appName(\"Clase PySpark Big Data\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .getOrCreate()\n# .appName() - nombre de la aplicaci√≥n (aparece en Spark UI)\n# .master(\"local[*]\") - modo de ejecuci√≥n: local usando todos los cores de CPU\n# .config() - configuraciones adicionales (aqu√≠: 4GB de RAM para el driver)\n# .getOrCreate() - crea nueva sesi√≥n o retorna la existente\n\n# Verificar que la sesi√≥n se cre√≥ correctamente\nprint(f\"‚úÖ Spark Session creada\")\n\n# Mostrar la versi√≥n de Spark instalada\nprint(f\"üìå Versi√≥n de Spark: {spark.version}\")\n\n# Mostrar el nombre de la aplicaci√≥n\nprint(f\"üìå Aplicaci√≥n: {spark.sparkContext.appName}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. Creaci√≥n de DataFrames\n\n### ¬øQu√© es un DataFrame en PySpark?\n\nUn **DataFrame** es una colecci√≥n distribuida de datos organizados en columnas con nombre. Es conceptualmente similar a una tabla en una base de datos relacional o un DataFrame de Pandas, pero con optimizaciones para procesamiento distribuido.\n\n#### Caracter√≠sticas clave de los DataFrames en PySpark:\n\n1. **Distribuidos**: Los datos se dividen autom√°ticamente en particiones que se procesan en paralelo\n2. **Inmutables**: Una vez creado, no puede modificarse (pero puedes crear nuevos DataFrames transformados)\n3. **Lazy Evaluation**: Las transformaciones no se ejecutan inmediatamente, solo cuando se llama una acci√≥n\n4. **Optimizados**: El motor Catalyst optimiza autom√°ticamente las consultas antes de ejecutarlas\n5. **Tipados**: Cada columna tiene un tipo de dato espec√≠fico (String, Integer, Float, etc.)\n\n#### Ventajas sobre RDDs (Resilient Distributed Datasets):\n\n- API de m√°s alto nivel y m√°s f√°cil de usar\n- Optimizaci√≥n autom√°tica de consultas\n- Mejor rendimiento (hasta 10x m√°s r√°pido)\n- Interoperabilidad con SQL\n- Soporte para m√∫ltiples lenguajes (Python, Scala, Java, R)\n\n### Formas de crear DataFrames:\n\n1. Desde listas o tuplas de Python\n2. Desde DataFrames de Pandas\n3. Leyendo archivos (CSV, JSON, Parquet, etc.)\n4. Desde bases de datos (JDBC)\n5. Desde RDDs existentes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.1 Crear DataFrame desde una lista de Python\n\nEsta es la forma m√°s simple de crear un DataFrame, ideal para:\n- Prototipos y pruebas\n- Ejemplos educativos\n- Datasets peque√±os generados din√°micamente\n\n**Proceso:**\n1. Definir los datos como lista de tuplas (cada tupla es una fila)\n2. Definir los nombres de las columnas\n3. Usar `spark.createDataFrame()` para crear el DataFrame\n\n**Nota importante**: En producci√≥n, generalmente cargar√°s datos desde archivos o bases de datos, no desde listas en memoria."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CREAR DATAFRAME DESDE LISTA DE PYTHON\n# ============================================\n\n# Paso 1: Definir los datos como lista de tuplas\n# Cada tupla representa una fila con los datos de un empleado\ndatos_empleados = [\n    # (nombre, apellido, edad, departamento, salario)\n    (\"Juan\", \"P√©rez\", 28, \"Ventas\", 45000),\n    (\"Mar√≠a\", \"Garc√≠a\", 35, \"IT\", 65000),\n    (\"Carlos\", \"L√≥pez\", 42, \"Finanzas\", 58000),\n    (\"Ana\", \"Mart√≠nez\", 31, \"IT\", 62000),\n    (\"Pedro\", \"S√°nchez\", 29, \"Ventas\", 47000),\n    (\"Laura\", \"Rodr√≠guez\", 38, \"Recursos Humanos\", 55000),\n    (\"Miguel\", \"Fern√°ndez\", 45, \"IT\", 72000),\n    (\"Sof√≠a\", \"Gonz√°lez\", 27, \"Marketing\", 43000),\n    (\"Diego\", \"Ruiz\", 33, \"Finanzas\", 61000),\n    (\"Elena\", \"Jim√©nez\", 40, \"Ventas\", 53000)\n]\n\n# Paso 2: Definir los nombres de las columnas\n# Esta lista debe tener el mismo orden que los valores en las tuplas\ncolumnas = [\"nombre\", \"apellido\", \"edad\", \"departamento\", \"salario\"]\n\n# Paso 3: Crear el DataFrame usando spark.createDataFrame()\n# Argumentos: datos (lista de tuplas), esquema (lista de nombres de columnas)\ndf_empleados = spark.createDataFrame(datos_empleados, columnas)\n\n# Confirmar que el DataFrame se cre√≥ exitosamente\nprint(\"‚úÖ DataFrame creado exitosamente\")\n\n# Mostrar el contenido del DataFrame\n# .show() es una ACCI√ìN que ejecuta y muestra los resultados\ndf_empleados.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.2 Crear DataFrame con esquema expl√≠cito\n\n#### ¬øPor qu√© definir un esquema expl√≠cito?\n\nCuando creamos DataFrames, PySpark intenta inferir autom√°ticamente los tipos de datos. Sin embargo, definir un esquema expl√≠cito tiene ventajas importantes:\n\n**Ventajas:**\n1. **Rendimiento mejorado**: No necesita escanear los datos para inferir tipos\n2. **Seguridad de tipos**: Garantiza que los datos tengan el formato correcto\n3. **Claridad del c√≥digo**: Documenta la estructura esperada de los datos\n4. **Validaci√≥n autom√°tica**: Rechaza datos que no cumplan con el esquema\n5. **Esencial para producci√≥n**: En ambientes empresariales es una mejor pr√°ctica\n\n#### Componentes del esquema:\n\n- **StructType**: Define la estructura completa del DataFrame (contenedor de campos)\n- **StructField**: Define cada columna individual con:\n  - `nombre`: Nombre de la columna\n  - `tipo`: Tipo de dato (StringType, IntegerType, FloatType, DateType, etc.)\n  - `nullable`: Si la columna puede contener valores NULL (True/False)\n\n#### Tipos de datos disponibles:\n\n- **StringType**: Cadenas de texto\n- **IntegerType**: N√∫meros enteros\n- **LongType**: N√∫meros enteros largos\n- **FloatType**: N√∫meros decimales de precisi√≥n simple\n- **DoubleType**: N√∫meros decimales de doble precisi√≥n\n- **BooleanType**: Valores booleanos (True/False)\n- **DateType**: Fechas (sin hora)\n- **TimestampType**: Fechas con hora\n- **ArrayType**: Arrays/listas\n- **MapType**: Diccionarios/mapas\n- **StructType**: Estructuras anidadas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CREAR DATAFRAME CON ESQUEMA EXPL√çCITO\n# ============================================\n\n# Definir la estructura del DataFrame de forma expl√≠cita\n# Esto es m√°s eficiente y seguro que dejar que Spark infiera los tipos\n\n# StructType: contenedor que define la estructura completa\nesquema = StructType([\n    # StructField(nombre_columna, tipo_dato, nullable)\n    # nullable=True significa que la columna puede contener valores NULL\n    StructField(\"nombre\", StringType(), True),          # Columna de texto\n    StructField(\"apellido\", StringType(), True),        # Columna de texto\n    StructField(\"edad\", IntegerType(), True),           # Columna num√©rica entera\n    StructField(\"departamento\", StringType(), True),    # Columna de texto\n    StructField(\"salario\", IntegerType(), True)         # Columna num√©rica entera\n])\n\n# Crear DataFrame usando los datos anteriores pero con esquema expl√≠cito\n# Esto valida que los datos coincidan con los tipos definidos\ndf_empleados_schema = spark.createDataFrame(datos_empleados, esquema)\n\n# Mostrar el esquema del DataFrame\nprint(\"üìã Esquema del DataFrame:\")\n# .printSchema() muestra la estructura en formato de √°rbol jer√°rquico\ndf_empleados_schema.printSchema()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Operaciones B√°sicas con DataFrames\n\nEn esta secci√≥n aprenderemos las operaciones fundamentales para explorar y entender nuestros datos. Estas operaciones son el primer paso en cualquier an√°lisis de datos y te permiten:\n\n- Entender la estructura de tus datos\n- Identificar problemas de calidad\n- Obtener estad√≠sticas descriptivas\n- Validar que los datos se cargaron correctamente\n\n**Concepto importante - Acciones vs Transformaciones:**\n\nLas operaciones que veremos aqu√≠ son principalmente **Acciones**, lo que significa que:\n- Se ejecutan inmediatamente (no son lazy)\n- Devuelven resultados al driver\n- Pueden ser costosas con datasets grandes\n- √ötiles para exploraci√≥n interactiva\n\n### 4.1 Exploraci√≥n de datos\n\nLa exploraci√≥n de datos es el primer paso esencial en cualquier proyecto de an√°lisis. Nos permite entender qu√© contienen nuestros datos antes de comenzar transformaciones complejas."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXPLORACI√ìN: MOSTRAR PRIMERAS FILAS\n# ============================================\n\nprint(\"üìä Primeras 5 filas:\")\n\n# .show(n) - Muestra las primeras n filas del DataFrame\n# Es una ACCI√ìN, por lo que ejecuta todas las transformaciones pendientes\n# Por defecto muestra 20 filas si no se especifica el n√∫mero\ndf_empleados.show(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXPLORACI√ìN: VER EL ESQUEMA\n# ============================================\n\nprint(\"\\nüìã Esquema del DataFrame:\")\n\n# .printSchema() - Muestra la estructura del DataFrame\n# Imprime: nombre de columnas, tipo de datos, y si admite NULL\n# Formato de √°rbol jer√°rquico f√°cil de leer\ndf_empleados.printSchema()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXPLORACI√ìN: DIMENSIONES DEL DATAFRAME\n# ============================================\n\n# .count() - ACCI√ìN que cuenta el n√∫mero total de filas\n# Puede ser costosa en datasets grandes\nnum_filas = df_empleados.count()\n\n# .columns - Atributo que retorna una lista con los nombres de columnas\n# len() calcula la cantidad de elementos en la lista\nnum_columnas = len(df_empleados.columns)\n\n# Mostrar las dimensiones del DataFrame\nprint(f\"\\nüìè Dimensiones del DataFrame:\")\nprint(f\"   - Filas: {num_filas}\")\nprint(f\"   - Columnas: {num_columnas}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXPLORACI√ìN: NOMBRES DE COLUMNAS\n# ============================================\n\nprint(\"\\nüìë Columnas del DataFrame:\")\n\n# .columns - Retorna una lista de Python con los nombres de todas las columnas\n# √ötil para verificar qu√© columnas est√°n disponibles\nprint(df_empleados.columns)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EXPLORACI√ìN: ESTAD√çSTICAS DESCRIPTIVAS\n# ============================================\n\nprint(\"\\nüìà Estad√≠sticas descriptivas:\")\n\n# .describe() - Calcula estad√≠sticas b√°sicas de columnas num√©ricas:\n#   - count: n√∫mero de valores no nulos\n#   - mean: promedio\n#   - stddev: desviaci√≥n est√°ndar\n#   - min: valor m√≠nimo\n#   - max: valor m√°ximo\n# Retorna un DataFrame, por eso necesitamos .show() para visualizar\ndf_empleados.describe().show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Selecci√≥n de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SELECCI√ìN: COLUMNAS ESPEC√çFICAS (M√âTODO 1)\n# ============================================\n\nprint(\"üéØ Selecci√≥n de columnas (nombre y departamento):\")\n\n# .select() - TRANSFORMACI√ìN que selecciona columnas espec√≠ficas\n# Pasa los nombres de columnas como strings separados por comas\n# Retorna un nuevo DataFrame con solo las columnas seleccionadas\ndf_empleados.select(\"nombre\", \"departamento\").show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SELECCI√ìN: USANDO LA FUNCI√ìN col() (M√âTODO 2)\n# ============================================\n\nprint(\"\\nüéØ Selecci√≥n usando col():\")\n\n# col() - Funci√≥n que crea una referencia a una columna\n# Permite operaciones m√°s avanzadas sobre columnas\n# Es necesario cuando quieres hacer operaciones (ej: col(\"salario\") * 1.1)\ndf_empleados.select(col(\"nombre\"), col(\"salario\")).show(5)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Filtrado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# FILTRADO: CONDICI√ìN SIMPLE\n# ============================================\n\nprint(\"üîç Empleados del departamento IT:\")\n\n# .filter() - TRANSFORMACI√ìN que filtra filas seg√∫n una condici√≥n\n# col(\"departamento\") == \"IT\" - condici√≥n booleana\n# Solo retorna filas donde la condici√≥n es verdadera\n# Tambi√©n se puede usar .where() que es equivalente\ndf_empleados.filter(col(\"departamento\") == \"IT\").show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# FILTRADO: CONDICI√ìN NUM√âRICA\n# ============================================\n\nprint(\"\\nüí∞ Empleados con salario > 60,000:\")\n\n# Filtrar usando operador de comparaci√≥n (>)\n# col(\"salario\") > 60000 - condici√≥n que compara valores num√©ricos\n# Retorna solo empleados cuyo salario sea mayor a 60000\ndf_empleados.filter(col(\"salario\") > 60000).show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# FILTRADO: M√öLTIPLES CONDICIONES CON AND\n# ============================================\n\nprint(\"\\nüéØ Empleados de IT con salario > 60,000:\")\n\n# Combinar m√∫ltiples condiciones con & (AND l√≥gico)\n# IMPORTANTE: Usar & para AND (no \"and\" de Python)\n# Cada condici√≥n debe ir entre par√©ntesis\n# Ambas condiciones deben ser verdaderas para incluir la fila\ndf_empleados.filter(\n    (col(\"departamento\") == \"IT\") & (col(\"salario\") > 60000)\n).show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# FILTRADO: M√öLTIPLES CONDICIONES CON OR\n# ============================================\n\nprint(\"\\nüéØ Empleados de IT o Finanzas:\")\n\n# Combinar condiciones con | (OR l√≥gico)\n# IMPORTANTE: Usar | para OR (no \"or\" de Python)\n# Cada condici√≥n debe ir entre par√©ntesis\n# Al menos una condici√≥n debe ser verdadera para incluir la fila\ndf_empleados.filter(\n    (col(\"departamento\") == \"IT\") | (col(\"departamento\") == \"Finanzas\")\n).show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. Transformaciones y Acciones\n\nEste es uno de los conceptos m√°s importantes en PySpark. Entender la diferencia entre transformaciones y acciones es crucial para escribir c√≥digo eficiente.\n\n### Conceptos fundamentales:\n\n#### ‚ö° Transformaciones (Lazy Operations)\n\nLas **transformaciones** son operaciones que definen un nuevo DataFrame pero **NO se ejecutan inmediatamente**. Son \"perezosas\" (lazy).\n\n**Caracter√≠sticas:**\n- No se ejecutan hasta que se llama una acci√≥n\n- Retornan un nuevo DataFrame (los DataFrames son inmutables)\n- Se pueden encadenar m√∫ltiples transformaciones\n- Spark las optimiza antes de ejecutar\n\n**Ejemplos de transformaciones:**\n- `select()`: Seleccionar columnas\n- `filter()` / `where()`: Filtrar filas\n- `groupBy()`: Agrupar datos\n- `orderBy()` / `sort()`: Ordenar datos\n- `join()`: Unir DataFrames\n- `withColumn()`: Agregar/modificar columnas\n- `drop()`: Eliminar columnas\n- `distinct()`: Eliminar duplicados\n\n#### üéØ Acciones (Eager Operations)\n\nLas **acciones** son operaciones que **ejecutan el c√°lculo** y devuelven resultados al driver o escriben datos.\n\n**Caracter√≠sticas:**\n- Se ejecutan inmediatamente\n- Disparan la ejecuci√≥n de todas las transformaciones pendientes\n- Devuelven valores a Python (no DataFrames)\n- Pueden ser costosas en tiempo y memoria\n\n**Ejemplos de acciones:**\n- `show()`: Mostrar datos en pantalla\n- `count()`: Contar filas\n- `collect()`: Traer todos los datos al driver (¬°CUIDADO!)\n- `take(n)`: Traer n filas al driver\n- `first()`: Obtener primera fila\n- `write()`: Escribir a disco/base de datos\n\n### ¬øPor qu√© Lazy Evaluation?\n\nLa evaluaci√≥n perezosa permite a Spark:\n1. **Optimizar el plan de ejecuci√≥n completo**: Ve todas las transformaciones antes de ejecutar\n2. **Eliminar operaciones innecesarias**: Si solo necesitas 10 filas, no procesa todo\n3. **Reordenar operaciones**: Para minimizar movimiento de datos\n4. **Procesar en paralelo eficientemente**: Divide el trabajo de forma √≥ptima\n\n### Ejemplo del flujo:\n\n```python\n# Estas son transformaciones (NO se ejecutan todav√≠a)\ndf2 = df.filter(col(\"edad\") > 30)\ndf3 = df2.select(\"nombre\", \"salario\")\ndf4 = df3.orderBy(\"salario\")\n\n# Esta es una acci√≥n (AHORA se ejecutan todas las transformaciones)\ndf4.show()\n```\n\nSpark construye un **DAG (Directed Acyclic Graph)** con todas las transformaciones y lo optimiza antes de ejecutar."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# TRANSFORMACI√ìN: AGREGAR NUEVA COLUMNA\n# ============================================\n\nprint(\"‚ûï Agregar columna 'salario_anual_bonus':\")\n\n# .withColumn() - TRANSFORMACI√ìN que agrega o modifica una columna\n# Par√°metros: nombre_nueva_columna, expresi√≥n_de_c√°lculo\n# Si la columna existe, la reemplaza; si no existe, la crea\n# col(\"salario\") * 1.10 - calcula el salario con 10% de aumento\ndf_con_bonus = df_empleados.withColumn(\n    \"salario_con_bonus\",      # Nombre de la nueva columna\n    col(\"salario\") * 1.10     # Expresi√≥n: salario original * 1.10\n)\n\n# Mostrar el DataFrame con la nueva columna\ndf_con_bonus.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# TRANSFORMACI√ìN: RENOMBRAR COLUMNA\n# ============================================\n\nprint(\"\\n‚úèÔ∏è Renombrar columna:\")\n\n# .withColumnRenamed() - TRANSFORMACI√ìN que renombra una columna\n# Par√°metros: nombre_actual, nombre_nuevo\n# La columna mantiene sus datos, solo cambia el nombre\ndf_renombrado = df_empleados.withColumnRenamed(\"salario\", \"salario_anual\")\n# Renombra \"salario\" a \"salario_anual\"\n\n# Mostrar solo las primeras 5 filas del DataFrame renombrado\ndf_renombrado.show(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# TRANSFORMACI√ìN: ELIMINAR COLUMNA\n# ============================================\n\nprint(\"\\nüóëÔ∏è Eliminar columna:\")\n\n# .drop() - TRANSFORMACI√ìN que elimina una o m√°s columnas\n# Pasa el nombre de la(s) columna(s) a eliminar como string\n# Retorna un nuevo DataFrame sin esas columnas\ndf_sin_apellido = df_empleados.drop(\"apellido\")\n# Elimina la columna \"apellido\"\n\n# Mostrar el DataFrame sin la columna apellido\ndf_sin_apellido.show(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# TRANSFORMACI√ìN: ORDENAR DATOS\n# ============================================\n\nprint(\"\\nüìä Ordenar por salario (descendente):\")\n\n# .orderBy() - TRANSFORMACI√ìN que ordena el DataFrame\n# col(\"salario\").desc() - ordena por salario de mayor a menor (descendente)\n# Para orden ascendente usar .asc() o no especificar nada\n# Tambi√©n se puede usar .sort() que es equivalente\ndf_empleados.orderBy(col(\"salario\").desc()).show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# TRANSFORMACI√ìN: ELIMINAR DUPLICADOS\n# ============================================\n\nprint(\"\\nüîÑ Eliminar duplicados por departamento:\")\n\n# .distinct() - TRANSFORMACI√ìN que elimina filas duplicadas\n# Primero seleccionamos solo la columna \"departamento\"\n# Luego distinct() elimina valores repetidos\n# Retorna solo los valores √∫nicos\ndf_empleados.select(\"departamento\").distinct().show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 5.2 Agregaciones y GroupBy\n\nLas agregaciones son operaciones que resumen m√∫ltiples filas en un solo valor. Son fundamentales en an√°lisis de datos para obtener insights.\n\n#### GroupBy en PySpark\n\n`groupBy()` funciona similar a SQL GROUP BY:\n1. Agrupa filas que comparten valores en columnas especificadas\n2. Aplica funciones de agregaci√≥n a cada grupo\n3. Retorna un DataFrame con una fila por grupo\n\n#### Funciones de agregaci√≥n m√°s comunes:\n\n- **count()**: Cuenta el n√∫mero de filas/valores\n- **sum()**: Suma valores num√©ricos\n- **avg()** / **mean()**: Calcula el promedio\n- **max()**: Encuentra el valor m√°ximo\n- **min()**: Encuentra el valor m√≠nimo\n- **first()**: Primer valor del grupo\n- **last()**: √öltimo valor del grupo\n- **stddev()**: Desviaci√≥n est√°ndar\n- **variance()**: Varianza\n\n#### M√©todos para agregar:\n\n1. **M√©todo simple**: `df.groupBy(\"columna\").count()`\n2. **M√©todo .agg()**: Permite m√∫ltiples agregaciones simult√°neas\n   ```python\n   df.groupBy(\"categoria\").agg(\n       sum(\"ventas\").alias(\"total_ventas\"),\n       avg(\"precio\").alias(\"precio_promedio\")\n   )\n   ```\n\n#### .alias() - Renombrar columnas de resultado\n\nUsar `.alias()` es una buena pr√°ctica porque:\n- Hace los nombres de columnas m√°s descriptivos\n- Evita nombres autom√°ticos como \"sum(ventas)\"\n- Facilita referenciar las columnas despu√©s"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# AGREGACI√ìN: GROUPBY SIMPLE\n# ============================================\n\nprint(\"üë• Empleados por departamento:\")\n\n# .groupBy() - TRANSFORMACI√ìN que agrupa filas por valores comunes\n# Par√°metro: columna(s) por la cual agrupar\n# .count() - Cuenta el n√∫mero de filas en cada grupo\n# Es similar a: SELECT departamento, COUNT(*) FROM empleados GROUP BY departamento\ndf_empleados.groupBy(\"departamento\").count().show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# AGREGACI√ìN: M√öLTIPLES AGREGACIONES CON .agg()\n# ============================================\n\nprint(\"\\nüìä Estad√≠sticas por departamento:\")\n\n# .agg() - Permite aplicar m√∫ltiples funciones de agregaci√≥n simult√°neamente\n# Despu√©s de agrupar, calculamos varias m√©tricas a la vez:\ndf_empleados.groupBy(\"departamento\").agg(\n    count(\"nombre\").alias(\"num_empleados\"),      # Contar empleados\n    avg(\"salario\").alias(\"salario_promedio\"),    # Calcular promedio de salario\n    max(\"salario\").alias(\"salario_maximo\"),      # Encontrar salario m√°s alto\n    min(\"salario\").alias(\"salario_minimo\")       # Encontrar salario m√°s bajo\n).orderBy(col(\"salario_promedio\").desc()).show()\n# .alias() - Renombra la columna resultado para que sea m√°s descriptiva\n# .orderBy() - Ordena el resultado por salario promedio (descendente)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# AGREGACI√ìN: SUMA POR GRUPO\n# ============================================\n\nprint(\"\\nüí∞ Suma total de salarios por departamento:\")\n\n# Agrupar por departamento y sumar todos los salarios\n# Esto calcula la \"masa salarial\" o costo total en salarios por departamento\ndf_empleados.groupBy(\"departamento\").agg(\n    sum(\"salario\").alias(\"total_salarios\")       # Sumar todos los salarios\n).orderBy(col(\"total_salarios\").desc()).show()   # Ordenar de mayor a menor"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Acciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# ACCI√ìN: CONTAR FILAS\n# ============================================\n\n# .count() - ACCI√ìN que cuenta el n√∫mero total de filas\n# Dispara la ejecuci√≥n de todas las transformaciones pendientes\n# Retorna un n√∫mero entero (no un DataFrame)\ntotal_empleados = df_empleados.count()\n\nprint(f\"üìä Total de empleados: {total_empleados}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# ACCI√ìN: COLLECT - TRAER DATOS AL DRIVER\n# ============================================\n\n# .collect() - ACCI√ìN que trae TODOS los datos al driver (tu m√°quina)\n# ‚ö†Ô∏è PELIGROSO con datasets grandes - puede llenar la memoria\n# Retorna una lista de objetos Row de Python\n# Cada Row se comporta como un diccionario\ndatos = df_empleados.select(\"nombre\", \"departamento\").collect()\n\nprint(\"\\nüì• Primeros 3 registros con collect():\")\n# Iterar sobre los primeros 3 elementos\nfor row in datos[:3]:\n    # Acceder a los valores usando notaci√≥n de diccionario\n    print(f\"   {row['nombre']} - {row['departamento']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# ACCI√ìN: OBTENER PRIMERA FILA\n# ============================================\n\n# .first() - ACCI√ìN que retorna solo la primera fila del DataFrame\n# Retorna un objeto Row (similar a una fila de una tabla)\n# M√°s eficiente que .collect() cuando solo necesitas una fila\nprimera_fila = df_empleados.first()\n\nprint(f\"\\nü•á Primera fila: {primera_fila}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# ACCI√ìN: TOMAR N FILAS\n# ============================================\n\n# .take(n) - ACCI√ìN que retorna las primeras n filas\n# Similar a .collect() pero limitado a n filas\n# M√°s seguro que collect() porque controlas cu√°ntas filas traes\n# Retorna una lista de objetos Row\nprimeras_tres = df_empleados.take(3)\n\nprint(\"\\nüìã Primeras 3 filas con take():\")\nfor row in primeras_tres:\n    print(f\"   {row}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. SQL en PySpark\n\nUna de las caracter√≠sticas m√°s poderosas de PySpark es la capacidad de ejecutar **consultas SQL est√°ndar** directamente sobre DataFrames. Esto es especialmente √∫til para:\n\n- Analistas que ya conocen SQL\n- Consultas complejas que son m√°s claras en SQL\n- Migraci√≥n de c√≥digo SQL existente\n- Colaboraci√≥n con equipos que prefieren SQL\n\n### Ventajas de usar SQL en PySpark:\n\n1. **Sintaxis familiar**: Si conoces SQL, ya sabes c√≥mo usarlo\n2. **Legibilidad**: Consultas complejas pueden ser m√°s claras\n3. **Potencia completa**: Acceso a todas las funciones SQL est√°ndar\n4. **Mismo rendimiento**: Se compila al mismo c√≥digo que la API de DataFrames\n5. **Interoperabilidad**: Puedes mezclar SQL y API de DataFrames\n\n### ¬øC√≥mo funciona?\n\n1. Registras un DataFrame como una **vista temporal** (tabla virtual)\n2. La vista existe solo en la sesi√≥n actual (se pierde al cerrar)\n3. Ejecutas consultas SQL usando `spark.sql()`\n4. El resultado es un DataFrame que puedes seguir transformando\n\n### Tipos de vistas:\n\n- **createOrReplaceTempView()**: Vista temporal de sesi√≥n (se elimina al cerrar la sesi√≥n)\n- **createGlobalTempView()**: Vista global accesible desde todas las sesiones\n- **createOrReplaceGlobalTempView()**: Versi√≥n que reemplaza si existe\n\n**Nota**: Las vistas temporales NO se escriben a disco, solo existen en memoria durante la sesi√≥n."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Crear una vista temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SQL: REGISTRAR VISTA TEMPORAL\n# ============================================\n\n# .createOrReplaceTempView() - Registra el DataFrame como una tabla temporal SQL\n# Par√°metro: nombre de la tabla (string)\n# La \"tabla\" solo existe en memoria durante esta sesi√≥n de Spark\n# Permite ejecutar consultas SQL sobre el DataFrame\ndf_empleados.createOrReplaceTempView(\"empleados\")\n\nprint(\"‚úÖ Vista temporal 'empleados' creada\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Ejecutar consultas SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SQL: CONSULTA B√ÅSICA CON SELECT, WHERE, ORDER BY\n# ============================================\n\nprint(\"üîç SELECT con SQL:\")\n\n# spark.sql() - Ejecuta una consulta SQL y retorna un DataFrame\n# La consulta usa sintaxis SQL est√°ndar\n# Consulta: seleccionar columnas, filtrar y ordenar\nresultado = spark.sql(\"\"\"\n    SELECT nombre, departamento, salario\n    FROM empleados\n    WHERE salario > 50000\n    ORDER BY salario DESC\n\"\"\")\n# SELECT - selecciona las columnas que queremos\n# FROM - especifica la tabla (vista temporal que creamos)\n# WHERE - filtra filas donde salario > 50000\n# ORDER BY DESC - ordena por salario de mayor a menor\n\n# Mostrar los resultados\nresultado.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SQL: CONSULTA CON AGREGACIONES (GROUP BY)\n# ============================================\n\nprint(\"\\nüìä Agregaciones con SQL:\")\n\n# Consulta SQL con funciones de agregaci√≥n\nresultado_agg = spark.sql(\"\"\"\n    SELECT \n        departamento,\n        COUNT(*) as num_empleados,         \n        AVG(salario) as salario_promedio,  \n        MAX(salario) as salario_maximo     \n    FROM empleados\n    GROUP BY departamento                  \n    ORDER BY salario_promedio DESC         \n\"\"\")\n# COUNT(*) - cuenta el n√∫mero de filas por grupo\n# AVG(salario) - calcula el promedio de salario\n# MAX(salario) - encuentra el salario m√°ximo\n# GROUP BY - agrupa por departamento\n# ORDER BY - ordena por salario promedio descendente\n\nresultado_agg.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# SQL: CONSULTA CON CASE WHEN (CONDICIONALES)\n# ============================================\n\nprint(\"\\nüè∑Ô∏è Categorizaci√≥n con CASE WHEN:\")\n\n# Consulta SQL con l√≥gica condicional (CASE WHEN)\nresultado_case = spark.sql(\"\"\"\n    SELECT \n        nombre,\n        departamento,\n        salario,\n        CASE \n            WHEN salario >= 65000 THEN 'Alto'\n            WHEN salario >= 50000 THEN 'Medio'\n            ELSE 'Bajo'\n        END as nivel_salario\n    FROM empleados\n    ORDER BY salario DESC\n\"\"\")\n# CASE WHEN - estructura condicional (similar a if-elif-else)\n# Eval√∫a condiciones en orden y asigna el primer valor que cumpla\n# THEN - valor a asignar si la condici√≥n es verdadera\n# ELSE - valor por defecto si ninguna condici√≥n se cumple\n# END - cierra la estructura CASE\n# as nivel_salario - nombre de la nueva columna creada\n\nresultado_case.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. An√°lisis de Datos Reales\n\nEn esta secci√≥n aplicaremos todo lo aprendido en un caso de uso realista: an√°lisis de ventas de una tienda online.\n\n### Objetivos del an√°lisis:\n\nEste tipo de an√°lisis es com√∫n en Business Intelligence y Data Analytics. Buscaremos responder preguntas de negocio como:\n\n- ¬øQu√© productos generan m√°s ingresos?\n- ¬øQu√© categor√≠as son m√°s rentables?\n- ¬øQu√© ciudades tienen mejor desempe√±o?\n- ¬øQui√©nes son nuestros clientes m√°s valiosos?\n- ¬øC√≥mo evolucionan las ventas en el tiempo?\n\n### Escenario:\n\nTenemos datos transaccionales de una tienda online de electr√≥nica. Cada registro representa una venta con informaci√≥n sobre:\n\n- **Fecha**: Cu√°ndo ocurri√≥ la venta\n- **Producto**: Qu√© se vendi√≥\n- **Categor√≠a**: Clasificaci√≥n del producto\n- **Precio unitario**: Precio de cada unidad\n- **Cantidad**: Unidades vendidas\n- **Cliente**: ID del cliente\n- **Ciudad**: Ubicaci√≥n de la venta\n\nEste es un dataset t√≠pico que encontrar√≠as en sistemas de e-commerce, puntos de venta, o ERPs."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Crear dataset de ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CREAR DATASET DE VENTAS - CASO PR√ÅCTICO\n# ============================================\n\n# Dataset de ventas de una tienda online\n# Cada tupla representa una transacci√≥n de venta\ndatos_ventas = [\n    # (fecha, producto, categor√≠a, precio_unitario, cantidad, cliente, ciudad)\n    (\"2024-01-15\", \"Laptop\", \"Electr√≥nica\", 1200, 2, \"Cliente_001\", \"Madrid\"),\n    (\"2024-01-16\", \"Mouse\", \"Accesorios\", 25, 5, \"Cliente_002\", \"Barcelona\"),\n    (\"2024-01-17\", \"Teclado\", \"Accesorios\", 75, 3, \"Cliente_003\", \"Valencia\"),\n    (\"2024-01-18\", \"Monitor\", \"Electr√≥nica\", 350, 1, \"Cliente_001\", \"Madrid\"),\n    (\"2024-01-19\", \"Laptop\", \"Electr√≥nica\", 1200, 1, \"Cliente_004\", \"Sevilla\"),\n    (\"2024-01-20\", \"Auriculares\", \"Accesorios\", 85, 4, \"Cliente_005\", \"Bilbao\"),\n    (\"2024-02-15\", \"Laptop\", \"Electr√≥nica\", 1200, 3, \"Cliente_006\", \"Madrid\"),\n    (\"2024-02-16\", \"Tablet\", \"Electr√≥nica\", 450, 2, \"Cliente_002\", \"Barcelona\"),\n    (\"2024-02-17\", \"Mouse\", \"Accesorios\", 25, 10, \"Cliente_007\", \"Valencia\"),\n    (\"2024-02-18\", \"Monitor\", \"Electr√≥nica\", 350, 2, \"Cliente_003\", \"Valencia\"),\n    (\"2024-02-19\", \"Teclado\", \"Accesorios\", 75, 5, \"Cliente_008\", \"M√°laga\"),\n    (\"2024-03-15\", \"Laptop\", \"Electr√≥nica\", 1200, 1, \"Cliente_009\", \"Madrid\"),\n    (\"2024-03-16\", \"Auriculares\", \"Accesorios\", 85, 6, \"Cliente_010\", \"Zaragoza\"),\n    (\"2024-03-17\", \"Monitor\", \"Electr√≥nica\", 350, 3, \"Cliente_001\", \"Madrid\"),\n    (\"2024-03-18\", \"Tablet\", \"Electr√≥nica\", 450, 1, \"Cliente_005\", \"Bilbao\")\n]\n\n# Definir nombres de columnas\ncolumnas_ventas = [\"fecha\", \"producto\", \"categoria\", \"precio_unitario\", \"cantidad\", \"cliente\", \"ciudad\"]\n\n# Crear DataFrame de ventas\ndf_ventas = spark.createDataFrame(datos_ventas, columnas_ventas)\n\nprint(\"‚úÖ Dataset de ventas creado\")\ndf_ventas.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 An√°lisis de ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# AN√ÅLISIS: CALCULAR TOTAL DE VENTA\n# ============================================\n\nprint(\"üí∞ Agregar columna de total de venta:\")\n\n# Crear columna calculada: precio_unitario * cantidad\n# .withColumn() agrega una nueva columna al DataFrame\ndf_ventas = df_ventas.withColumn(\n    \"total_venta\",                              # Nombre de la nueva columna\n    col(\"precio_unitario\") * col(\"cantidad\")    # F√≥rmula: precio √ó cantidad\n)\n# Ejemplo: Si una laptop cuesta 1200‚Ç¨ y vendemos 2, total_venta = 2400‚Ç¨\n\ndf_ventas.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# AN√ÅLISIS: VENTAS POR CATEGOR√çA\n# ============================================\n\nprint(\"\\nüìä Ventas totales por categor√≠a:\")\n\n# An√°lisis de negocio: ¬øQu√© categor√≠a genera m√°s ingresos?\ndf_ventas.groupBy(\"categoria\").agg(\n    sum(\"total_venta\").alias(\"ventas_totales\"),        # Suma de todas las ventas\n    count(\"*\").alias(\"num_transacciones\"),             # N√∫mero de transacciones\n    avg(\"total_venta\").alias(\"venta_promedio\")         # Venta promedio por transacci√≥n\n).orderBy(col(\"ventas_totales\").desc()).show()         # Ordenar de mayor a menor"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productos m√°s vendidos\n",
    "print(\"\\nüèÜ Top 5 productos m√°s vendidos:\")\n",
    "df_ventas.groupBy(\"producto\").agg(\n",
    "    sum(\"cantidad\").alias(\"cantidad_vendida\"),\n",
    "    sum(\"total_venta\").alias(\"ingresos_totales\")\n",
    ").orderBy(col(\"ingresos_totales\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventas por ciudad\n",
    "print(\"\\nüåç Ventas por ciudad:\")\n",
    "df_ventas.groupBy(\"ciudad\").agg(\n",
    "    sum(\"total_venta\").alias(\"ventas_totales\"),\n",
    "    count(\"*\").alias(\"num_transacciones\")\n",
    ").orderBy(col(\"ventas_totales\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clientes m√°s valiosos\n",
    "print(\"\\nüëë Top clientes por valor:\")\n",
    "df_ventas.groupBy(\"cliente\").agg(\n",
    "    sum(\"total_venta\").alias(\"total_gastado\"),\n",
    "    count(\"*\").alias(\"num_compras\")\n",
    ").orderBy(col(\"total_gastado\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 An√°lisis con SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vista temporal\n",
    "df_ventas.createOrReplaceTempView(\"ventas\")\n",
    "\n",
    "# An√°lisis de ventas por mes\n",
    "print(\"üìÖ Ventas por mes:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        SUBSTRING(fecha, 1, 7) as mes,\n",
    "        COUNT(*) as num_transacciones,\n",
    "        SUM(total_venta) as ventas_totales,\n",
    "        AVG(total_venta) as venta_promedio\n",
    "    FROM ventas\n",
    "    GROUP BY mes\n",
    "    ORDER BY mes\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Productos con ventas superiores al promedio\n",
    "print(\"\\nüìà Productos con ventas por encima del promedio:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        producto,\n",
    "        SUM(total_venta) as ventas_totales\n",
    "    FROM ventas\n",
    "    GROUP BY producto\n",
    "    HAVING SUM(total_venta) > (\n",
    "        SELECT AVG(ventas_producto)\n",
    "        FROM (\n",
    "            SELECT SUM(total_venta) as ventas_producto\n",
    "            FROM ventas\n",
    "            GROUP BY producto\n",
    "        )\n",
    "    )\n",
    "    ORDER BY ventas_totales DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 8. Trabajar con archivos CSV\n\nEn producci√≥n, raramente crearemos DataFrames desde listas en memoria. Lo m√°s com√∫n es leer datos desde archivos o bases de datos.\n\n### ¬øPor qu√© CSV?\n\nCSV (Comma-Separated Values) es uno de los formatos m√°s comunes para intercambio de datos porque:\n\n- **Universal**: Soportado por todas las herramientas\n- **Legible**: Puedes abrirlo en cualquier editor de texto\n- **Simple**: F√°cil de generar y procesar\n- **Compatible**: Excel, Google Sheets, bases de datos, etc.\n\n### Limitaciones de CSV:\n\n- No tiene tipos de datos (todo es texto, hay que inferir o especificar)\n- Menos eficiente que formatos binarios (Parquet, ORC)\n- Problemas con caracteres especiales, comillas, saltos de l√≠nea\n- No soporta compresi√≥n nativa eficiente\n- Para Big Data, Parquet es mejor opci√≥n\n\n### 8.1 Guardar DataFrame como CSV\n\n**Opciones importantes al escribir:**\n\n- **`.mode()`**: Qu√© hacer si el archivo existe\n  - `\"overwrite\"`: Reemplaza el archivo existente\n  - `\"append\"`: Agrega al archivo existente\n  - `\"ignore\"`: No hace nada si existe\n  - `\"error\"`: Error si existe (default)\n  \n- **`.option(\"header\", \"true\")`**: Incluir nombres de columnas en la primera fila\n- **`.option(\"sep\", \",\")`**: Separador (por defecto es coma)\n- **`.coalesce(1)`**: Escribe en un solo archivo (√∫til para archivos peque√±os)\n  - Por defecto, Spark escribe m√∫ltiples archivos (uno por partici√≥n)\n  - Para datasets grandes, mant√©n m√∫ltiples particiones"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# GUARDAR DATAFRAME COMO ARCHIVO CSV\n# ============================================\n\n# Definir la ruta donde guardar el archivo\nruta_salida = \"datos_ventas.csv\"\n\n# Guardar el DataFrame como CSV\ndf_ventas.coalesce(1).write \\\n    .mode(\"overwrite\") \\\n    .option(\"header\", \"true\") \\\n    .csv(ruta_salida)\n# .coalesce(1) - combina todas las particiones en un solo archivo\n# .write - inicia el proceso de escritura\n# .mode(\"overwrite\") - sobrescribe si el archivo ya existe\n# .option(\"header\", \"true\") - incluye nombres de columnas en la primera fila\n# .csv(ruta) - especifica el formato y la ubicaci√≥n\n\nprint(f\"‚úÖ Datos guardados en {ruta_salida}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 8.2 Leer DataFrame desde CSV\n\n**Opciones importantes al leer:**\n\n- **`.option(\"header\", \"true\")`**: La primera fila contiene nombres de columnas\n  - Si es \"false\", Spark asigna nombres gen√©ricos (_c0, _c1, _c2...)\n  \n- **`.option(\"inferSchema\", \"true\")`**: Spark escanea el archivo para detectar tipos\n  - **Ventaja**: Autom√°tico, detecta int, float, string, etc.\n  - **Desventaja**: M√°s lento, escanea datos dos veces\n  - **Alternativa**: Definir esquema expl√≠cito (m√°s r√°pido y confiable)\n\n- **Otras opciones √∫tiles**:\n  - `.option(\"sep\", \";\")`: Especificar separador diferente\n  - `.option(\"encoding\", \"UTF-8\")`: Especificar codificaci√≥n\n  - `.option(\"nullValue\", \"NA\")`: Qu√© valor representa NULL\n  - `.option(\"dateFormat\", \"yyyy-MM-dd\")`: Formato de fechas\n  - `.option(\"quote\", \"\\\"\")`: Car√°cter de comillas\n\n### Mejores pr√°cticas para CSV en producci√≥n:\n\n1. **Definir esquema expl√≠cito** en lugar de inferSchema para mejor rendimiento\n2. **Validar datos** despu√©s de cargar\n3. **Considerar Parquet** para datos grandes (10x m√°s r√°pido, comprimido)\n4. **Particionar** archivos grandes apropiadamente\n5. **Documentar** la estructura esperada de los datos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# LEER DATAFRAME DESDE ARCHIVO CSV\n# ============================================\n\n# Leer el archivo CSV que acabamos de guardar\ndf_leido = spark.read \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .csv(ruta_salida)\n# spark.read - inicia el proceso de lectura\n# .option(\"header\", \"true\") - la primera fila contiene nombres de columnas\n# .option(\"inferSchema\", \"true\") - Spark detecta autom√°ticamente los tipos de datos\n#    (escanea el archivo para determinar si son strings, enteros, etc.)\n# .csv(ruta) - especifica que es un archivo CSV y su ubicaci√≥n\n\nprint(\"üìÇ Datos le√≠dos desde CSV:\")\ndf_leido.show(5)         # Mostrar primeras 5 filas\ndf_leido.printSchema()   # Mostrar el esquema inferido"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: An√°lisis de empleados\n",
    "Usando el DataFrame `df_empleados`, responde:\n",
    "1. ¬øCu√°l es el salario promedio por departamento?\n",
    "2. ¬øCu√°ntos empleados tienen m√°s de 35 a√±os?\n",
    "3. ¬øCu√°l es el departamento con mayor masa salarial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ - Ejercicio 1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ - Ejercicio 1.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ - Ejercicio 1.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: An√°lisis de ventas\n",
    "Usando el DataFrame `df_ventas`, responde:\n",
    "1. ¬øCu√°l es el ticket promedio de compra?\n",
    "2. ¬øQu√© ciudad genera m√°s ingresos?\n",
    "3. Crea una columna que categorice las ventas en 'Alta' (>1000), 'Media' (500-1000), 'Baja' (<500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ - Ejercicio 2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ - Ejercicio 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ - Ejercicio 2.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: SQL Avanzado\n",
    "Usando SQL, crea una consulta que muestre:\n",
    "- Los 3 productos m√°s vendidos por categor√≠a\n",
    "- El porcentaje de ventas que representa cada producto dentro de su categor√≠a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu c√≥digo aqu√≠ - Ejercicio 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Soluciones a los Ejercicios\n",
    "\n",
    "### Soluci√≥n Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1.1 - Salario promedio por departamento\n",
    "print(\"üíº Salario promedio por departamento:\")\n",
    "df_empleados.groupBy(\"departamento\").agg(\n",
    "    avg(\"salario\").alias(\"salario_promedio\")\n",
    ").orderBy(col(\"salario_promedio\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1.2 - Empleados con m√°s de 35 a√±os\n",
    "num_empleados_35 = df_empleados.filter(col(\"edad\") > 35).count()\n",
    "print(f\"üë• Empleados con m√°s de 35 a√±os: {num_empleados_35}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1.3 - Departamento con mayor masa salarial\n",
    "print(\"üí∞ Departamento con mayor masa salarial:\")\n",
    "df_empleados.groupBy(\"departamento\").agg(\n",
    "    sum(\"salario\").alias(\"masa_salarial\")\n",
    ").orderBy(col(\"masa_salarial\").desc()).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soluci√≥n Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2.1 - Ticket promedio\n",
    "ticket_promedio = df_ventas.agg(avg(\"total_venta\").alias(\"ticket_promedio\")).collect()[0][0]\n",
    "print(f\"üé´ Ticket promedio: {ticket_promedio:.2f}‚Ç¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2.2 - Ciudad con m√°s ingresos\n",
    "print(\"üèôÔ∏è Ciudad con m√°s ingresos:\")\n",
    "df_ventas.groupBy(\"ciudad\").agg(\n",
    "    sum(\"total_venta\").alias(\"ingresos_totales\")\n",
    ").orderBy(col(\"ingresos_totales\").desc()).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2.3 - Categorizar ventas\n",
    "print(\"üè∑Ô∏è Categorizaci√≥n de ventas:\")\n",
    "df_ventas_categorizado = df_ventas.withColumn(\n",
    "    \"categoria_venta\",\n",
    "    when(col(\"total_venta\") > 1000, \"Alta\")\n",
    "    .when((col(\"total_venta\") >= 500) & (col(\"total_venta\") <= 1000), \"Media\")\n",
    "    .otherwise(\"Baja\")\n",
    ")\n",
    "df_ventas_categorizado.select(\"producto\", \"total_venta\", \"categoria_venta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soluci√≥n Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 3 - Top 3 productos por categor√≠a con porcentaje\n",
    "print(\"üèÜ Top 3 productos por categor√≠a con % de ventas:\")\n",
    "spark.sql(\"\"\"\n",
    "    WITH ventas_producto AS (\n",
    "        SELECT \n",
    "            categoria,\n",
    "            producto,\n",
    "            SUM(total_venta) as ventas_producto\n",
    "        FROM ventas\n",
    "        GROUP BY categoria, producto\n",
    "    ),\n",
    "    ventas_categoria AS (\n",
    "        SELECT \n",
    "            categoria,\n",
    "            SUM(total_venta) as ventas_categoria\n",
    "        FROM ventas\n",
    "        GROUP BY categoria\n",
    "    )\n",
    "    SELECT \n",
    "        vp.categoria,\n",
    "        vp.producto,\n",
    "        vp.ventas_producto,\n",
    "        ROUND((vp.ventas_producto / vc.ventas_categoria) * 100, 2) as porcentaje\n",
    "    FROM ventas_producto vp\n",
    "    JOIN ventas_categoria vc ON vp.categoria = vc.categoria\n",
    "    ORDER BY vp.categoria, vp.ventas_producto DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Cerrar la SparkSession\n",
    "\n",
    "Siempre es buena pr√°ctica cerrar la SparkSession al finalizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerrar SparkSession\n",
    "# spark.stop()\n",
    "# print(\"‚úÖ SparkSession cerrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 12. Recursos Adicionales\n\n### Documentaci√≥n Oficial\n\n- **[PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)**: Referencia completa de la API\n- **[Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)**: Gu√≠a de Spark SQL\n- **[Spark Configuration](https://spark.apache.org/docs/latest/configuration.html)**: Opciones de configuraci√≥n\n\n### Conceptos Clave de Big Data\n\n#### 1. **RDD (Resilient Distributed Dataset)**\n- Estructura de datos fundamental de Spark\n- Colecci√≥n distribuida e inmutable de objetos\n- Tolerante a fallos (puede reconstruirse si se pierde una partici√≥n)\n- API de bajo nivel (preferir DataFrames)\n\n#### 2. **DataFrame**\n- API de alto nivel construida sobre RDDs\n- Datos organizados en columnas con nombre\n- Similar a tablas SQL o DataFrames de Pandas\n- Optimizado autom√°ticamente por Catalyst\n\n#### 3. **Transformaciones Lazy (Lazy Evaluation)**\n- Las operaciones no se ejecutan inmediatamente\n- Spark construye un plan de ejecuci√≥n (DAG)\n- Optimiza el plan completo antes de ejecutar\n- Mejora dram√°ticamente el rendimiento\n\n#### 4. **Particionamiento**\n- Divisi√≥n de datos para procesamiento paralelo\n- Cada partici√≥n se procesa en un executor diferente\n- N√∫mero √≥ptimo depende del tama√±o de datos y cluster\n- `df.repartition(n)`: Redistribuir en n particiones\n- `df.coalesce(n)`: Reducir particiones (sin shuffle)\n\n#### 5. **DAG (Directed Acyclic Graph)**\n- Grafo dirigido ac√≠clico que representa el plan de ejecuci√≥n\n- Spark construye el DAG de todas las transformaciones\n- El optimizador Catalyst mejora el DAG\n- Se divide en stages y tasks para ejecuci√≥n\n\n#### 6. **Catalyst Optimizer**\n- Motor de optimizaci√≥n de consultas de Spark SQL\n- Aplica reglas de optimizaci√≥n (predicate pushdown, constant folding, etc.)\n- Genera c√≥digo optimizado en tiempo de ejecuci√≥n\n- Funciona igual para DataFrames y SQL\n\n#### 7. **Shuffle**\n- Redistribuci√≥n de datos entre particiones\n- Operaci√≥n costosa (involucra red y disco)\n- Ocurre en: groupBy, join, repartition, orderBy\n- Minimizar shuffles mejora rendimiento\n\n### Mejores Pr√°cticas para Producci√≥n\n\n#### Rendimiento:\n- ‚úÖ **Usar DataFrames** en lugar de RDDs cuando sea posible\n- ‚úÖ **Evitar collect()** en datasets grandes (trae todo al driver)\n- ‚úÖ **Usar cache()** o **persist()** para DataFrames reutilizados m√∫ltiples veces\n- ‚úÖ **Particionar datos** apropiadamente (ni muy pocas ni demasiadas particiones)\n- ‚úÖ **Usar formatos eficientes** como Parquet o ORC para almacenamiento\n- ‚úÖ **Aprovechar funciones built-in** en lugar de UDFs (User Defined Functions)\n- ‚úÖ **Filtrar datos temprano** (pushdown predicates)\n- ‚úÖ **Evitar shuffles** innecesarios\n\n#### Almacenamiento:\n- ‚úÖ **Parquet**: Formato columnar comprimido (mejor para lectura)\n- ‚úÖ **ORC**: Similar a Parquet, optimizado para Hive\n- ‚úÖ **Avro**: Formato orientado a filas (mejor para escritura)\n- ‚ùå **CSV/JSON**: Solo para intercambio, no para procesamiento\n\n#### Memoria:\n```python\n# Niveles de persistencia\nfrom pyspark import StorageLevel\n\ndf.persist(StorageLevel.MEMORY_ONLY)  # Solo RAM\ndf.persist(StorageLevel.MEMORY_AND_DISK)  # RAM + disco\ndf.persist(StorageLevel.DISK_ONLY)  # Solo disco\ndf.persist(StorageLevel.MEMORY_AND_DISK_SER)  # Serializado\n```\n\n#### Depuraci√≥n:\n- Use `df.explain()` para ver el plan de ejecuci√≥n\n- Use `df.printSchema()` para verificar tipos de datos\n- Revise la Spark UI (http://localhost:4040) para monitorear jobs\n\n### Formatos de archivo recomendados:\n\n| Formato | Cu√°ndo usar | Ventajas | Desventajas |\n|---------|-------------|----------|-------------|\n| **Parquet** | Procesamiento anal√≠tico | Columnar, comprimido, r√°pido | No legible por humanos |\n| **CSV** | Intercambio simple | Universal, legible | Lento, sin tipos |\n| **JSON** | APIs, datos semi-estructurados | Flexible, legible | Ineficiente para Big Data |\n| **Avro** | Streaming, escritura intensiva | Schema evolution | Menos com√∫n |\n| **ORC** | Hive, lectura anal√≠tica | Muy comprimido | Espec√≠fico de Hadoop |\n\n### Comandos √∫tiles de configuraci√≥n:\n\n```python\n# Ver configuraci√≥n actual\nspark.conf.get(\"spark.sql.shuffle.partitions\")\n\n# Cambiar configuraci√≥n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")\n\n# Configuraciones comunes\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")  # Optimizaci√≥n adaptiva\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\")  # 10MB\n```\n\n### Recursos de aprendizaje adicionales:\n\n- **[Learning Spark (Libro)](https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf)**: Libro gratuito oficial\n- **[Databricks Academy](https://academy.databricks.com/)**: Cursos gratuitos\n- **[Spark by Examples](https://sparkbyexamples.com/)**: Tutoriales y ejemplos\n- **[Stack Overflow - apache-spark tag](https://stackoverflow.com/questions/tagged/apache-spark)**: Comunidad activa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì ¬°Felicitaciones!\n",
    "\n",
    "Has completado esta clase de PySpark para Big Data. Ahora conoces:\n",
    "\n",
    "- ‚úÖ Fundamentos de PySpark\n",
    "- ‚úÖ Creaci√≥n y manipulaci√≥n de DataFrames\n",
    "- ‚úÖ Transformaciones y acciones\n",
    "- ‚úÖ Agregaciones y an√°lisis de datos\n",
    "- ‚úÖ Consultas SQL en PySpark\n",
    "- ‚úÖ Trabajo con archivos CSV\n",
    "- ‚úÖ An√°lisis de datos reales\n",
    "\n",
    "**¬°Sigue practicando y explorando el mundo del Big Data! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}