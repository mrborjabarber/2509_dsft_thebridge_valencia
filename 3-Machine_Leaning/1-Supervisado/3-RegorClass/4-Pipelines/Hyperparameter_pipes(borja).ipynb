{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearch & Pipelines\n",
    "GridSearch es una herramienta de optimización que usamos cuando ajustamos hiperparámetros. Definimos la cuadrícula(grid) de parámetros que queremos buscar y seleccionamos la mejor combinación de parámetros para nuestros datos.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método 1\n",
    "Itera un único algoritmo sobre un conjunto de hiperparámetros, mediante la validación cruzada, iterando con el dataset dividido en train y val para recoger los errores y evaluar la mejor métrica. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos el módulo warnings para gestionar advertencias\nimport warnings\n\n# Ignoramos las advertencias de deprecación para mantener limpia la salida\n# Esto evita que se muestren mensajes de funciones que están quedando obsoletas\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculamos el número total de modelos que GridSearch evaluará\n# 4 kernels × 7 valores de C × 7 valores de degree × 2 valores de gamma = 392 combinaciones\nprint(4*7*7*2, \"modelos\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos las librerías necesarias para el GridSearch básico\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import GridSearchCV\n\n# Cargamos el dataset Iris (clasificación de flores con 4 características)\niris = datasets.load_iris()\n\n# Definimos el espacio de búsqueda de hiperparámetros para SVM\nparameters = {\n    'kernel': ['linear', 'rbf', 'sigmoid', 'poly'],  # Tipos de kernel a probar\n    'C': [0.001, 0.1, 0.5, 1, 5, 10, 100],  # Parámetro de regularización\n    'degree': [1,2,3,4,5,6,7],  # Grado del polinomio (solo para kernel 'poly')\n    'gamma': ['scale', 'auto']  # Coeficiente del kernel para 'rbf', 'poly' y 'sigmoid'\n}\n\n# Creamos una instancia del clasificador SVM sin hiperparámetros específicos\nsvc = svm.SVC()\n\n# Configuramos GridSearchCV para buscar la mejor combinación de hiperparámetros\nclf = GridSearchCV(estimator = svc,  # Modelo a optimizar\n                  param_grid = parameters,  # Espacio de búsqueda\n                  n_jobs = -1,  # Usar todos los núcleos del procesador\n                  cv = 10,  # Validación cruzada con 10 particiones\n                  scoring=\"accuracy\")  # Métrica de evaluación\n\n# Entrenamos el modelo probando todas las combinaciones\nclf.fit(iris.data, iris.target)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos el mejor estimador (modelo) encontrado por GridSearch\n# Este es el modelo con los hiperparámetros óptimos\nclf.best_estimator_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos los mejores hiperparámetros encontrados\nprint(clf.best_params_)\n\n# Mostramos el mejor score (precisión) obtenido con validación cruzada\n# Este es el promedio de accuracy en los 10 folds\nprint(clf.best_score_)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos la función para validación cruzada manual\nfrom sklearn.model_selection import cross_val_score\n\n# Creamos un modelo SVM con los mejores hiperparámetros encontrados\nclf = svm.SVC(C=0.1, degree=2, gamma='auto', kernel='poly')\n\n# Realizamos validación cruzada con 10 folds para evaluar el modelo\n# Esto nos da un score por cada fold (partición de los datos)\nscores = cross_val_score(clf, iris.data, iris.target, cv=10)\nscores"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos numpy para cálculos estadísticos\nimport numpy as np\n\n# Calculamos la media de los scores de validación cruzada\n# Esto nos da una estimación de la precisión esperada del modelo\nprint(np.mean(scores))\n\n# Calculamos la desviación estándar de los scores\n# Un valor bajo indica que el modelo es estable entre diferentes particiones\nprint(np.std(scores))"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método 2\n",
    "\n",
    "Una forma más senior es montar un único gridsearch para iterar con varios modelos con otros hiperparámetros y con la validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos pickle para guardar y cargar modelos entrenados\n# Pickle nos permite serializar objetos de Python a archivos\nimport pickle"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos todas las librerías necesarias para el Método 2\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression  # Modelo de regresión logística\nfrom sklearn.ensemble import RandomForestClassifier  # Modelo de bosque aleatorio\nfrom sklearn.model_selection import GridSearchCV  # Búsqueda de hiperparámetros\nfrom sklearn.pipeline import Pipeline  # Para encadenar transformaciones y modelo\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler  # Escaladores de datos\nfrom sklearn.model_selection import train_test_split  # División de datos\nfrom sklearn import svm  # Support Vector Machines\n\n# Establecemos semilla aleatoria para reproducibilidad de resultados\nnp.random.seed(0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cargamos el dataset Iris\niris = datasets.load_iris()\n\n# Separamos las características (X) de las etiquetas (y)\nX = iris.data  # Matriz de características (150 muestras × 4 características)\ny = iris.target  # Vector de etiquetas (0, 1, o 2 para cada especie)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Dividimos los datos en conjunto de entrenamiento (80%) y prueba (20%)\nX_train, X_test, y_train, y_test = train_test_split(X,  # Características\n                                                    y,  # Etiquetas\n                                                    test_size=0.2,  # 20% para test\n                                                    random_state=2)  # Semilla para reproducibilidad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Creamos un Pipeline que encadena preprocesamiento y modelo\n# Un pipeline ejecuta pasos secuencialmente: primero escala, luego clasifica\npipe = Pipeline(steps=[(\"scaler\", StandardScaler()),  # Paso 1: Estandarización\n    ('classifier', svm.SVC())  # Paso 2: Clasificador\n])\n\n# CONFIGURACIÓN 1: Regresión Logística con diferentes iteraciones y penalizaciones\nlogistic_params = {\n    # Probamos dos configuraciones de regresión logística\n    'classifier': [LogisticRegression(max_iter=1000, solver='liblinear'), \n                   LogisticRegression(max_iter=10, solver='liblinear')],\n    'classifier__penalty': ['l1', 'l2']  # Penalización L1 (Lasso) o L2 (Ridge)\n}\n\n# CONFIGURACIÓN 2: Random Forest con diferentes escaladores y profundidades\nrandom_forest_params = {\n    'scaler': [StandardScaler(), MinMaxScaler(), None],  # Diferentes escaladores o ninguno\n    'classifier': [RandomForestClassifier()],  # Clasificador de bosque aleatorio\n    'classifier__max_depth': [2,3]  # Profundidad máxima de los árboles\n}\n\n# CONFIGURACIÓN 3: SVM con diferentes valores de C (regularización)\nsvm_param = {\n    'classifier': [svm.SVC()],  # Clasificador SVM\n    'classifier__C': [0.001, 0.1, 0.5, 1, 5, 10, 100],  # Parámetro de regularización\n}\n\n# Creamos una lista con todos los espacios de búsqueda\n# GridSearch probará cada uno de estos espacios independientemente\nsearch_space = [\n    logistic_params,\n    random_forest_params,\n    svm_param\n]\n\n# Configuramos GridSearchCV para comparar múltiples algoritmos\nclf = GridSearchCV(estimator = pipe,  # Pipeline base\n                  param_grid = search_space,  # Espacio de búsqueda multi-modelo\n                  cv = 5,  # Validación cruzada con 5 folds\n                  verbose=2,  # Muestra información del proceso\n                  n_jobs=-1)  # Usa todos los núcleos\n\n# Entrenamos todos los modelos y encontramos el mejor\nclf.fit(X_train, y_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos el pipeline completo del mejor modelo encontrado\nprint(clf.best_estimator_)\n\n# Mostramos el mejor score obtenido en validación cruzada\nprint(clf.best_score_)\n\n# Mostramos los mejores parámetros (incluyendo el modelo y sus hiperparámetros)\nprint(clf.best_params_)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Usamos el mejor modelo encontrado para hacer predicciones en el conjunto de test\n# Esto nos muestra las clases predichas para cada muestra del test\nclf.best_estimator_.predict(X_test)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluamos el mejor modelo en el conjunto de test\n# Esto nos da la accuracy (precisión) del modelo en datos no vistos durante el entrenamiento\nclf.best_estimator_.score(X_test,y_test)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos nuevamente el mejor estimador\n# Es el pipeline completo con scaler y clasificador\nclf.best_estimator_"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro uso puede ser la construcción de pipelines (tuberías) específicos para cada tipo de modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos todas las librerías necesarias para el Método 3\nfrom sklearn.preprocessing import StandardScaler  # Estandarización de datos\nfrom sklearn.impute import SimpleImputer  # Imputación de valores faltantes\nfrom sklearn.pipeline import Pipeline  # Para crear pipelines\nfrom sklearn.model_selection import GridSearchCV  # Búsqueda de hiperparámetros\nfrom sklearn.feature_selection import SelectKBest  # Selección de mejores características\nfrom sklearn.metrics import accuracy_score  # Métrica de evaluación\n\nimport pandas as pd  # Manipulación de datos tabulares\nimport numpy as np  # Operaciones numéricas\n\n# Importamos los clasificadores que usaremos\nfrom sklearn.svm import SVC  # Support Vector Classifier\nfrom sklearn.linear_model import LogisticRegression  # Regresión Logística\nfrom sklearn.ensemble import RandomForestClassifier  # Random Forest"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# PIPELINE 1: Regresión Logística con preprocesamiento completo\nreg_log = Pipeline(steps = [\n    (\"imputer\", SimpleImputer()),  # Paso 1: Imputa valores faltantes\n    (\"scaler\", StandardScaler()),  # Paso 2: Estandariza las características\n    (\"reglog\", LogisticRegression())  # Paso 3: Modelo de regresión logística\n])\n\n# Espacio de búsqueda para Regresión Logística\nreg_log_param = {\n    \"imputer__strategy\": ['mean', 'median'],  # Estrategia de imputación: media o mediana\n    \"reglog__penalty\": ['l1', 'l2'],  # Tipo de regularización\n    \"reglog__C\": np.logspace(0, 4, 10)  # 10 valores de C entre 10^0 y 10^4 (escala logarítmica)\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": "# PIPELINE 2: Random Forest (sin pipeline, modelo directo)\nrand_forest = RandomForestClassifier()\n\n# Espacio de búsqueda para Random Forest\nrand_forest_param = {\n    \"n_estimators\": [10, 100, 1000],  # Número de árboles en el bosque\n    \"max_features\": [1,2,3]  # Número máximo de características a considerar por división\n}\n\n# PIPELINE 3: SVM con preprocesamiento y selección de características\nsvm = Pipeline(steps=[\n    (\"scaler\", StandardScaler()),  # Paso 1: Estandarización\n    (\"selectkbest\", SelectKBest()),  # Paso 2: Selección de k mejores características\n    (\"svm\", SVC())  # Paso 3: Clasificador SVM\n])\n\n# Espacio de búsqueda para SVM\nsvm_param = {\n    'selectkbest__k': [2, 3, 4],  # Número de características a seleccionar\n    'svm__kernel': ['linear', 'rbf', 'sigmoid', 'poly'],  # Tipo de kernel\n    'svm__C': [0.001, 0.1, 0.5, 1, 5, 10, 100],  # Parámetro de regularización\n    'svm__degree': [1,2,3,4],  # Grado del polinomio (solo para kernel 'poly')\n    'svm__gamma': ['scale', 'auto']  # Coeficiente del kernel\n}\n\n# Creamos GridSearchCV para cada modelo con su respectivo espacio de búsqueda\ngs_reg_log = GridSearchCV(reg_log,  # Pipeline de regresión logística\n                         reg_log_param,  # Parámetros a buscar\n                         cv = 10,  # 10-fold cross validation\n                         scoring = 'accuracy',  # Métrica de evaluación\n                         verbose = 1,  # Muestra información del proceso\n                         n_jobs = -1)  # Usa todos los núcleos\n\ngs_rand_forest = GridSearchCV(rand_forest,  # Modelo Random Forest\n                         rand_forest_param,\n                         cv = 10,\n                         scoring = 'accuracy',\n                         verbose = 1,\n                         n_jobs = -1)\n\ngs_svm = GridSearchCV(svm,  # Pipeline SVM\n                         svm_param,\n                         cv = 10,\n                         scoring = 'accuracy',\n                         verbose = 1,\n                         n_jobs = -1)\n\n# Creamos un diccionario con todos los grid searches para iterar sobre ellos\ngrids = {\"gs_reg_log\": gs_reg_log,\n        \"gs_rand_forest\": gs_rand_forest,\n        \"gs_svm\": gs_svm}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos train_test_split para dividir los datos\nfrom sklearn.model_selection import train_test_split \n\n# Preparamos las características y las etiquetas\nX = iris.data  # Características del dataset Iris\ny = iris.target  # Etiquetas (especies de flores)\n\n# Dividimos en train (80%) y test (20%)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2,  # 20% para test\n                                                    random_state=42)  # Semilla para reproducibilidad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Iteramos sobre cada grid search y lo entrenamos\n# Esto evaluará cada modelo con su respectivo espacio de hiperparámetros\nfor nombre, grid_search in grids.items():\n    grid_search.fit(X_train, y_train)  # Entrenamos cada modelo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": "# Mostramos los resultados del mejor modelo de Regresión Logística\nprint(gs_reg_log.best_score_)  # Mejor accuracy en validación cruzada\nprint(gs_reg_log.best_params_)  # Mejores hiperparámetros encontrados\nprint(gs_reg_log.best_estimator_)  # Pipeline completo del mejor modelo\nprint(gs_reg_log.best_estimator_['reglog'])  # Solo el modelo de regresión logística"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos los resultados del mejor modelo de Random Forest\nprint(gs_rand_forest.best_score_)  # Mejor accuracy en validación cruzada\nprint(gs_rand_forest.best_params_)  # Mejores hiperparámetros encontrados\nprint(gs_rand_forest.best_estimator_)  # Modelo completo del mejor Random Forest"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos los resultados del mejor modelo SVM\nprint(gs_svm.best_score_)  # Mejor accuracy en validación cruzada\nprint(gs_svm.best_params_)  # Mejores hiperparámetros encontrados\nprint(gs_svm.best_estimator_)  # Pipeline completo del mejor modelo\nprint(gs_svm.best_estimator_['svm'])  # Solo el clasificador SVM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparamos los scores de todos los modelos\n# Creamos una lista de tuplas con el nombre y el mejor score de cada grid\nbest_grids = [(i, j.best_score_) for i, j in grids.items()]\n\n# Convertimos a DataFrame y ordenamos por mejor score (descendente)\nbest_grids = pd.DataFrame(best_grids, columns=[\"Grid\", \"Best score\"]).sort_values(by=\"Best score\", ascending=False)\nbest_grids  # Mostramos la tabla comparativa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos el mejor estimador de SVM (el modelo con mejor score en validación)\ngs_svm.best_estimator_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hacemos predicciones con el mejor modelo SVM en el conjunto de test\npreds = gs_svm.best_estimator_.predict(X_test)\n\n# Calculamos la accuracy comparando predicciones con valores reales\naccuracy_score(y_test, preds)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos el mejor pipeline de Regresión Logística\ngs_reg_log.best_estimator_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluamos el mejor modelo de Regresión Logística en el conjunto de test\npreds = gs_reg_log.best_estimator_.predict(X_test)\n\n# Calculamos la accuracy - ¡Obtenemos 100%! El modelo generaliza perfectamente\naccuracy_score(y_test, preds)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluamos el mejor modelo de Random Forest en el conjunto de test\npreds = gs_rand_forest.best_estimator_.predict(X_test)\n\n# Calculamos la accuracy - ¡También 100%! Generaliza perfectamente\naccuracy_score(y_test, preds)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tanto la regresión logísitca(pipeline) como el random forest son los modelos que mejor generalizan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos nuevamente el mejor modelo SVM\ngs_svm.best_estimator_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Accedemos solo al clasificador SVM dentro del pipeline\n# Usando notación de diccionario ['svm']\ngs_svm.best_estimator_['svm']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluamos el SVM en el conjunto de test\npreds = gs_svm.best_estimator_.predict(X_test)\n\n# Calculamos la accuracy - 96.67%, ligeramente inferior a los otros dos\naccuracy_score(y_test, preds)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Accedemos al clasificador SVM del pipeline\ngs_svm.best_estimator_['svm']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CONCLUSIÓN: El mejor modelo es la Regresión Logística\n# Guardamos el mejor modelo para usar en producción\nbest_model = gs_reg_log.best_estimator_\n\n# Evaluamos el modelo final en el conjunto de test\nbest_model.score(X_test, y_test)  # Accuracy de 100%"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos los mejores parámetros del modelo ganador\ngs_reg_log.best_params_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confirmamos que el mejor modelo es Regresión Logística\nbest_model = gs_reg_log.best_estimator_\n\n# Evaluamos nuevamente para confirmar el resultado\nbest_model.score(X_test, y_test)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos el pipeline completo del mejor modelo\ngs_reg_log.best_estimator_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos una vez más el rendimiento del mejor modelo\nbest_model = gs_reg_log.best_estimator_\nbest_model.score(X_test, y_test)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pipeline final del modelo ganador\ngs_reg_log.best_estimator_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos pickle para serialización\nimport pickle\n\n# Definimos el nombre del archivo donde guardaremos el modelo\nfilename = 'finished_model'\n\n# Guardamos el modelo en un archivo binario\n# 'wb' = write binary (escritura binaria)\nwith open(filename, 'wb') as archivo_salida:\n    pickle.dump(best_model, archivo_salida)  # Serializamos el modelo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cargamos el modelo guardado desde el archivo\n# 'rb' = read binary (lectura binaria)\nwith open(filename, 'rb') as archivo_entrada:\n    modelo_importado = pickle.load(archivo_entrada)  # Deserializamos el modelo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos que el modelo cargado funciona correctamente\n# Evaluamos en el conjunto de test y multiplicamos por 100 para ver el porcentaje\nmodelo_importado.score(X_test, y_test)*100  # 100.0% de accuracy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hacemos predicciones con el modelo cargado\n# Esto demuestra que el modelo guardado funciona perfectamente\nmodelo_importado.predict(X_test)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos la estructura completa del modelo importado\n# Verificamos que mantiene todos los pasos del pipeline\nmodelo_importado"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Código comentado: Ejemplo de cómo usar el modelo con nuevos datos\n# modelo_importado.predict(X_new)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya hemos escogido modelo gracias a los datos de validación. Ahora habría que entrenar el modelo con TODOS los datos de train."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearch\n",
    "El problema que tiene el GridSearchCV es que computacionalmente es muy costoso cuando el espacio dimensional de los hiperparámetros es grande.\n",
    "\n",
    "Mediante el RandomSearch no se prueban todas las combinaciones, sino unas cuantas de manera aleatoria. Funciona bien con datasets con pocas features. Incluso [hay papers](https://www.jmlr.org/papers/v13/bergstra12a.html) que aseguran que es más eficiente RandomSearch frente a GridSearch\n",
    "\n",
    "![imagen](https://miro.medium.com/proxy/1*ZTlQm_WRcrNqL-nLnx6GJA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generamos 100 valores espaciados logarítmicamente entre 10^-2 y 10^4\n# Esto es útil para definir rangos de hiperparámetros en escala logarítmica\n# Por ejemplo, para el parámetro C en regresión logística\nnp.logspace(-2, 4, 100)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos RandomizedSearchCV para búsqueda aleatoria de hiperparámetros\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Creamos el pipeline para Regresión Logística\nreg_log = Pipeline(steps=[\n                          (\"imputer\",SimpleImputer()),  # Imputación de valores faltantes\n                          (\"scaler\",StandardScaler()),  # Estandarización\n                          (\"reglog\",LogisticRegression(max_iter=100000))  # Modelo con más iteraciones\n                         ])\n\n# Definimos el espacio de búsqueda de hiperparámetros\nreg_log_param = {    \n                 \"imputer__strategy\": ['mean', 'median', 'most_frequent'],  # 3 estrategias de imputación\n                 \"reglog__penalty\": [\"l1\",\"l2\"],  # 2 tipos de penalización\n                 \"reglog__C\": np.logspace(-2, 4, 100)  # 100 valores de C\n                }\n# En total hay 3×2×100 = 600 combinaciones posibles\n\n# Configuramos RandomizedSearchCV\nsearch = RandomizedSearchCV(reg_log,\n                           reg_log_param,\n                           n_iter = 50,  # Solo probamos 50 combinaciones aleatorias (en lugar de 600)\n                           scoring='accuracy',  # Métrica de evaluación\n                           n_jobs=-1,  # Usa todos los núcleos\n                           cv=10)  # Validación cruzada con 10 folds\n\n# Ejecutamos la búsqueda aleatoria\nresult = search.fit(X_train, y_train)\n\n# Mostramos los resultados\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)\nprint('Best Estimator: %s' % result.best_estimator_)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos nuevamente los resultados del RandomizedSearchCV\n# Best Score: El mejor accuracy obtenido en validación cruzada\nprint('Best Score: %s' % result.best_score_)\n\n# Best Hyperparameters: Los mejores hiperparámetros encontrados aleatoriamente\nprint('Best Hyperparameters: %s' % result.best_params_)\n\n# Best Estimator: El pipeline completo con los mejores hiperparámetros\nprint('Best Estimator: %s' % result.best_estimator_)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Código comentado: Ejemplo de cómo crear un DataFrame con metadatos del modelo\n# Esto sería útil para documentar y comparar diferentes modelos en un proyecto\n\n# pd.DataFrame({\"modelo\":filename_model,\n#             \"notebook\":notebook_name,\n#             \"accuracy\":accuracy})"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}