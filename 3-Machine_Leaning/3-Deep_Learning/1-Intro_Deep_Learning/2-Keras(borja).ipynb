{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n",
    "# ============================================\n",
    "\n",
    "# import tensorflow as tf  # TensorFlow es el framework base\n",
    "from tensorflow import keras  # Keras es la API de alto nivel de TensorFlow\n",
    "from tensorflow.keras import layers  # Módulo de capas para construir redes neuronales\n",
    "\n",
    "import pandas as pd  # Para manipulación y análisis de datos\n",
    "import numpy as np  # Para operaciones numéricas y manejo de arrays"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGA DEL DATASET MNIST\n",
    "# ============================================\n",
    "# MNIST es un dataset clásico de dígitos escritos a mano (0-9)\n",
    "# Cada imagen es de 28x28 píxeles en escala de grises\n",
    "\n",
    "# Keras incluye datasets populares que podemos cargar directamente\n",
    "# load_data() retorna 4 arrays: X_train, y_train, X_test, y_test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPLORACIÓN DE LAS DIMENSIONES DEL DATASET\n",
    "# ============================================\n",
    "'''\n",
    "Resultado esperado:\n",
    "- X_train: 60.000 imágenes de 28x28 píxeles (conjunto de entrenamiento)\n",
    "- y_train: 60.000 etiquetas (los números del 0 al 9)\n",
    "- X_test: 10.000 imágenes de prueba\n",
    "- y_test: 10.000 etiquetas de prueba\n",
    "'''\n",
    "\n",
    "# Shape de los datos de entrenamiento\n",
    "print(X_train.shape)  # (60000, 28, 28)\n",
    "print(y_train.shape)  # (60000,)\n",
    "\n",
    "# Shape de los datos de prueba\n",
    "print(X_test.shape)   # (10000, 28, 28)\n",
    "print(y_test.shape)   # (10000,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE UNA IMAGEN DEL DATASET\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mostramos la primera imagen del conjunto de entrenamiento\n",
    "# cmap='Greys' muestra la imagen en escala de grises\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel representa una escala de grises que va del 0 al 255. Siendo 0 el blanco y 255 negro.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NORMALIZACIÓN DE LOS DATOS (FEATURE SCALING)\n",
    "# ============================================\n",
    "# Los valores de los píxeles van de 0 a 255\n",
    "# Dividimos entre 255 para normalizar los valores entre 0 y 1\n",
    "# Esto ayuda a que la red neuronal aprenda más rápido y mejor\n",
    "\n",
    "# Convertimos a float32 para mayor precisión y dividimos por 255\n",
    "X_train = X_train.astype(\"float32\") / 255  # Normalización del conjunto de entrenamiento\n",
    "X_test = X_test.astype(\"float32\") / 255    # Normalización del conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFICACIÓN DE LA NORMALIZACIÓN\n",
    "# ============================================\n",
    "# Comprobamos que ahora los valores están entre 0 y 1\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONVERSIÓN DE ETIQUETAS A FLOAT32\n",
    "# ============================================\n",
    "# Convertimos las etiquetas (y) a float32 por consistencia\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREACIÓN DEL CONJUNTO DE VALIDACIÓN\n",
    "# ============================================\n",
    "# Separamos 10.000 muestras del conjunto de entrenamiento para validación\n",
    "# La validación se usa durante el entrenamiento para evaluar el modelo\n",
    "# en cada epoch y detectar overfitting\n",
    "\n",
    "# Últimas 10.000 muestras para validación\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "# Las primeras 50.000 muestras quedan para entrenamiento\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFICACIÓN DE LAS DIMENSIONES FINALES\n",
    "# ============================================\n",
    "print(X_train.shape)  # (50000, 28, 28) - Entrenamiento\n",
    "print(X_val.shape)    # (10000, 28, 28) - Validación\n",
    "print(X_test.shape)   # (10000, 28, 28) - Prueba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONSTRUCCIÓN DE LA ARQUITECTURA DE LA RED NEURONAL\n",
    "# ============================================\n",
    "# Vamos a crear una red neuronal secuencial (capa por capa)\n",
    "\n",
    "# Inicializamos el modelo secuencial\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# CAPA DE ENTRADA (Flatten)\n",
    "# Convierte la matriz 2D (28x28) en un vector 1D de 784 elementos\n",
    "# Esto es necesario porque las capas Dense requieren entrada 1D\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# PRIMERA CAPA OCULTA (Hidden Layer 1)\n",
    "# 300 neuronas con función de activación ReLU\n",
    "# ReLU (Rectified Linear Unit): f(x) = max(0, x)\n",
    "# Es rápida de calcular y evita el problema de gradientes que desaparecen\n",
    "model.add(keras.layers.Dense(units=300,\n",
    "                              activation='relu'))\n",
    "\n",
    "# SEGUNDA CAPA OCULTA (Hidden Layer 2)\n",
    "# 100 neuronas con función de activación ReLU\n",
    "# Al tener menos neuronas, va reduciendo la dimensionalidad\n",
    "model.add(keras.layers.Dense(units=100,\n",
    "                              activation='relu'))\n",
    "\n",
    "# CAPA DE SALIDA (Output Layer)\n",
    "# 10 neuronas (una por cada dígito del 0 al 9)\n",
    "# Softmax: convierte los valores en probabilidades que suman 1\n",
    "# Ideal para problemas de clasificación multiclase\n",
    "model.add(keras.layers.Dense(units=10,\n",
    "                              activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FORMA ALTERNATIVA DE DECLARAR LA RED NEURONAL\n",
    "# ============================================\n",
    "# Esta forma es más compacta y legible cuando tenemos muchas capas\n",
    "\n",
    "# Definimos todas las capas en una lista\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # Capa de entrada\n",
    "    keras.layers.Dense(units=300, activation='relu'),  # Hidden layer 1\n",
    "    keras.layers.Dense(units=100, activation='relu'),  # Hidden layer 2\n",
    "    keras.layers.Dense(units=10, activation='softmax')  # Capa de salida\n",
    "]\n",
    "\n",
    "# Creamos el modelo pasando la lista de capas directamente\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# INSPECCIÓN DE LAS CAPAS DEL MODELO\n",
    "# ============================================\n",
    "# Podemos acceder a cada capa individualmente usando índices\n",
    "print(model.layers[0])  # Primera capa (Flatten)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE PESOS Y SESGOS INICIALES\n",
    "# ============================================\n",
    "# Keras inicializa los pesos aleatoriamente y los sesgos (bias) en 0\n",
    "# Esto ocurre antes del entrenamiento\n",
    "\n",
    "# Accedemos a la primera capa oculta (índice 1)\n",
    "hidden1 = model.layers[1]\n",
    "\n",
    "# get_weights() retorna una tupla: (weights, biases)\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la matriz de pesos (valores aleatorios iniciales)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número total de pesos en esta capa\n",
    "# Debería ser: 784 (entradas) × 300 (neuronas) = 235,200\n",
    "weights.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPILACIÓN DEL MODELO (FORMA DETALLADA)\n",
    "# ============================================\n",
    "# La compilación configura el proceso de aprendizaje del modelo\n",
    "\n",
    "model.compile(\n",
    "    # OPTIMIZADOR: SGD (Stochastic Gradient Descent)\n",
    "    # Actualiza los pesos usando el gradiente descendente estocástico\n",
    "    optimizer=keras.optimizers.SGD(),\n",
    "    \n",
    "    # FUNCIÓN DE PÉRDIDA: Sparse Categorical Crossentropy\n",
    "    # Mide el error entre las predicciones y las etiquetas reales\n",
    "    # \"Sparse\" porque las etiquetas son enteros (0-9), no one-hot encoded\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # MÉTRICA: Accuracy (precisión)\n",
    "    # Porcentaje de predicciones correctas durante el entrenamiento\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPILACIÓN DEL MODELO (FORMA SIMPLIFICADA)\n",
    "# ============================================\n",
    "# Equivalente al anterior pero usando nombres de string\n",
    "# Esta forma es más corta y comúnmente usada\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"sgd\",  # Optimizador SGD\n",
    "    loss=\"sparse_categorical_crossentropy\",  # Función de pérdida\n",
    "    metrics=[\"accuracy\"]  # Métrica de evaluación\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RESUMEN DEL MODELO\n",
    "# ============================================\n",
    "# Muestra la arquitectura completa: capas, shapes y parámetros\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el shape de los datos de entrenamiento\n",
    "X_train.shape  # (50000, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ENTRENAMIENTO DEL MODELO\n",
    "# ============================================\n",
    "# Aquí es donde la red neuronal \"aprende\" ajustando sus pesos\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,  # Datos de entrada (imágenes)\n",
    "    y_train,  # Etiquetas (targets)\n",
    "    \n",
    "    # BATCH_SIZE: número de muestras procesadas antes de actualizar pesos\n",
    "    # Un batch más pequeño = más actualizaciones pero más lento\n",
    "    batch_size=128,\n",
    "    \n",
    "    # EPOCHS: número de veces que el modelo ve todo el dataset\n",
    "    # Más epochs = más aprendizaje, pero riesgo de overfitting\n",
    "    epochs=50,\n",
    "    \n",
    "    # VALIDATION_DATA: datos para evaluar el modelo en cada epoch\n",
    "    # Permite monitorear si hay overfitting (loss de validación aumenta)\n",
    "    validation_data=(X_val, y_val)  # Alternativa: validation_split=0.1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# REENTRENAMIENTO DEL MODELO\n",
    "# ============================================\n",
    "# Podemos seguir entrenando un modelo ya entrenado\n",
    "# NO empieza desde cero, continúa desde donde se quedó\n",
    "# Útil para hacer ajustes finos (fine-tuning)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,  # Probamos con un batch diferente\n",
    "    epochs=10,  # Solo 10 epochs adicionales\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXPLORACIÓN DEL HISTÓRICO DE ENTRENAMIENTO\n",
    "# ============================================\n",
    "# El objeto history contiene métricas de cada epoch\n",
    "# print(history.params)  # Parámetros del entrenamiento\n",
    "# print(history.epoch)   # Número de epochs ejecutadas\n",
    "print(history.history)  # Diccionario con loss, accuracy, val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos el diccionario completo del histórico\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos las métricas disponibles en el histórico\n",
    "history.history.keys()  # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos el histórico a DataFrame para visualizarlo mejor\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DEL PROCESO DE ENTRENAMIENTO\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficamos todas las métricas (loss, accuracy, val_loss, val_accuracy)\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "\n",
    "# Configuramos la gráfica\n",
    "plt.grid(True)  # Añadimos cuadrícula\n",
    "plt.gca().set_ylim(0, 1)  # Rango vertical de 0 a 1\n",
    "\n",
    "# Interpretación:\n",
    "# - Si val_loss aumenta mientras loss disminuye = OVERFITTING\n",
    "# - Si ambas disminuyen = el modelo está aprendiendo bien\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUACIÓN DEL MODELO CON DATOS DE PRUEBA\n",
    "# ============================================\n",
    "# Evaluamos el modelo con datos que NUNCA ha visto durante el entrenamiento\n",
    "# Esto nos da una idea del rendimiento real del modelo\n",
    "\n",
    "# evaluate() retorna [loss, accuracy]\n",
    "results = model.evaluate(X_test, y_test)\n",
    "results  # [pérdida en test, accuracy en test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE UN EJEMPLO DE PRUEBA\n",
    "# ============================================\n",
    "# Mostramos la primera imagen del conjunto de prueba\n",
    "plt.imshow(X_test[0].reshape(28, 28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos los datos crudos de la primera imagen\n",
    "X_test[:1]  # Array con valores normalizados entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICCIONES DEL MODELO\n",
    "# ============================================\n",
    "# Hacemos una predicción con la primera imagen del test\n",
    "\n",
    "predictions = model.predict(X_test[:1])\n",
    "\n",
    "# Shape de las predicciones: (1, 10)\n",
    "# 1 muestra, 10 probabilidades (una por cada dígito 0-9)\n",
    "print(predictions.shape)\n",
    "\n",
    "# Redondeamos a 3 decimales para ver las probabilidades\n",
    "# La suma de todas las probabilidades debe ser 1 (gracias a softmax)\n",
    "np.round(predictions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OBTENER LA CLASE PREDICHA\n",
    "# ============================================\n",
    "# argmax() retorna el índice del valor más alto\n",
    "# Es decir, el dígito con mayor probabilidad\n",
    "predictions.argmax()  # Dígito predicho (0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICCIONES PARA TODO EL CONJUNTO DE PRUEBA\n",
    "# ============================================\n",
    "# Obtenemos las predicciones para todas las imágenes del test\n",
    "# axis=1 indica que argmax se aplica a cada fila (cada predicción)\n",
    "model.predict(X_test).argmax(axis=1)  # Array con todos los dígitos predichos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la segunda imagen del test\n",
    "plt.imshow(X_test[1].reshape(28, 28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MATRIZ DE CONFUSIÓN\n",
    "# ============================================\n",
    "# Muestra los errores del modelo: qué dígitos confunde con cuáles\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Filas = etiquetas reales, Columnas = predicciones\n",
    "# Diagonal = predicciones correctas\n",
    "confusion_matrix(y_test, model.predict(X_test).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# REPORTE DE CLASIFICACIÓN\n",
    "# ============================================\n",
    "# Muestra precision, recall, f1-score para cada clase (dígito)\n",
    "# También incluye accuracy global y promedios macro/weighted\n",
    "print(classification_report(y_test, model.predict(X_test).argmax(axis=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGA DEL DATASET CALIFORNIA HOUSING\n",
    "# ============================================\n",
    "# Dataset de regresión: predecir precios de casas en California\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargamos el dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Convertimos a DataFrame para visualizar mejor\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['target'] = housing['target']  # Añadimos la columna objetivo (precio)\n",
    "\n",
    "df.head()  # Primeras 5 filas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIVISIÓN Y NORMALIZACIÓN DE DATOS\n",
    "# ============================================\n",
    "\n",
    "# PRIMERA DIVISIÓN: Train completo (75%) y Test (25%)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data,\n",
    "    housing.target\n",
    ")\n",
    "\n",
    "# SEGUNDA DIVISIÓN: Train (75% de 75%) y Validation (25% de 75%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full\n",
    ")\n",
    "\n",
    "# ESTANDARIZACIÓN (StandardScaler)\n",
    "# Transforma los datos para tener media=0 y desviación estándar=1\n",
    "# IMPORTANTE: fit_transform solo en train, transform en valid/test\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Aprende y transforma\n",
    "X_valid = scaler.transform(X_valid)      # Solo transforma\n",
    "X_test = scaler.transform(X_test)        # Solo transforma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el shape de los datos de entrenamiento\n",
    "# Debería mostrar (num_muestras, 8) porque hay 8 características\n",
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MODELO DE REGRESIÓN CON KERAS\n",
    "# ============================================\n",
    "\n",
    "# Creamos una red neuronal simple para regresión\n",
    "model = keras.models.Sequential([\n",
    "    # CAPA OCULTA\n",
    "    # 30 neuronas con activación ReLU\n",
    "    # input_shape=[8] porque tenemos 8 características\n",
    "    # X_train.shape[1:] es otra forma de especificar las dimensiones\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape=X_train.shape[1:]),\n",
    "    \n",
    "    # CAPA DE SALIDA\n",
    "    # 1 neurona SIN función de activación (regresión lineal)\n",
    "    # Predice un valor continuo (precio de la casa)\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# COMPILACIÓN\n",
    "# loss='mean_squared_error': función de pérdida para regresión\n",
    "# Mide la distancia cuadrática entre predicción y valor real\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=\"sgd\")\n",
    "\n",
    "# ENTRENAMIENTO\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RESUMEN DEL MODELO DE REGRESIÓN\n",
    "# ============================================\n",
    "model.summary()\n",
    "# Total params = (8 inputs × 30 neurons) + 30 bias + (30 × 1) + 1 bias = 271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EVALUACIÓN DEL MODELO DE REGRESIÓN\n",
    "# ============================================\n",
    "# Calculamos el MSE (Mean Squared Error) en el conjunto de test\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)  # Cuanto menor sea el MSE, mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PREDICCIONES DEL MODELO DE REGRESIÓN\n",
    "# ============================================\n",
    "# Predecimos los precios de las primeras 5 casas del test\n",
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred  # Array con los precios predichos (valores continuos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GUARDAR EL MODELO\n",
    "# ============================================\n",
    "# Guardamos el modelo completo (arquitectura + pesos + configuración)\n",
    "# Formato .keras es el recomendado (antes se usaba .h5)\n",
    "model.save(\"my_keras_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGAR UN MODELO GUARDADO\n",
    "# ============================================\n",
    "# Cargamos el modelo completo desde el archivo\n",
    "# Podemos usar este modelo directamente para predicciones\n",
    "model = keras.models.load_model(\"my_keras_model.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CALLBACK: MODEL CHECKPOINT\n",
    "# ============================================\n",
    "# Guarda el modelo automáticamente después de cada epoch\n",
    "# Útil para no perder progreso si se interrumpe el entrenamiento\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "\n",
    "# El modelo se guardará en \"callback_model.h5\" tras cada epoch\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    callbacks=[checkpoint_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CALLBACK: EARLY STOPPING\n",
    "# ============================================\n",
    "# Detiene el entrenamiento automáticamente cuando no hay mejora\n",
    "# Previene overfitting y ahorra tiempo de entrenamiento\n",
    "\n",
    "# patience=3: espera 3 epochs sin mejora antes de detener\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3)\n",
    "\n",
    "# Podemos combinar múltiples callbacks en una lista\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,  # Máximo 50 epochs\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    # Se detendrá antes si val_loss no mejora durante 3 epochs\n",
    "                    callbacks=[early_stopping_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
