{"cells":[{"cell_type":"markdown","source":["# RDD creation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ca24a700-f10c-4c9b-81f7-fa2e6595fc95","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### [Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e49600cc-756b-42f9-9659-24d1be46e64f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["In this notebook we will introduce two different ways of getting data into the basic Spark data structure, the **Resilient Distributed Dataset** or **RDD**. An RDD is a distributed collection of elements. All work in Spark is expressed as either creating new RDDs, transforming existing RDDs, or calling actions on RDDs to compute a result. Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0cec11ce-b693-4e81-a7b3-8c07cfca153a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["#### References"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aee3870e-8985-4200-a46a-8296fff0c2e2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The reference book for these and other Spark related topics is *Learning Spark* by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"427d7286-aec0-4cf3-9abe-80d4fbe83d59","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The KDD Cup 1999 competition dataset is described in detail [here](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bb702640-703b-4fb7-be49-b43a53ede5d7","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## SparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ed6bed1-485c-44f5-b6e6-766e13894241","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import pyspark\n\nsc = spark.sparkContext # spark es el Sparksession"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c911c36b-8c99-4c83-bf3d-0c844ed549ce","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sc"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"85f08f34-4051-4e00-94c2-2071357563f1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=5028638602737485#setting/sparkui/1206-134243-tyq9uien/driver-7689348177991462143\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=5028638602737485#setting/sparkui/1206-134243-tyq9uien/driver-7689348177991462143\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["## Getting the data files"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bfff9866-d435-41f4-8f45-33b228d52054","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["In this notebook we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a *Gzip* file that we will download locally."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69a58919-999e-44f6-89f9-470eb9c493f2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\nfrom pyspark import SparkFiles\nspark.sparkContext.addFile(url)"],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d44040e-b605-438f-810b-3be8e0631aad","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["myRDD = sc.textFile(\"file://\"+SparkFiles.get(\"kddcup.data_10_percent.gz\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"52a643c5-84c8-4e00-ae1d-708013819a05","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["SparkFiles"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e09a6cf-8e64-481b-a48f-32e3af7d4a69","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[8]: pyspark.files.SparkFiles","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[8]: pyspark.files.SparkFiles"]}}],"execution_count":0},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/databricks-datasets/COVID/USAFacts/\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75a8433a-5cae-40a3-88ac-f8d423faa853","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/databricks-datasets/COVID/USAFacts/USAFacts_readme.md","USAFacts_readme.md",2507,1615898752000],["dbfs:/databricks-datasets/COVID/USAFacts/covid_confirmed_usafacts.csv","covid_confirmed_usafacts.csv",4939138,1615898753000],["dbfs:/databricks-datasets/COVID/USAFacts/covid_deaths_usafacts.csv","covid_deaths_usafacts.csv",3397903,1615898753000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/USAFacts/USAFacts_readme.md</td><td>USAFacts_readme.md</td><td>2507</td><td>1615898752000</td></tr><tr><td>dbfs:/databricks-datasets/COVID/USAFacts/covid_confirmed_usafacts.csv</td><td>covid_confirmed_usafacts.csv</td><td>4939138</td><td>1615898753000</td></tr><tr><td>dbfs:/databricks-datasets/COVID/USAFacts/covid_deaths_usafacts.csv</td><td>covid_deaths_usafacts.csv</td><td>3397903</td><td>1615898753000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[" myRDD.take(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce6d83fc-0056-49c1-85ef-26a4336b011c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[10]: ['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[10]: ['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,219,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,39,39,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,217,2032,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,6,6,0.00,0.00,0.00,0.00,1.00,0.00,0.00,49,49,1.00,0.00,0.02,0.00,0.00,0.00,0.00,0.00,normal.']"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Creating a RDD from a file"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"393a0328-afd5-4edd-8719-d7278c84e6e1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The most common way of creating an RDD is to load it from a file. Notice that Spark's `textFile` can handle compressed files directly."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff6bc0dc-3818-4475-ba81-6cf04fd90b55","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["type(myRDD)"],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"905133c3-87ba-424a-9175-24d6171a24fd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[14]: pyspark.rdd.RDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[14]: pyspark.rdd.RDD"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now we have our data file loaded into the `raw_data` RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6220357f-c846-4a40-99cc-2ea24cc28cb4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Without getting into Spark *transformations* and *actions*, the most basic thing we can do to check that we got our RDD contents right is to `count()` the number of lines loaded from the file into the RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d785721f-af07-49c5-921b-b4bdf9098033","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["myRDD.count() #Cantidad de filas"],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d4308611-22c4-4ea4-9a33-3a4379388c92","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[16]: 494021","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: 494021"]}}],"execution_count":0},{"cell_type":"markdown","source":["We can also check the first few entries in our data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b495291-d729-413c-8f46-8e1f7ce6941e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["myRDD.take(2)"],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bacd9309-ab70-47cd-b4cc-2536d03f5ca5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[17]: ['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.']","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[17]: ['0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n '0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.']"]}}],"execution_count":0},{"cell_type":"markdown","source":["In the following notebooks, we will use this raw data to learn about the different Spark transformations and actions."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6255eb59-a813-4094-a93a-d5e031163597","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Creating and RDD using `parallelize`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e4239a0f-ecd3-4e06-8c50-1f03535965a6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Another way of creating an RDD is to parallelize an already existing list."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1ed0cd4-62bd-4152-a9f6-d4b43b4512fa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["a = range(100)\ndata = sc.parallelize(a) # leer del range y me va a devolver un RDD, cuantas particiones? Cuando no digo nada, depende de los cores\ntype(data) # todo esto es lazing, se lo ha apuntado, no se ha ejecutado."],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a5331cd-9265-495a-96c1-23e25d33147d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[18]: pyspark.rdd.PipelinedRDD","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[18]: pyspark.rdd.PipelinedRDD"]}}],"execution_count":0},{"cell_type":"markdown","source":["As we did before, we can `count()` the number of elements in the RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7986616-230a-4d09-95fc-b1fd1a0b4be1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data.count()"],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d6155408-9c2b-4038-98ec-8b24b0795481","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[19]: 100","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[19]: 100"]}}],"execution_count":0},{"cell_type":"markdown","source":["As before, we can access the first few elements on our RDD."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eacf4df9-1849-4c2f-98e6-2c1b6cb3f999","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data.take(5)"],"metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e02238e-7fd6-480a-9bce-303e96880bb0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[20]: [0, 1, 2, 3, 4]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[20]: [0, 1, 2, 3, 4]"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Get data and partitions"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cdac7b8a-e6a8-45af-ba09-ab7c6e439dfd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["rddCollect = data.collect() # data RDD, resilient distribuited dataset, es una estructura. Con collect le todas las particiones y se meten en el nodo dirver, y se puede da√±ar.\nprint(\"Number of Partitions: \" + str(data.getNumPartitions()))\nprint(\"Action: First element: \" + str(data.first()))\nprint(rddCollect)\n\n'''\ndata = sc.parallelize(a, p) # hace p particiones\n'''"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0b6169e4-4fce-411e-b77f-10e64003a00c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Number of Partitions: 8\nAction: First element: 0\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\nOut[21]: '\\ndata = sc.parallelize(a, p) # hace p particiones\\n'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Number of Partitions: 8\nAction: First element: 0\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\nOut[21]: '\\ndata = sc.parallelize(a, p) # hace p particiones\\n'"]}}],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"1-rdd-creation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3550433515547940}},"nbformat":4,"nbformat_minor":0}
