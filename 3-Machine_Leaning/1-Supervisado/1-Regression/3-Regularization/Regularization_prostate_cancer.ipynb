{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicción de cáncer de próstata\n",
    "\n",
    "#### Datos\n",
    "\n",
    "Consideramos un estudio médico realizado en 97 hombres con cáncer de próstata.\n",
    "El enfoque está en la relación entre el antígeno prostático específico (psa), que está elevado en hombres con cáncer de próstata, y otras medidas clínicas. \n",
    "Las otras medidas clínicas son las variables predictoras, recopiladas en un examen médico, y la cantidad de expresión del antígeno asociado con la detección del cáncer es la variable de respuesta (lpsa).\n",
    "\n",
    "Por lo tanto, el marco de datos está compuesto por 97 observaciones sobre 9 variables:\n",
    "* lcavol: logaritmo del volumen del cáncer\n",
    "* lweight: logaritmo del peso de la próstata\n",
    "* age: edad del paciente en años\n",
    "* lbph: logaritmo de la cantidad de hiperplasia prostática benigna\n",
    "* svi: invasión de vesícula seminal\n",
    "* lcp: logaritmo de la penetración capsular\n",
    "* gleason: puntuación de Gleason\n",
    "* pgg45: porcentaje de puntuación de Gleason 4 o 5\n",
    "* lpsa: logaritmo del antígeno prostático específico\n",
    "\n",
    "El objetivo es encontrar modelos que predigan la respuesta lpsa.\n",
    "\n",
    "#### Modelos\n",
    "\n",
    "Los datos están representados por $n$ puntos en $p$ dimensiones, por lo tanto la variable predictora se escribe $X\\in\\mathbb{R}^{n\\times p}$ y la variable de respuesta es $y\\in\\mathbb{R}^n$.\n",
    "\n",
    "En este trabajo, estamos interesados en la relación entre el predictor $X$ y la respuesta $y$.\n",
    "Para determinar esta relación, adoptamos modelos de regresión.\n",
    "La línea base estándar se logra con regresión lineal y comparamos resultados para regresiones regularizadas: **Regresión Ridge**, **Lasso** y **Elastic Net**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, metrics, model_selection\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga el dataset desde un archivo de texto separado por tabuladores\n",
    "lpsa_data = pd.read_csv('data/prostate_dataset.txt', delimiter='\\t')\n",
    "\n",
    "# Selecciona todas las columnas a partir de 'lcavol' hasta el final del dataset\n",
    "lpsa_data = lpsa_data.loc[:, 'lcavol':]\n",
    "\n",
    "# Muestra las primeras 5 filas del dataset resultante\n",
    "lpsa_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(lpsa_data);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lpsa está casi distribuido normalmente.\n",
    "* la presencia de svi es binaria\n",
    "* lcp: debido a mediciones no apropiadas, para valores pequeños de penetración capsular, se ha establecido arbitrariamente en -1.25.\n",
    "* gleason y pgg45 no parecen estar correlacionados...\n",
    "\n",
    "Veamos la correlación entre las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODE\n",
    "# sns.diverging_palette(145, 280, s=85, l=25, n=7)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.heatmap(lpsa_data.corr(),\n",
    "           vmin=-1,\n",
    "           vmax=1,\n",
    "           cmap=sns.diverging_palette(145, 280, s=85, l=25, n=7),\n",
    "           annot=True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La variable más correlacionada con la respuesta lpsa es lcavol.\n",
    "  Por lo tanto, en un análisis de datos, la variable lcavol debe incluirse como predictor.\n",
    "\n",
    "* La matriz de correlación muestra que gleason y pgg45 están efectivamente correlacionadas. \n",
    "  De hecho, la variable pgg45 mide el porcentaje de puntuaciones de Gleason 4 o 5 que se registraron antes de la puntuación actual final de Gleason.\n",
    "\n",
    "Grafiquemos la relación entre la respuesta lpsa y la característica lcavol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(lpsa_data['lcavol'], lpsa_data['lpsa'])\n",
    "plt.xlabel('lcavol', fontsize=16)\n",
    "plt.ylabel('lpsa', fontsize=16)\n",
    "plt.title(\"Relationship between lpsa and lcavol\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay una relación lineal bastante clara con correlación positiva, como se ve en la matriz de correlación."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### División de datos entrenamiento/prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : the first rows \n",
    "# test : the last rows \n",
    "n_split = 60\n",
    "\n",
    "# Selecciona las primeras 60 filas como datos de entrenamiento (todas las columnas menos las dos últimas → se excluye 'pgg45')\n",
    "X_train = lpsa_data.iloc[:n_split, 0:-2]\n",
    "\n",
    "# Selecciona el resto de filas como datos de prueba (mismas columnas que en entrenamiento)\n",
    "X_test = lpsa_data.iloc[n_split:, 0:-2]\n",
    "\n",
    "# Selecciona la columna 'lpsa' (última del dataset) como variable objetivo en entrenamiento\n",
    "y_train = lpsa_data.iloc[:n_split, -1]\n",
    "\n",
    "# Selecciona la columna 'lpsa' para el conjunto de prueba\n",
    "y_test = lpsa_data.iloc[n_split:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de aprendizaje\n",
    "\n",
    "# 1) Baseline de regresión lineal\n",
    "\n",
    "La regresión lineal intenta modelar la relación entre las variables predictoras $X$ y la variable de respuesta $y$.\n",
    "Consiste en encontrar una función lineal $f:\\mathbb{R}^p \\to \\mathbb{R}$ que prediga la respuesta $y_i$ a partir de los predictores $X_{i1},...,X_{ip}$ dadas $n$ observaciones para $i=1,...,n$.\n",
    "\n",
    "En Python, la regresión lineal está implementada como [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) en el módulo linear_model de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "baseline_error = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Train MSE:\", metrics.mean_squared_error(y_train, lr.predict(X_train)))\n",
    "print(\"Test MSE:\", metrics.mean_squared_error(y_test, lr.predict(X_test)))\n",
    "print(\"Test RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, lr.predict(X_test))))\n",
    "print(\"Test R2:\", metrics.r2_score(y_test, lr.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretación\n",
    "1. **Train MSE (0.288) vs Test MSE (2.864)**  \n",
    "   - En entrenamiento el error es bajo.  \n",
    "   - En prueba el error es casi **10 veces mayor** → hay un **claro sobreajuste** (el modelo aprendió demasiado los datos de train y no generaliza).  \n",
    "\n",
    "2. **Test RMSE (1.692)**  \n",
    "   - Significa que, en promedio, las predicciones se equivocan por ~1.7 unidades en la escala de `lpsa`.  \n",
    "   - Para este dataset, ese error es bastante grande.  \n",
    "\n",
    "3. **Test R² (-4.64)**  \n",
    "   - Un valor negativo implica que el modelo predice **peor que si simplemente tomaras la media de `y_test` como predicción constante**.  \n",
    "   - Esto es una fuerte señal de que el modelo no está funcionando bien.  \n",
    "\n",
    "###  Conclusión\n",
    "Sí, estos números son **malos**. Tu modelo **se ajusta demasiado al train** y **fracasa en test**.  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Regularización\n",
    "\n",
    "Para evitar el sobreaprendizaje, el método de regularización permite controlar la complejidad del modelo.\n",
    "El modelo minimiza el error más un término de regularización $\\lambda Reg(\\beta)$ que mide la complejidad, donde $Reg(\\beta)$ es un término de penalización y $\\lambda$ es un hiperparámetro.\n",
    "El hiperparámetro controla la influencia relativa del término de error y la cantidad de regularización.\n",
    "El valor óptimo de $\\lambda$ se puede encontrar mediante validación cruzada (ver repositorio [cross-validation](https://github.com/christelle-git/cross-validation/)). \n",
    "\n",
    "## 2.1) Regresión Ridge \n",
    "\n",
    "En la regresión Ridge, el término de regularización es $Reg(\\beta) = ||\\beta||_2^2$.\n",
    "La regresión Ridge permite reducir la magnitud de los pesos $\\beta_i$ de la regresión lineal, y así evitar el sobreaprendizaje.\n",
    "La regresión Ridge tiene un efecto de selección agrupada: las variables correlacionadas tienen los mismos pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Crea un modelo de regresión Ridge con alpha=10 (regularización L2)\n",
    "ridgeR = Ridge(alpha=10)\n",
    "\n",
    "# Ajusta el modelo con los datos de entrenamiento\n",
    "ridgeR.fit(X_train, y_train)\n",
    "\n",
    "# Compara el modelo sin regularización (LinearRegression) vs con regularización (Ridge)\n",
    "\n",
    "# Error cuadrático medio (MSE) en entrenamiento sin regularización\n",
    "print(\"Train MSE sin regularización:\", round(metrics.mean_squared_error(y_train, lr.predict(X_train)), 2))\n",
    "\n",
    "# Error cuadrático medio (MSE) en prueba sin regularización\n",
    "print(\"Test MSE sin regularización:\", round(metrics.mean_squared_error(y_test, lr.predict(X_test)), 2))\n",
    "\n",
    "# Error cuadrático medio (MSE) en entrenamiento con Ridge\n",
    "print(\"Train MSE con regularización:\", round(metrics.mean_squared_error(y_train, ridgeR.predict(X_train)), 2))\n",
    "\n",
    "# Error cuadrático medio (MSE) en prueba con Ridge\n",
    "print(\"Test MSE con regularización:\", round(metrics.mean_squared_error(y_test, ridgeR.predict(X_test)), 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Para preparar un rango de valores de `alpha`** (el hiperparámetro de regularización en Ridge o Lasso) para que después los uses en un bucle, validación cruzada o directamente con `RidgeCV` / `LassoCV`. Se hce de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de valores de alpha que queremos generar\n",
    "n_alphas = 100\n",
    "\n",
    "# Genera 100 valores de alpha entre 10^-4 (0.0001) y 10^3 (1000) en escala logarítmica\n",
    "alphas = np.logspace(-4, 3, n_alphas) \n",
    "\n",
    "# Muestra el valor mínimo del rango de alphas (debería ser 0.0001)\n",
    "print(np.min(alphas))\n",
    "\n",
    "# Muestra el valor máximo del rango de alphas (debería ser 1000.0)\n",
    "print(np.max(alphas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de valores de alpha a generar\n",
    "n_alphas = 100\n",
    "\n",
    "# Genera 100 valores de alpha en escala logarítmica entre 10^-4 y 10^3\n",
    "alphas = np.logspace(-4, 3, n_alphas) \n",
    "\n",
    "# Listas vacías para guardar resultados\n",
    "coef_ridge = []   # guardará los coeficientes de cada modelo Ridge\n",
    "err_ridge = []    # guardará el error MSE en test para cada alpha\n",
    "baseline = []     # guardará el error baseline (modelo inicial sin regularización)\n",
    "\n",
    "# Itera sobre cada valor de alpha\n",
    "for a in alphas:\n",
    "    # Crea un modelo Ridge con el alpha iterado a \n",
    "    ridge = Ridge(alpha=a) \n",
    "    \n",
    "    # Ajusta el modelo con los datos de entrenamiento\n",
    "    ridge.fit(X_train, y_train)\n",
    "    \n",
    "    # Guarda los coeficientes del modelo entrenado\n",
    "    coef_ridge.append(ridge.coef_)\n",
    "    \n",
    "    # Predice en el conjunto de prueba\n",
    "    y_pred = ridge.predict(X_test)\n",
    "    \n",
    "    # Calcula el error MSE en test y lo guarda\n",
    "    ridge_error = metrics.mean_squared_error(y_pred, y_test)\n",
    "    err_ridge.append(ridge_error)\n",
    "    \n",
    "    # Guarda el baseline (error del modelo inicial sin regularización)\n",
    "    baseline.append(baseline_error) \n",
    "    \n",
    "#En resumen:\n",
    "\n",
    "#Estás probando 100 valores de alpha distintos en Ridge.\n",
    "\n",
    "#Guardas los coeficientes de cada modelo y sus errores en test (MSE).\n",
    "\n",
    "#También repites el baseline para poder comparar después en un gráfico.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficar alpha vs error MSE en test\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(alphas, err_ridge, label=\"Ridge MSE\", marker=\"o\")\n",
    "plt.plot(alphas, baseline, label=\"Baseline MSE\", linestyle=\"--\")\n",
    "\n",
    "# Escala logarítmica en el eje X (porque alpha se generó en logspace)\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "plt.xlabel(\"Alpha (log scale)\")\n",
    "plt.ylabel(\"MSE en test\")\n",
    "plt.title(\"Ridge Regression: Alpha vs MSE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression: Alpha vs MSE\n",
    "\n",
    "Este gráfico muestra cómo varía el **error cuadrático medio (MSE) en test** al cambiar el parámetro de regularización **α (alpha)** en una **regresión Ridge**.\n",
    "\n",
    "- **Eje X (Alpha log scale):**  \n",
    "  Representa los valores de α en escala logarítmica.  \n",
    "  - Valores pequeños (≈ `10^-4`) se parecen a una regresión lineal sin regularización.  \n",
    "  - Valores grandes (≈ `10^3`) implican una regularización muy fuerte, que puede empeorar el desempeño.\n",
    "\n",
    "- **Eje Y (MSE en test):**  \n",
    "  Indica el **error cuadrático medio** en el conjunto de prueba.  \n",
    "  Un valor más bajo significa mejor rendimiento del modelo.\n",
    "\n",
    "- **Curva azul (Ridge MSE):**  \n",
    "  Muestra el MSE del modelo Ridge para distintos valores de α.  \n",
    "  - El error disminuye al aumentar α hasta cierto punto (óptimo alrededor de `α ≈ 10`).  \n",
    "  - Luego, el error vuelve a crecer cuando la regularización es excesiva.\n",
    "\n",
    "- **Línea naranja discontinua (Baseline MSE):**  \n",
    "  Es la referencia del MSE de un modelo base (ejemplo: regresión lineal sin regularización).  \n",
    "  Sirve para comparar si la regresión Ridge realmente mejora el rendimiento.\n",
    "\n",
    "\n",
    "**Conclusión:**  \n",
    "Existe un valor óptimo de α (aprox. en torno a `10`) que **minimiza el error en test**. La regularización Ridge ayuda a mejorar el desempeño respecto al modelo base, evitando tanto el sobreajuste como el subajuste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge \n",
    "  \n",
    "# Train the model with alpha = 10 \n",
    "ridgeR = Ridge(alpha = 10) \n",
    "ridgeR.fit(X_train, y_train) \n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, ridgeR.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, ridgeR.predict(X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El valor óptimo del coeficiente de regularización $\\lambda$ está alrededor de 10.\n",
    "* Para $\\lambda \\to 0$ el término de regularización desaparece llevando al mismo resultado que la regresión lineal.\n",
    "* Para $\\lambda \\to \\infty$ la magnitud del término de regularización domina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coef_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coef_ridge, linewidth=5)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('weight value', fontsize=30)\n",
    "plt.title('Ridge coefficients paths', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La regresión Ridge restringe algunas variables reduciendo la magnitud de sus pesos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Operador de Selección y Reducción Absoluta Mínima\n",
    "\n",
    "El siguiente método va más allá al seleccionar algunas variables para ser eliminadas de la regresión Ridge, reduciendo así la dimensión.\n",
    "El método se llama Operador de Selección y Reducción Absoluta Mínima (Lasso) y el modelo simplificado resultante es un **modelo disperso** o parsimonioso.\n",
    "En Lasso, el término de regularización se define por $Reg(\\beta) = ||\\beta||_1$.\n",
    "\n",
    "Lasso realiza una selección de características del modelo: para variables correlacionadas, retiene solo una variable y establece otras variables correlacionadas en cero.\n",
    "La contrapartida es que obviamente induce una pérdida de información resultando en menor precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "\n",
    "# Creamos un modelo Lasso con alpha = 0.1 (fuerza de regularización)\n",
    "# Lasso hace regresión lineal con penalización L1 (reduce algunos coeficientes a cero)\n",
    "lassoR = Lasso(alpha=0.1)\n",
    "\n",
    "# Entrenamos el modelo Lasso con los datos de entrenamiento\n",
    "lassoR.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos el modelo base (lr) que sería la regresión lineal sin regularización\n",
    "print(\"Train MSE sin regularización:\", round(metrics.mean_squared_error(y_train, lr.predict(X_train)), 2))\n",
    "print(\"Test MSE sin regularización:\", round(metrics.mean_squared_error(y_test, lr.predict(X_test)), 2))\n",
    "\n",
    "# Evaluamos el modelo Lasso con alpha=0.1\n",
    "# MSE en entrenamiento\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, lassoR.predict(X_train)))\n",
    "\n",
    "# MSE en prueba\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, lassoR.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de MSE: Regresión Lineal vs Lasso (α=0.1)\n",
    "\n",
    "| Modelo                        | Train MSE | Test MSE |\n",
    "|-------------------------------|-----------|----------|\n",
    "| **Sin regularización (lr)**   | 0.29      | 2.86     |\n",
    "| **Con Lasso (α=0.1)**         | 0.3740    | 2.0788   |\n",
    "\n",
    "\n",
    "\n",
    "###  Interpretación\n",
    "- El modelo **sin regularización** ajusta mejor al *train* (0.29), pero se dispara en *test* (2.86) → **sobreajuste**.  \n",
    "- El modelo **con Lasso** sacrifica un poco de ajuste en *train* (0.3740), pero **mejora en test** (2.0788).  \n",
    "- Conclusión: **Lasso generaliza mejor** al reducir el error en datos nuevos.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "# Creamos un modelo Lasso inicial\n",
    "# fit_intercept=False indica que no se añade un término independiente (bias) en la regresión\n",
    "lasso = linear_model.Lasso(fit_intercept=False)\n",
    "\n",
    "# Listas para guardar resultados\n",
    "coef_lasso = []   # Aquí guardaremos los coeficientes del modelo para cada alpha\n",
    "err_lasso = []    # Aquí guardaremos el MSE en test para cada alpha\n",
    "\n",
    "# Recorremos cada valor de alpha\n",
    "for a in alphas:\n",
    "    # Establecemos el valor de alpha actual en el modelo\n",
    "    lasso.set_params(alpha=a)\n",
    "    \n",
    "    # Entrenamos el modelo con los datos de entrenamiento\n",
    "    lasso.fit(X_train, y_train)\n",
    "    \n",
    "    # Guardamos los coeficientes ajustados para este alpha\n",
    "    coef_lasso.append(lasso.coef_)\n",
    "    \n",
    "    # Hacemos predicciones en el conjunto de prueba\n",
    "    y_pred = lasso.predict(X_test)\n",
    "    \n",
    "    # Calculamos el error cuadrático medio en test\n",
    "    lasso_error = metrics.mean_squared_error(y_pred, y_test)\n",
    "    \n",
    "    # Guardamos el error correspondiente a este alpha\n",
    "    err_lasso.append(lasso_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, err_lasso, linewidth=5, color='red', label=\"Lasso\")\n",
    "ax.plot(alphas, baseline, linewidth=4,linestyle='--', color='blue', label='Linear regression')\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('error', fontsize=30)\n",
    "ax.legend(fontsize=30)\n",
    "plt.title(r'Regression error ($\\lambda$)', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoR = Lasso(alpha = 0.02) \n",
    "lassoR.fit(X_train, y_train) \n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, lassoR.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, lassoR.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test MAPE: %0.4f\" % metrics.mean_absolute_percentage_error(y_test, lassoR.predict(X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El valor óptimo del coeficiente de regularización $\\lambda$ está entre $10^{-2}$ y $10^{-1}$.\n",
    "* Para $\\lambda \\to 0$ el término de regularización desaparece por lo que la regresión Lasso tiende a la regresión lineal.\n",
    "* Para $\\lambda \\to \\infty$ la magnitud del término de regularización domina el error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coef_lasso, linewidth=5)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('weight value', fontsize=30)\n",
    "plt.title('Lasso coefficients paths', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Coefficients Paths\n",
    "\n",
    "Este gráfico muestra cómo cambian los **coeficientes (pesos)** de las variables en un modelo de **regresión Lasso** cuando se varía la fuerza de la regularización (λ o alpha).\n",
    "\n",
    "- **Eje X (λ en escala logarítmica):**  \n",
    "  - Representa la intensidad de la regularización.  \n",
    "  - Valores pequeños de λ → poca regularización (parecido a regresión lineal).  \n",
    "  - Valores grandes de λ → mucha regularización (coeficientes se acercan a 0).\n",
    "\n",
    "- **Eje Y (weight value):**  \n",
    "  - Muestra el valor del coeficiente de cada variable.  \n",
    "  - Cada línea de color corresponde a una variable distinta.\n",
    "\n",
    "- **Curvas de colores:**  \n",
    "  - Con poca regularización, los coeficientes tienen valores libres.  \n",
    "  - A medida que λ aumenta, los coeficientes van disminuyendo hacia 0.  \n",
    "  - Algunos coeficientes llegan exactamente a 0 → esas variables son eliminadas del modelo.  \n",
    "  - Esto significa que **Lasso no solo regula la magnitud de los coeficientes, sino que también hace selección de variables**.\n",
    "\n",
    "\n",
    "\n",
    "✅ **Conclusión:**  \n",
    "El gráfico ilustra cómo **Lasso va “apagando” variables progresivamente** al aumentar la regularización, quedándose únicamente con las más importantes y simplificando el modelo.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lasso elimina algunas variables poniendo su peso en cero.<br>\n",
    "  Este es el caso si dos variables están correlacionadas.\n",
    "* Cuando $\\lambda \\to \\infty$ los pesos desaparecen por lo que el modelo se vuelve muy **disperso**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3) Elastic net\n",
    "\n",
    "El método Elastic Net es un híbrido de la regresión Ridge y Lasso, superando así el problema de pérdida de información.\n",
    "El término de regularización combina tanto las regularizaciones $L_1$ como $L_2$.\n",
    "Más precisamente, el término de regularización se establece como $Reg(\\beta) = \\lambda((1-\\alpha)||\\beta||_1+\\alpha||\\beta||_2^2)$ donde $\\alpha$ es un parámetro adicional a ajustar.\n",
    "\n",
    "Elastic net tiene un efecto de selección en variables como Lasso pero mantiene variables correlacionadas como la regresión Ridge.\n",
    "Así, el modelo Elastic net es menos disperso que Lasso, manteniendo más información. \n",
    "Sin embargo, el modelo es más exigente en recursos computacionales.\n",
    "\n",
    "En lo que sigue presentamos resultados para $\\alpha=0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic_net = ElasticNet(alpha = 1, l1_ratio = 0.5)\n",
    "elastic_net.fit(X_train, y_train)\n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, elastic_net.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, elastic_net.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import metrics\n",
    "\n",
    "# Listas para guardar resultados\n",
    "coef_eln = []     # Coeficientes del modelo para cada alpha\n",
    "err_eln = []      # MSE en test para cada alpha\n",
    "baseline = []     # Error base (para comparar con un modelo sencillo)\n",
    "\n",
    "# Recorremos distintos valores de alpha\n",
    "for a in alphas:\n",
    "    # Creamos el modelo ElasticNet con el alpha actual\n",
    "    # l1_ratio = 1 → equivale a Lasso (solo penalización L1)\n",
    "    # (si l1_ratio estuviera entre 0 y 1, sería una mezcla de Ridge y Lasso)\n",
    "    elastic_net = ElasticNet(alpha=a, l1_ratio=1)\n",
    "\n",
    "    # Entrenamos el modelo con los datos de entrenamiento\n",
    "    elastic_net.fit(X_train, y_train)\n",
    "\n",
    "    # Guardamos los coeficientes para este alpha\n",
    "    coef_eln.append(elastic_net.coef_)\n",
    "\n",
    "    # Hacemos predicciones sobre el conjunto de prueba\n",
    "    y_pred = elastic_net.predict(X_test)\n",
    "\n",
    "    # Calculamos el error cuadrático medio (MSE) en test\n",
    "    elasticnet_error = metrics.mean_squared_error(y_pred, y_test)\n",
    "    err_eln.append(elasticnet_error)\n",
    "\n",
    "    # Guardamos el error baseline (referencia) para este alpha\n",
    "    baseline.append(baseline_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, err_eln, linewidth=5, color='red', label=\"Elastic net\")\n",
    "ax.plot(alphas, baseline, linewidth=4,linestyle='--', color='blue', label='Linear regression')\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('error', fontsize=30)\n",
    "ax.legend(fontsize=30)\n",
    "plt.title(r'Regression error ($\\alpha=0.5$)', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_net = ElasticNet(alpha = 0.02, l1_ratio=1)\n",
    "elastic_net.fit(X_train, y_train) \n",
    "\n",
    "print(\"Train MSE: %0.4f\" % metrics.mean_squared_error(y_train, elastic_net.predict(X_train)))\n",
    "print(\"Test MSE: %0.4f\" % metrics.mean_squared_error(y_test, elastic_net.predict(X_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El valor óptimo del coeficiente de regularización $\\lambda$ está entre $10^{-2}$ y $10^{-1}$.\n",
    "* Para $\\lambda \\to 0$ el término de regularización desaparece llevando al mismo resultado que la regresión lineal.\n",
    "* Para $\\lambda \\to \\infty$ la magnitud del término de regularización domina el error que es menor que con Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coef_eln, linewidth=5)\n",
    "ax.set_xscale('log')\n",
    "plt.xlabel('$\\lambda$', fontsize=30)\n",
    "plt.xticks(fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.ylabel('weight value', fontsize=30)\n",
    "plt.title(r'Elastic net coefficients paths ($\\alpha=0.5$)', fontsize=30)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Como se esperaba, Elastic net mantiene más variables que Lasso.\n",
    "* Se puede obtener un mejor rendimiento variando el valor del hiperparámetro $\\alpha$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Selección de modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear regression error:   %0.4f\" % baseline_error)\n",
    "print(\"Minimun ridge error:       %0.4f\" % min(err_ridge))\n",
    "print(\"Minimum lasso error:       %0.4f\" % min(err_lasso))\n",
    "print(\"Minimum elastic net error: %0.4f\" % min(err_eln))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lasso funciona mejor que otros métodos (Elastic net: $\\alpha=0.5$). \n",
    "* Lasso es más parsimonioso pero es probable que haya una pérdida de precisión.\n",
    "* Elastic net funciona mejor que la regresión Ridge (con $\\alpha=0.5$).\n",
    "* Elastic net se puede ajustar para superar a Lasso pero es más exigente en recursos computacionales.\n",
    "\n",
    "**=> Elastic net es un buen equilibrio para el balance entre precisión y costo computacional entre la regresión Ridge y Lasso**.\n",
    "\n",
    "Para optimizar el modelo ajustando los parámetros óptimos, se puede realizar una validación cruzada.\n",
    "Las funciones sklearn.linear_model.RidgeCV, sklearn.linear_model.LassoCV y sklearn.linear_model.ElasticNetCV en Python realizan un ajuste automático de hiperparámetros para la regresión Ridge, Lasso y Elastic Net respectivamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
