{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de empezar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[robots.txt](https://www.pisos.com/robots.txt)  \n",
    "[html curso](https://www.w3schools.com/html/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beautiful Soup Tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como científico de datos, tarde o temprano llegarás a un punto en el que tendrás que recopilar grandes cantidades de datos. Ya sea un proyecto o por pasatiempo y no siempre podremos contar con las API, pero tranquilo tenemos el web scraping... ¡Y una de las mejores herramientas de web scraping es Beautiful Soup!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Pero.... qué es el web scraping?\n",
    "El web scraping consiste en obtener datos de páginas web de manera automatizada. Cuando visitas un sitio web, el servidor devuelve un documento HTML.   \n",
    "\n",
    "En pocas palabras, el web scraping es la recopilación automatizada de datos de sitios web (para ser más precisos, del contenido HTML de los sitios web).\n",
    "\n",
    "En este Jupyter, aprenderás los conceptos básicos sobre cómo extraer datos de HTML. \n",
    "\n",
    "El web scraping consiste en obtener datos de páginas web de manera automatizada. Cuando visitas un sitio web, el servidor devuelve un documento HTML. \n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "> **Nota ética:** Antes de raspar un sitio web, revisa sus términos de uso y su archivo `robots.txt` para asegurarte de que permites el scraping. No envíes muchas solicitudes en poco tiempo para no sobrecargar el servidor. Usa esta técnica de forma responsable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conoce a tus nuevos mejores amigos: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beautiful Soup\n",
    "- Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala la librería beautifulsoup4 que permite parsear y navegar por documentos HTML/XML\n",
    "# Esta es la herramienta principal para hacer web scraping\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener la experiencia completa de Beautiful Soup, también deberás instalar un parser, dentro de ellos tenemos..\n",
    "\n",
    "- html.parser\n",
    "- lxml\n",
    "- html5lib\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el lxml ya que es el mas rápido "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! + código → Ejecuta un comando del sistema (en una terminal) y lo ejecuta directamente.  \n",
    "Se ejecuta en la primera versión de Python que el sistema encuentre, así que no tienes control sobre en qué entorno o versión se instala algo si tienes varios Python instalados.  \n",
    "\n",
    "% + código → Ejecuta un comando mágico de Jupyter (o del kernel de VS Code) dentro del entorno o kernel activo, por lo que usa el mismo Python que está ejecutando tu notebook o script.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala lxml, un parser HTML/XML muy rápido que BeautifulSoup utilizará\n",
    "# para procesar el código HTML de manera eficiente\n",
    "!pip install lxml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se necesita una cosa más para que podamos comenzar a hacer web scraping, y es la biblioteca de ```requests```. Con ```requests``` podemos solicitar páginas web de sitios web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala la librería requests para hacer peticiones HTTP a sitios web\n",
    "# Permite descargar el contenido HTML de las páginas web que queremos scrapear\n",
    "!pip install requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora asi manos a la obra.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi primer scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre lo primero es importar las librerías "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa las librerías necesarias para el web scraping y manipulación de datos\n",
    "from bs4 import BeautifulSoup as bs  # Para parsear HTML\n",
    "import requests  # Para hacer peticiones HTTP\n",
    "import pandas as pd  # Para manipulación de datos (aunque no se usa en este notebook)\n",
    "import numpy as np  # Para operaciones numéricas (aunque no se usa en este notebook)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, estamos listos para solicitar nuestra primera página web. No es nada complicado: guardamos la URL que queremos raspar en la variable URL, luego solicitamos la URL (requests.get (url)) y guardamos la respuesta en la variable de respuesta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[web](https://www.weather-forecast.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solicita al usuario que introduzca el nombre de una ciudad\n",
    "# y construye la URL de weather-forecast.com para esa ciudad\n",
    "city=str(input(\"Introduzca la ciudad:\"))\n",
    "url = \"https://www.weather-forecast.com/locations/\"+city+\"/forecasts/latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra la URL construida para verificar que es correcta\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza una petición HTTP GET a la URL especificada\n",
    "# y almacena la respuesta del servidor en la variable response\n",
    "response=requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica el código de estado HTTP de la respuesta\n",
    "# 200 significa que la conexión fue exitosa y se obtuvo el contenido\n",
    "response.status_code # ¡conexión correcta!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posibles respuestas:\n",
    "\n",
    "- [Respuestas informativas](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#information_responses) (100–199)\n",
    "- [Respuestas exitosas](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#successful_responses) (200–299)\n",
    "- [Mensajes de redirección](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#redirection_messages) (300–399)\n",
    "- [Respuestas de error del cliente](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#client_error_responses) (400–499)\n",
    "- [Respuestas de error del servidor](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status#server_error_responses) (500–599)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero necesitamos el contenido HTML de la página web solicitada, así que como siguiente paso guardamos el contenido de la respuesta a html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae el contenido HTML de la respuesta en formato bytes\n",
    "# Este es el código fuente de la página web que vamos a analizar\n",
    "html = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprueba el tipo de dato de la variable html\n",
    "# Muestra que es de tipo 'bytes' (datos binarios)\n",
    "type(html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo podemos imprimir para ver su estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime el HTML en bruto\n",
    "# Se verá difícil de leer porque está sin formato y con muchas etiquetas mezcladas\n",
    "print(html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el resultado obtenido en HTML de la página de la previsión metereológica, pero es realmente difícil de leer..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero para eso usamos BeautifulSoup y lxml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo lo hacemos?.."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos un objeto BeautifulSoup llamado soup(sopa) con la siguiente línea de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un objeto BeautifulSoup que parsea el HTML usando lxml\n",
    "# lxml es el parser elegido porque es el más rápido\n",
    "# soup es el objeto que nos permitirá navegar y buscar en el HTML fácilmente\n",
    "soup = bs(html, \"lxml\") # lxlm es un parser(corta en trozitos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[documentacion](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bs?\n",
    "\n",
    "> from bs4 import BeautifulSoup as bs\n",
    "\n",
    "El primer parámetro del método bs() es html (que fue la variable en la que guardamos ese contenido HTML difícil de leer).\n",
    "\n",
    "El segundo parámetro ('lxml'), es el parser que se usa en html "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a ver el cambio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime el HTML parseado por BeautifulSoup\n",
    "# Ahora se ve más organizado y estructurado que el HTML en bruto\n",
    "print(soup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo navegar por un objeto de Beautiful Soup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML consta de elementos como enlaces, párrafos, encabezados, bloques, etc. Estos elementos están envueltos entre etiquetas; dentro de la etiqueta de apertura y cierre se puede encontrar el contenido del elemento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los elementos HTML también pueden tener atributos que contienen información adicional sobre el elemento. Los atributos se definen en las etiquetas de apertura con la siguiente sintaxis: nombre del atributo = \"valor del atributo\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos aprendido algo de HTML básico, finalmente podemos comenzar a extraer datos de soup. Simplemente escriba un nombre de etiqueta después de soup y un punto (como soup.title), y observe cómo se desarrolla la magia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accede a la primera etiqueta <title> del HTML\n",
    "# Devuelve el elemento completo con sus etiquetas de apertura y cierre\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accede a la primera etiqueta <h1> del HTML\n",
    "# Devuelve el encabezado principal de la página con todos sus atributos\n",
    "soup.h1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos las etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae solo el texto contenido en la etiqueta <h1>, sin las etiquetas HTML\n",
    "# get_text() elimina todas las etiquetas y devuelve solo el contenido textual\n",
    "soup.h1.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos los datos que nos interesan: en este caso, etiquetas ```td``` y ```p```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una lista vacía para almacenar las descripciones del tiempo\n",
    "w_desc=[]\n",
    "\n",
    "# Busca todas las etiquetas <td> con la clase especificada que contienen las descripciones del clima\n",
    "for weather in soup.find_all('td', class_='b-forecast__table-description-cell--js'):\n",
    "    # Dentro de cada <td>, busca la etiqueta <p> con la clase especificada y extrae su texto\n",
    "    desc = weather.find('p', class_='b-forecast__table-description-content').text\n",
    "    # Añade la descripción a la lista\n",
    "    w_desc.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra la lista con todas las descripciones del tiempo recopiladas\n",
    "# Contiene predicciones meteorológicas de varios días\n",
    "w_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscamos ahora datos dentro de la etiqueta ```tbody``` cuya ```class```es la indicada en el ejercicio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una lista vacía para almacenar filas de datos de estaciones meteorológicas\n",
    "alist=[]\n",
    "# Busca la primera tabla con clase específica y recorre cada fila <tr>\n",
    "for i in soup.find_all('tbody', class_=\"b-metar-table__body\")[0]: # Los vientos\n",
    "    print(i) # Imprime cada fila (elemento <tr>) que contiene etiquetas <td>\n",
    "    alist.append((i).encode('utf-8'))  # Codifica cada fila en UTF-8 y la añade a la lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra el primer elemento de la lista alist\n",
    "# Contiene el HTML codificado en bytes de la primera fila de la tabla meteorológica\n",
    "alist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda la primera fila (primer elemento) de la tabla en una variable\n",
    "city_weather_info=alist[0] # vamos al primer tr\n",
    "# Crea un nuevo objeto BeautifulSoup parseando solo esa fila específica\n",
    "soup1 = bs(city_weather_info,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra el objeto soup1 parseado\n",
    "# Contiene solo la información de la primera estación meteorológica en formato HTML legible\n",
    "soup1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos los datos contenidos en la fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrae y muestra información específica de la estación meteorológica:\n",
    "\n",
    "# Busca la celda con información de la estación meteorológica\n",
    "station=soup1.find('td', class_=\"b-metar-table__weather-station\")\n",
    "print(\"estación\", station.text)  # Imprime nombre y ubicación de la estación\n",
    "\n",
    "# Busca y extrae la temperatura\n",
    "temp=soup1.find('span', class_=\"temp\")\n",
    "tem=temp.text\n",
    "tem=tem + ' C'  # Añade la unidad de grados Celsius\n",
    "\n",
    "# Busca y extrae la información del viento\n",
    "wind=soup1.find('div', class_=\"b-metar-table__wind-text\")\n",
    "print(\"viento\", wind.text)  # Imprime dirección y velocidad del viento\n",
    "\n",
    "# Busca información sobre nubosidad y visibilidad\n",
    "cloud_visi=soup1.find('div', class_=\"b-metar-table__additionally-container\")\n",
    "print(\"nubosidad\", cloud_visi.text)  # Imprime información adicional sobre nubes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a almacenar los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa el módulo csv para trabajar con archivos CSV\n",
    "import csv\n",
    "\n",
    "# Abre (o crea) un archivo CSV en modo append (añadir al final)\n",
    "with open(\"Weather_Forecast.csv\", \"a\", newline='') as file: # a es de append para añadir\n",
    "    # Define los nombres de las columnas del CSV\n",
    "    field_names = ['Weather Today days', 'Weather days', '10 Day Weather days', 'Weather_Station_Info', 'Temperature', 'Wind', 'Cloud_Visibility']\n",
    "    # Crea un objeto writer que escribirá diccionarios con las columnas especificadas\n",
    "    writer = csv.DictWriter(file, fieldnames=field_names)\n",
    "    # Escribe la fila de encabezados con los nombres de las columnas\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Escribe una fila de datos con toda la información meteorológica recopilada\n",
    "    writer.writerow(\n",
    "        {\n",
    "            'Weather Today days': w_desc[0],  # Predicción para hoy\n",
    "            'Weather days': w_desc[1],  # Predicción para los próximos días\n",
    "            '10 Day Weather days': w_desc[2],  # Predicción a 10 días\n",
    "            'Weather_Station_Info': station.text,  # Información de la estación\n",
    "            'Temperature': tem,  # Temperatura actual\n",
    "            'Wind': wind.text,  # Información del viento\n",
    "            'Cloud_Visibility': cloud_visi.text  # Nubosidad y visibilidad\n",
    "        }\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
