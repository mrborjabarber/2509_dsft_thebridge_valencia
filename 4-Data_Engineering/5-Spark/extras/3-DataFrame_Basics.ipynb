{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de DataFrames en Spark\n",
    "\n",
    "Los DataFrames de Spark son el componente principal y la forma más importante de trabajar con Spark y Python después de Spark 2.0. Los DataFrames actúan como versiones potentes de tablas, con filas y columnas, manejando fácilmente grandes conjuntos de datos. El cambio a DataFrames proporciona muchas ventajas:\n",
    "* **Una sintaxis mucho más simple**\n",
    "* **Capacidad de usar SQL directamente en el dataframe**\n",
    "* **Las operaciones se distribuyen automáticamente entre RDDs**\n",
    "    \n",
    "Si has usado R o incluso la librería pandas con Python, probablemente ya estés familiarizado con el concepto de DataFrames. Los DataFrames de Spark amplían muchos de estos conceptos, permitiéndote transferir ese conocimiento fácilmente al comprender la sintaxis simple de los DataFrames de Spark. Recuerda que la ventaja principal de usar DataFrames de Spark versus esos otros programas es que **Spark puede manejar datos a través de muchos RDDs, conjuntos de datos enormes que nunca cabrían en una sola computadora**. Eso viene con un ligero costo de algunas opciones de sintaxis \"peculiares\", pero después de este curso te sentirás muy cómodo con todos esos temas.\n",
    "\n",
    "¡Comencemos!\n",
    "\n",
    "## Creando un DataFrame\n",
    "\n",
    "Primero necesitamos iniciar una SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos un archivo JSON desde el sistema de archivos de Databricks (DBFS)\n",
    "# spark.read.json() es el método para leer archivos en formato JSON\n",
    "# \"dbfs:/\" indica que el archivo está en el sistema de archivos distribuido de Databricks\n",
    "df = spark.read.json(\"dbfs:/databricks-datasets/structured-streaming/events/file-0.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego iniciamos la SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comentado: dbutils.fs.ls (\"FileStore/tables\") permitiría listar archivos en esa ruta\n",
    "# type(df) nos devuelve el tipo de objeto que es 'df'\n",
    "# Verificamos que efectivamente df es un DataFrame de PySpark\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero necesitarás obtener los datos de un archivo (o conectarte a un archivo distribuido grande como HDFS, hablaremos de esto más adelante una vez que pasemos a conjuntos de datos más grandes en AWS EC2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mostrando los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show() muestra las primeras 20 filas del DataFrame en formato tabla\n",
    "# Es el equivalente a head() en pandas, pero por defecto muestra 20 filas\n",
    "# Muy útil para inspeccionar rápidamente los datos y su estructura\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema() imprime el esquema (estructura) del DataFrame\n",
    "# Muestra el nombre de cada columna, su tipo de dato y si puede ser nulo\n",
    "# Es fundamental para entender qué tipo de datos tenemos en cada columna\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns devuelve una lista con los nombres de todas las columnas del DataFrame\n",
    "# Es útil cuando queremos ver rápidamente qué columnas tenemos disponibles\n",
    "# El resultado es una lista de Python que podemos usar en otros procesos\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe() genera estadísticas descriptivas de las columnas numéricas\n",
    "# Incluye: count (conteo), mean (media), stddev (desviación estándar), min y max\n",
    "# Devuelve un DataFrame, no imprime directamente (usa .show() para visualizar)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos tipos de datos facilitan la inferencia del esquema (como formatos tabulares como csv que mostraremos más adelante). \n",
    "\n",
    "Sin embargo, a menudo tienes que establecer el esquema tú mismo si no estás tratando con un método .read que no tiene inferSchema() incorporado.\n",
    "\n",
    "Spark tiene todas las herramientas que necesitas para esto, solo requiere una estructura muy específica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las clases necesarias para definir esquemas manualmente\n",
    "# StructField: define un campo individual (columna) con nombre, tipo y nulabilidad\n",
    "# StringType: tipo de dato para cadenas de texto\n",
    "# LongType: tipo de dato para números enteros largos\n",
    "# StructType: define la estructura completa del DataFrame (conjunto de campos)\n",
    "from pyspark.sql.types import StructField, StringType, LongType, StructType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación necesitamos crear la lista de campos de estructura\n",
    "* Parámetro name: string, nombre del campo.\n",
    "* Parámetro dataType: :class:`DataType` del campo.\n",
    "* Parámetro nullable: booleano, si el campo puede ser nulo (None) o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el esquema como una lista de StructField\n",
    "# Cada StructField especifica: nombre de columna, tipo de dato, y si acepta nulos\n",
    "# \"time\" será tipo Long (entero largo) y puede ser nulo (True)\n",
    "# \"action\" será tipo String (texto) y puede ser nulo (True)\n",
    "data_schema = [StructField(\"time\", LongType(), True),\n",
    "              StructField(\"action\", StringType(), True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objeto StructType que contiene todos los campos del esquema\n",
    "# Este es el objeto final que representa la estructura completa del DataFrame\n",
    "# Lo usaremos como parámetro 'schema' al leer datos para forzar esta estructura\n",
    "final_struc = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora leemos el JSON pero especificando nuestro esquema predefinido\n",
    "# schema=final_struc le dice a Spark exactamente qué estructura esperar\n",
    "# Esto es más eficiente que inferir el esquema y evita errores de interpretación\n",
    "df = spark.read.json(\"dbfs:/databricks-datasets/structured-streaming/events/file-0.json\", schema = final_struc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos el esquema para verificar que se aplicó correctamente\n",
    "# Ahora no hay necesidad de inferir tipos, Spark usa directamente lo que especificamos\n",
    "# Esto hace que la lectura sea más rápida y predecible\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obteniendo los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accedemos a una columna específica usando notación de corchetes\n",
    "# df[\"time\"] devuelve un objeto Column, NO un DataFrame\n",
    "# Es similar a acceder a una columna en pandas, pero el comportamiento es diferente\n",
    "df[\"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el tipo del objeto que devuelve df[\"time\"]\n",
    "# Confirma que es un objeto Column de PySpark\n",
    "# Las columnas se usan en operaciones de filtrado y transformación\n",
    "type(df[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select() selecciona columnas específicas y devuelve un nuevo DataFrame\n",
    "# A diferencia de df[\"time\"], select() SIEMPRE devuelve un DataFrame\n",
    "# Es la forma recomendada de seleccionar columnas en PySpark\n",
    "df.select(\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirmamos que select() devuelve un DataFrame, no una Column\n",
    "# Esto es importante porque los DataFrames tienen métodos diferentes a las Column\n",
    "# Por ejemplo, solo los DataFrames tienen el método .show()\n",
    "type(df.select(\"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos la columna \"time\" y mostramos el resultado en formato tabla\n",
    "# Este es el flujo típico: select() para crear un nuevo DataFrame y .show() para visualizarlo\n",
    "# Muestra las primeras 20 filas de la columna seleccionada\n",
    "df.select(\"time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(n) devuelve las primeras n filas como una lista de objetos Row\n",
    "# Aquí pedimos las primeras 2 filas del DataFrame completo\n",
    "# Cada Row contiene todos los valores de esa fila, accesibles por nombre de columna\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos TODAS las columnas del DataFrame usando df.columns\n",
    "# df.columns devuelve una lista con los nombres, y select() puede recibir una lista\n",
    "# Es útil cuando quieres seleccionar todas las columnas dinámicamente\n",
    "df.select(df.columns).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Múltiples columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select() puede recibir múltiples nombres de columnas como argumentos separados\n",
    "# Esto crea un nuevo DataFrame con solo las columnas \"time\" y \"action\"\n",
    "# No se modifica el DataFrame original, se crea uno nuevo\n",
    "df.select(\"time\", \"action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionamos múltiples columnas y mostramos el resultado\n",
    "# Es la forma más común de trabajar: seleccionar las columnas necesarias y visualizarlas\n",
    "# Útil para exploración de datos y análisis inicial\n",
    "df.select(\"time\", \"action\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando nuevas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn() crea una nueva columna o modifica una existente\n",
    "# Primer argumento: nombre de la nueva columna (\"newtime\")\n",
    "# Segundo argumento: expresión para calcular los valores (aquí sumamos 5 a \"time\")\n",
    "# Esto crea un nuevo DataFrame con todas las columnas originales más \"newtime\"\n",
    "df.withColumn(\"newtime\", df[\"time\"] + 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el DataFrame original para demostrar que NO se ha modificado\n",
    "# En PySpark, las transformaciones son inmutables: crean nuevos DataFrames\n",
    "# Para conservar los cambios, debemos reasignar: df = df.withColumn(...)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumnRenamed() renombra una columna existente\n",
    "# Primer argumento: nombre actual de la columna (\"action\")\n",
    "# Segundo argumento: nuevo nombre para la columna (\"superaction\")\n",
    "# También es una operación inmutable, devuelve un nuevo DataFrame\n",
    "df.withColumnRenamed(\"action\", \"superaction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operaciones más complicadas para crear nuevas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nueva columna multiplicando los valores de \"time\" por 2\n",
    "# Podemos usar operadores aritméticos directamente sobre columnas\n",
    "# show(5) limita la visualización a solo 5 filas en lugar de las 20 por defecto\n",
    "df.withColumn(\"doubletime\", df[\"time\"]*2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una columna sumando 1 a cada valor de \"time\"\n",
    "# Las operaciones aritméticas básicas (+, -, *, /) funcionan directamente\n",
    "# Spark maneja automáticamente la aplicación de la operación a todas las filas\n",
    "df.withColumn(\"add_one_time\", df[\"time\"] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos los valores de \"time\" por 2 para crear \"half_time\"\n",
    "# La división produce números decimales (tipo double en PySpark)\n",
    "# Nota la notación científica en los resultados (7.34750E8 = 734,750,000)\n",
    "df.withColumn(\"half_time\", df[\"time\"]/2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin .show(), withColumn() solo devuelve el objeto DataFrame\n",
    "# Aquí vemos que el DataFrame resultante tiene 3 columnas: time, action, half_time\n",
    "# Y \"half_time\" es de tipo double (número decimal de doble precisión)\n",
    "df.withColumn(\"half_time\", df[\"time\"]/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Discutiremos operaciones mucho más complicadas más adelante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando SQL\n",
    "\n",
    "Para usar consultas SQL directamente con el dataframe, necesitarás registrarlo en una vista temporal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createOrReplaceTempView() registra el DataFrame como una tabla temporal\n",
    "# Esto nos permite usar consultas SQL sobre el DataFrame\n",
    "# \"IoT\" es el nombre que le damos a la vista temporal (puede ser cualquier nombre)\n",
    "# La vista existe solo durante la sesión de Spark actual\n",
    "df.createOrReplaceTempView(\"IoT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql() ejecuta una consulta SQL y devuelve un DataFrame\n",
    "# Aquí usamos SELECT * para obtener todas las columnas de la vista \"IoT\"\n",
    "# Es útil si ya conoces SQL y prefieres esa sintaxis sobre los métodos de DataFrame\n",
    "sql_results = spark.sql(\"SELECT * FROM IoT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los resultados de la consulta SQL\n",
    "# El resultado es idéntico a hacer df.show()\n",
    "# Puedes usar cualquier consulta SQL válida: WHERE, GROUP BY, JOIN, etc.\n",
    "sql_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nos enfocaremos realmente en usar la sintaxis SQL para este curso en general, pero ten en cuenta que siempre está ahí para ayudarte a salir rápidamente de un apuro con tus habilidades de SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Muy bien, eso es todo lo que necesitamos saber por ahora!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
