{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducción de la Dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para encontrar los componentes principales utilizamos una técnica estándar de factorización de matrices llamada *Descomposición en Valores Singulares* (SVD) que puede descomponer la matriz de entrenamiento $\\bf{X}$ en la multiplicación de tres matrices $\\bf{U}$ $\\bf{\\Sigma}$ $\\bf{V}^T$, donde $\\bf{V}$ contiene los vectores unitarios que definen todos los componentes principales.\n",
    "\n",
    "$$\n",
    "  \\mathbf{V} = \\left( \n",
    "    \\begin{array}{cccc}\n",
    "    | & | & & | \\\\\n",
    "    c_1 & c_2 & ... & c_n \\\\\n",
    "    | & | & & | \n",
    "    \\end{array} \\right)\n",
    "$$\n",
    "\n",
    "Una vez que has identificado todos los componentes principales, puedes reducir la dimensionalidad del conjunto de datos a $d$ dimensiones proyectándolo sobre el hiperplano definido por los primeros $d$ componentes principales.\n",
    "\n",
    "$$\n",
    "  \\mathbf{X}_d = \\mathbf{X} \\mathbf{W}_d\n",
    "$$\n",
    "\n",
    "donde $\\mathbf{W}_d$ contiene las primeras $d$ columnas de $\\bf{V}$. Puedes recuperar el conjunto de datos original con\n",
    "\n",
    "$$\n",
    "  \\mathbf{X} = \\mathbf{X}_d \\mathbf{W}_d^T\n",
    "$$\n",
    "\n",
    "Aunque se pierde algo de información debido a la proyección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GENERACIÓN DE DATOS SINTÉTICOS EN 3D\n",
    "# ============================================\n",
    "# Este código crea un conjunto de datos 3D artificial para demostrar PCA\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Fijamos la semilla para reproducibilidad de los resultados\n",
    "np.random.seed(4)\n",
    "\n",
    "# Configuración de parámetros\n",
    "m = 60  # número de muestras (puntos) que vamos a generar\n",
    "w1, w2 = 0.1, 0.3  # pesos para crear la tercera dimensión como combinación lineal\n",
    "noise = 0.1  # nivel de ruido a añadir a los datos\n",
    "\n",
    "# Generamos ángulos aleatorios para crear un patrón curvo\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "\n",
    "# Creamos matriz vacía de 60 filas x 3 columnas\n",
    "X = np.empty((m, 3))\n",
    "\n",
    "# Primera dimensión: combinación de cos y sin con ruido\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "\n",
    "# Segunda dimensión: función seno escalada con ruido\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "\n",
    "# Tercera dimensión: COMBINACIÓN LINEAL de las dos primeras (baja varianza)\n",
    "# Esta dimensión es redundante, por eso PCA la descartará\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE LOS DATOS EN 3D\n",
    "# ============================================\n",
    "# Graficamos los datos para ver su estructura en el espacio tridimensional\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creamos figura con tamaño personalizado\n",
    "fig = plt.figure(figsize=(6, 3.8))\n",
    "\n",
    "# Añadimos subplot con proyección 3D\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Graficamos los puntos: \"bo\" = blue circles (círculos azules)\n",
    "ax.plot(X[:, 0], X[:, 1], X[:, 2], \"bo\")\n",
    "\n",
    "# Configuramos los límites de cada eje para mejor visualización\n",
    "ax.set_xlim([-1.5, 1.3])\n",
    "ax.set_ylim([-1.2, 1.2])\n",
    "ax.set_zlim([-1, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DESCOMPOSICIÓN EN VALORES SINGULARES (SVD)\n",
    "# ============================================\n",
    "# SVD es la técnica matemática subyacente de PCA\n",
    "\n",
    "# PASO 1: Centrar los datos (restar la media de cada columna)\n",
    "# Esto es CRUCIAL para PCA: los datos deben tener media 0\n",
    "X_centered = X - X.mean(axis=0)\n",
    "\n",
    "# PASO 2: Aplicar SVD para descomponer la matriz en U, s, Vt\n",
    "# X_centered = U @ S @ Vt\n",
    "# - U: matriz de vectores singulares izquierdos (m x m)\n",
    "# - s: valores singulares (diagonal de S)\n",
    "# - Vt: matriz de vectores singulares derechos TRANSPUESTA (n x n)\n",
    "#       Vt contiene los componentes principales en sus FILAS\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "\n",
    "# Mostramos Vt: cada FILA es un componente principal\n",
    "Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# EXTRACCIÓN DE LOS PRIMEROS 2 COMPONENTES PRINCIPALES\n",
    "# ============================================\n",
    "\n",
    "# Los componentes principales están en las COLUMNAS de Vt.T (o FILAS de Vt)\n",
    "# c1 es el primer componente principal (dirección de máxima varianza)\n",
    "c1 = Vt.T[:, 0]\n",
    "\n",
    "# c2 es el segundo componente principal (dirección de segunda mayor varianza)\n",
    "# Es perpendicular (ortogonal) a c1\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFICACIÓN DE LA DESCOMPOSICIÓN SVD\n",
    "# ============================================\n",
    "# Comprobamos que U @ S @ Vt reconstruye X_centered\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "# Creamos la matriz S completa (diagonal con valores singulares)\n",
    "S = np.zeros(X_centered.shape)  # matriz de ceros del mismo tamaño que X_centered\n",
    "S[:n, :n] = np.diag(s)  # colocamos los valores singulares en la diagonal\n",
    "\n",
    "# Verificamos que U @ S @ Vt ≈ X_centered\n",
    "# np.allclose verifica igualdad con tolerancia numérica\n",
    "np.allclose(X_centered, U.dot(S).dot(Vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/e/e9/Singular_value_decomposition.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PROYECCIÓN MANUAL A 2D USANDO LOS PRIMEROS 2 COMPONENTES\n",
    "# ============================================\n",
    "# Reducimos de 3D a 2D proyectando sobre los 2 componentes principales\n",
    "\n",
    "# W2 es la matriz de proyección: contiene los 2 primeros componentes principales\n",
    "# Forma: (3, 2) - cada columna es un componente principal\n",
    "W2 = Vt.T[:, :2]\n",
    "\n",
    "# Proyectamos los datos centrados multiplicando por W2\n",
    "# X_centered: (60, 3) @ W2: (3, 2) = X2D: (60, 2)\n",
    "# Esto transforma cada punto 3D en un punto 2D\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE LOS DATOS PROYECTADOS EN 2D\n",
    "# ============================================\n",
    "# Mostramos cómo se ven los datos después de la reducción dimensional\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal')  # aspect='equal' para mantener proporciones\n",
    "\n",
    "# Graficamos los puntos en 2D con + y puntos\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k+\")  # \"k+\" = cruces negras\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k.\")  # \"k.\" = puntos negros\n",
    "\n",
    "# Etiquetas de los ejes: z1 y z2 representan las nuevas dimensiones\n",
    "ax.set_xlabel(\"$z_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "\n",
    "# Configuramos límites y grid\n",
    "ax.axis([-1.5, 1.3, -1.2, 1.2])\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PCA CON SKLEARN - MÉTODO SIMPLIFICADO\n",
    "# ============================================\n",
    "# Ahora usamos la clase PCA de sklearn que hace todo automáticamente\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Creamos el objeto PCA especificando cuántos componentes queremos\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# fit_transform hace dos cosas:\n",
    "# 1. fit: calcula los componentes principales y centra los datos\n",
    "# 2. transform: proyecta los datos en el nuevo espacio 2D\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPONENTES PRINCIPALES CALCULADOS POR SKLEARN\n",
    "# ============================================\n",
    "# Verificamos que sklearn obtiene los mismos componentes que nuestro cálculo manual\n",
    "\n",
    "# pca.components_ contiene los componentes principales en las FILAS\n",
    "# Cada fila es un vector unitario que define una dirección de máxima varianza\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VARIANZA EXPLICADA POR CADA COMPONENTE\n",
    "# ============================================\n",
    "# Nos dice qué porcentaje de la varianza total captura cada componente\n",
    "\n",
    "# El primer componente captura ~84% de la varianza\n",
    "# El segundo componente captura ~15% de la varianza\n",
    "# INTERPRETACIÓN: los 2 primeros componentes capturan ~99% de la información\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VARIANZA PERDIDA EN LA REDUCCIÓN\n",
    "# ============================================\n",
    "# Calculamos cuánta información perdemos al eliminar el 3er componente\n",
    "\n",
    "# 1 - suma de varianzas explicadas = varianza perdida\n",
    "# ~1.1% de la varianza se pierde (la del 3er componente)\n",
    "# Esto confirma que la 3era dimensión era redundante\n",
    "1 - pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE LOS RESULTADOS DE PCA (SKLEARN)\n",
    "# ============================================\n",
    "# Graficamos los datos proyectados usando el resultado de sklearn\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "\n",
    "# Notamos que la visualización es similar a la manual\n",
    "# (puede tener signos invertidos, pero la estructura es la misma)\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k+\")\n",
    "ax.plot(X2D[:, 0], X2D[:, 1], \"k.\")\n",
    "\n",
    "ax.set_xlabel(\"$z_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "ax.axis([-1.3, 1.5, -1.2, 1.2])\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TRANSFORMACIÓN INVERSA: DE 2D DE VUELTA A 3D\n",
    "# ============================================\n",
    "# Intentamos reconstruir los datos originales a partir de los datos reducidos\n",
    "\n",
    "# inverse_transform proyecta de vuelta al espacio original (3D)\n",
    "# Pero la información del 3er componente se perdió, así que no será exacto\n",
    "X3D_inv = pca.inverse_transform(X2D)\n",
    "\n",
    "# Visualizamos los datos reconstruidos en 3D\n",
    "fig = plt.figure(figsize=(6, 3.8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Los puntos reconstruidos deberían estar cerca de los originales\n",
    "ax.plot(X3D_inv[:, 0], X3D_inv[:, 1], X3D_inv[:, 2], \"bo\")\n",
    "\n",
    "ax.set_xlim([-1.5, 1.3])\n",
    "ax.set_ylim([-1.2, 1.2])\n",
    "ax.set_zlim([-1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFICACIÓN: ¿SON IGUALES LOS DATOS RECONSTRUIDOS?\n",
    "# ============================================\n",
    "# Comprobamos si la reconstrucción es perfecta\n",
    "\n",
    "# np.allclose verifica igualdad con tolerancia numérica\n",
    "# Resultado: False - los datos NO son exactamente iguales\n",
    "# Esto es esperado: perdimos el ~1.1% de varianza\n",
    "np.allclose(X3D_inv, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ERROR CUADRÁTICO MEDIO (MSE) DE LA RECONSTRUCCIÓN\n",
    "# ============================================\n",
    "# Medimos el error promedio por punto\n",
    "\n",
    "# Calculamos la distancia euclidiana al cuadrado para cada punto\n",
    "# y promediamos sobre todos los puntos\n",
    "# MSE ≈ 0.01 - error muy pequeño, la reconstrucción es buena\n",
    "np.mean(np.sum(np.square(X3D_inv - X), axis=1)) # mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para elegir el número correcto de dimensiones, podemos especificar un valor mínimo deseado para la varianza y mantener las $d$ dimensiones que cumplan con este criterio (para la visualización de datos, debemos establecer $d$ en 2 o 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ELECCIÓN AUTOMÁTICA DE DIMENSIONES POR VARIANZA\n",
    "# ============================================\n",
    "# Encontramos cuántos componentes necesitamos para capturar 95% de la varianza\n",
    "\n",
    "# Calculamos la varianza explicada acumulada\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# np.argmax encuentra el primer índice donde cumsum >= 0.95\n",
    "# +1 porque los índices empiezan en 0\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "# Resultado: con 2 componentes ya capturamos >95% de la varianza\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGA DEL DATASET MNIST\n",
    "# ============================================\n",
    "# MNIST: 70,000 imágenes de dígitos escritos a mano (28x28 píxeles)\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Descargamos el dataset (puede tardar la primera vez)\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "\n",
    "# Convertimos las etiquetas a enteros sin signo (0-9)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIVISIÓN EN TRAIN Y TEST\n",
    "# ============================================\n",
    "# Separamos los datos para entrenar y evaluar modelos\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X: imágenes aplanadas (cada imagen 28x28 = 784 características)\n",
    "# y: etiquetas (0-9)\n",
    "X = mnist[\"data\"].values\n",
    "y = mnist[\"target\"].values\n",
    "\n",
    "# Dividimos en 75% train, 25% test (por defecto)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TAMAÑO DEL DATASET EN BYTES\n",
    "# ============================================\n",
    "# Vemos cuánta memoria ocupa el dataset completo en RAM\n",
    "\n",
    "# nbytes: número de bytes que ocupa el array X en memoria\n",
    "# ~439 MB para 70,000 imágenes\n",
    "X.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIMENSIONES DEL DATASET\n",
    "# ============================================\n",
    "# 70,000 muestras × 784 características (28×28 píxeles)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CÁLCULO MANUAL DEL TAMAÑO EN MEGABYTES\n",
    "# ============================================\n",
    "# Verificamos el cálculo del tamaño en MB\n",
    "\n",
    "# 70,000 imágenes × 8 bytes (float64) × 784 características\n",
    "# Dividido por 1024 dos veces para convertir de bytes a MB\n",
    "# Resultado: ~418.7 MB\n",
    "70000*8*784 / 1024 / 1024 # en megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DETERMINAR DIMENSIONES ÓPTIMAS PARA MNIST\n",
    "# ============================================\n",
    "# ¿Cuántos componentes necesitamos para capturar 95% de varianza?\n",
    "\n",
    "# Creamos PCA sin especificar n_components para calcular TODOS\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Calculamos varianza explicada acumulada\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Encontramos el número de componentes necesario para 95%\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "\n",
    "# Resultado: 153 componentes (de 784 originales)\n",
    "# ¡Reducción de ~80% en dimensionalidad manteniendo 95% de información!\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GRÁFICA DE VARIANZA EXPLICADA ACUMULADA\n",
    "# ============================================\n",
    "# Visualizamos cómo aumenta la varianza explicada con cada componente\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "# Graficamos la curva de varianza acumulada\n",
    "plt.plot(cumsum, linewidth=3)\n",
    "\n",
    "# Configuramos ejes\n",
    "plt.axis([0, 784, 0, 1])\n",
    "plt.xlabel(\"Dimensions\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "\n",
    "# Marcamos el punto donde llegamos a 95%\n",
    "plt.plot([d, d], [0, 0.95], \"k:\")  # línea vertical\n",
    "plt.plot([0, d], [0.95, 0.95], \"k:\")  # línea horizontal\n",
    "plt.plot(d, 0.95, \"ko\")  # punto de intersección\n",
    "\n",
    "# La curva muestra un \"codo\": después de ~150 componentes, \n",
    "# los componentes adicionales aportan muy poca información\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PCA CON UMBRAL DE VARIANZA AUTOMÁTICO\n",
    "# ============================================\n",
    "# Sklearn puede determinar automáticamente el número de componentes\n",
    "\n",
    "# n_components=0.95 significa: \"elige el número mínimo de componentes\n",
    "# que expliquen al menos el 95% de la varianza\"\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Aplicamos PCA a los datos de entrenamiento\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NÚMERO DE COMPONENTES SELECCIONADOS\n",
    "# ============================================\n",
    "# Verificamos cuántos componentes eligió sklearn\n",
    "\n",
    "# n_components_ (con guión bajo): número de componentes después del fit\n",
    "# Resultado: 153 (igual que nuestro cálculo manual)\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VERIFICACIÓN DE LA VARIANZA EXPLICADA TOTAL\n",
    "# ============================================\n",
    "# Confirmamos que realmente capturamos ~95% de la varianza\n",
    "\n",
    "# Sumamos la varianza explicada por los 153 componentes\n",
    "# Resultado: 0.950 (95.0%)\n",
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DESCOMPRESIÓN: RECONSTRUCCIÓN DE LAS IMÁGENES\n",
    "# ============================================\n",
    "# Intentamos recuperar las imágenes originales desde los 153 componentes\n",
    "\n",
    "# inverse_transform proyecta de vuelta al espacio original (784 dimensiones)\n",
    "# Las imágenes reconstruidas serán similares pero no idénticas\n",
    "X_recovered = pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FUNCIÓN AUXILIAR PARA VISUALIZAR DÍGITOS MNIST\n",
    "# ============================================\n",
    "# Esta función dibuja múltiples imágenes de dígitos en una grilla\n",
    "\n",
    "import matplotlib as mpl \n",
    "\n",
    "def plot_digits(instances, images_per_row=5, **options):\n",
    "    size = 28  # cada imagen es 28×28 píxeles\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    \n",
    "    # Convertimos cada vector de 784 elementos a una matriz 28×28\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    \n",
    "    # Calculamos cuántas filas necesitamos\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    \n",
    "    # Rellenamos con imágenes vacías si es necesario\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    \n",
    "    # Concatenamos imágenes horizontalmente por fila\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    \n",
    "    # Concatenamos todas las filas verticalmente\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    \n",
    "    # Mostramos en escala de grises\n",
    "    plt.imshow(image, cmap = mpl.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARACIÓN VISUAL: ORIGINALES VS COMPRIMIDOS\n",
    "# ============================================\n",
    "# Mostramos dígitos originales al lado de los reconstruidos\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# Panel izquierdo: imágenes originales\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])  # tomamos cada 2100-ésima imagen\n",
    "plt.title(\"Original\", fontsize=16)\n",
    "\n",
    "# Panel derecho: imágenes reconstruidas desde 153 componentes\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered[::2100])\n",
    "plt.title(\"Comprimido\", fontsize=16)\n",
    "\n",
    "# Observación: las imágenes comprimidas son muy similares a las originales\n",
    "# Perdimos solo 5% de información pero las imágenes siguen siendo reconocibles\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el conjunto de datos es demasiado grande para caber en la memoria, podemos usar PCA incremental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PCA INCREMENTAL PARA DATASETS GRANDES\n",
    "# ============================================\n",
    "# Cuando los datos no caben en memoria, procesamos por lotes (batches)\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from tqdm import tqdm  # barra de progreso\n",
    "\n",
    "n_batches = 100  # dividimos los datos en 100 lotes\n",
    "\n",
    "# IncrementalPCA puede aprender de forma incremental (lote por lote)\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "\n",
    "# Procesamos cada lote por separado\n",
    "for X_batch in tqdm(np.array_split(X_train, n_batches)):\n",
    "    # partial_fit actualiza el modelo con un nuevo lote\n",
    "    # Sin cargar todos los datos en memoria a la vez\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "# Ahora transformamos todos los datos de entrenamiento\n",
    "X_reduced = inc_pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RECONSTRUCCIÓN CON INCREMENTAL PCA\n",
    "# ============================================\n",
    "# Proyectamos de vuelta a 784 dimensiones\n",
    "\n",
    "# La reconstrucción con IncrementalPCA debería ser muy similar\n",
    "# a la de PCA estándar\n",
    "X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VARIANZA EXPLICADA CON INCREMENTAL PCA\n",
    "# ============================================\n",
    "# Verificamos la calidad del resultado\n",
    "\n",
    "# Resultado: ~94.98% de varianza explicada\n",
    "# Muy similar al PCA estándar (~95.0%)\n",
    "np.sum(inc_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARACIÓN VISUAL: INCREMENTAL PCA\n",
    "# ============================================\n",
    "# Las imágenes reconstruidas deberían verse casi idénticas\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "# Izquierda: originales\n",
    "plt.subplot(121)\n",
    "plot_digits(X_train[::2100])\n",
    "\n",
    "# Derecha: reconstruidas con IncrementalPCA\n",
    "plt.subplot(122)\n",
    "plot_digits(X_recovered_inc_pca[::2100])\n",
    "\n",
    "# Observación: los resultados son prácticamente indistinguibles\n",
    "# del PCA estándar, pero IncrementalPCA usa menos memoria\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos aplicar el *truco del kernel* para realizar proyecciones no lineales complejas para la reducción de dimensionalidad. Es bueno para preservar grupos de instancias después de la proyección e incluso desenrollar conjuntos de datos que se encuentran en un *manifold*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATASET SWISS ROLL (ROLLO SUIZO)\n",
    "# ============================================\n",
    "# Datos 3D con estructura NO LINEAL (manifold curvo)\n",
    "\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "# Generamos 1000 puntos en forma de \"rollo suizo\"\n",
    "# Este dataset es un ejemplo clásico de manifold no lineal\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# t contiene el \"tiempo\" o posición a lo largo del rollo (útil para colorear)\n",
    "\n",
    "# Configuración de ejes para visualización\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "# Visualizamos en 3D\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Coloreamos los puntos según su posición en el rollo (t)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap=plt.cm.hot)\n",
    "ax.view_init(10, -70)  # ángulo de vista\n",
    "\n",
    "# Etiquetas de ejes\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "\n",
    "ax.set_xlim(axes[0:2])\n",
    "ax.set_ylim(axes[2:4])\n",
    "ax.set_zlim(axes[4:6])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# KERNEL PCA - PCA NO LINEAL\n",
    "# ============================================\n",
    "# Comparamos diferentes kernels para desenrollar el swiss roll\n",
    "\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "# Tres tipos de kernels:\n",
    "\n",
    "# 1. Kernel lineal: equivalente a PCA estándar\n",
    "lin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\n",
    "\n",
    "# 2. Kernel RBF (Radial Basis Function): captura relaciones no lineales\n",
    "#    gamma controla el \"radio\" de influencia de cada punto\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\n",
    "\n",
    "# 3. Kernel sigmoide: similar a función de activación de redes neuronales\n",
    "#    gamma y coef0 son hiperparámetros\n",
    "sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n",
    "\n",
    "# Variable para clasificación binaria (dividir el rollo en dos)\n",
    "y = t > 6.9\n",
    "\n",
    "# Comparamos los tres kernels\n",
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "for subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), \n",
    "                            (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n",
    "                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n",
    "    # Aplicamos PCA con cada kernel\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    \n",
    "    # Guardamos el resultado RBF para uso posterior\n",
    "    if subplot == 132:\n",
    "        X_reduced_rbf = X_reduced\n",
    "    \n",
    "    # Graficamos\n",
    "    plt.subplot(subplot)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "\n",
    "# OBSERVACIÓN CLAVE:\n",
    "# - Kernel lineal NO desenrolla el rollo (falla con datos no lineales)\n",
    "# - Kernel RBF desenrolla el rollo exitosamente (preserva estructura)\n",
    "# - Kernel sigmoide también ayuda pero menos que RBF\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionar el mejor kernel y los valores de hiperparámetros no es una tarea seniclla, ya que PCA es una tarea no supervisada y no tenemos una métrica de rendimiento. Sin embargo, la reducción de dimensionalidad es normalmente un paso de preprocesamiento para una tarea de aprendizaje supervisado (por ejemplo, clasificación). Esto significa que podemos usar la búsqueda en cuadrícula para seleccionar los hiperparámetros óptimos que conduzcan al mejor rendimiento en esa tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GRID SEARCH PARA OPTIMIZAR HIPERPARÁMETROS DE KERNEL PCA\n",
    "# ============================================\n",
    "# Encontramos el mejor kernel y gamma usando validación cruzada\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Creamos un pipeline: KernelPCA + Regresión Logística\n",
    "# Idea: encontrar los hiperparámetros de KPCA que mejor funcionan\n",
    "# para clasificar los datos después de la reducción\n",
    "clf = Pipeline([\n",
    "        (\"kpca\", KernelPCA(n_components=2)),\n",
    "        (\"log_reg\", LogisticRegression(solver=\"lbfgs\"))\n",
    "    ])\n",
    "\n",
    "# Definimos la grilla de hiperparámetros a probar\n",
    "param_grid = [{\n",
    "        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),  # 10 valores entre 0.03 y 0.05\n",
    "        \"kpca__kernel\": [\"rbf\", \"sigmoid\"]  # probamos 2 kernels\n",
    "    }]\n",
    "\n",
    "# GridSearchCV prueba todas las combinaciones (10 × 2 = 20)\n",
    "# con validación cruzada de 3 folds\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "\n",
    "# Entrenamos y buscamos los mejores hiperparámetros\n",
    "# usando la tarea de clasificación (y) como criterio\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MEJORES HIPERPARÁMETROS ENCONTRADOS\n",
    "# ============================================\n",
    "# Mostramos la combinación óptima\n",
    "\n",
    "# Resultado: kernel RBF con gamma ≈ 0.0433\n",
    "# Es el que mejor preserva la estructura para la clasificación\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# RECONSTRUCCIÓN CON KERNEL PCA (PREIMAGEN)\n",
    "# ============================================\n",
    "# Proyectamos de vuelta al espacio 3D original\n",
    "\n",
    "# Usamos los mejores parámetros encontrados\n",
    "rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433,\n",
    "                    fit_inverse_transform=True)\n",
    "\n",
    "# Reducimos a 2D\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "\n",
    "# Intentamos reconstruir el 3D original (preimagen)\n",
    "# NOTA: con kernels no lineales, la reconstrucción es aproximada\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE LA PREIMAGEN\n",
    "# ============================================\n",
    "# Mostramos los datos reconstruidos en 3D\n",
    "\n",
    "axes = [-11.5, 14, -2, 23, -12, 15]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Graficamos la preimagen (reconstrucción)\n",
    "ax.scatter(X_preimage[:, 0], X_preimage[:, 1], X_preimage[:, 2], \n",
    "           c=t, cmap=plt.cm.hot)\n",
    "ax.view_init(10, -70)\n",
    "\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
    "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
    "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
    "\n",
    "# OBSERVACIÓN: la preimagen NO es exacta (se ve diferente al original)\n",
    "# La reconstrucción desde espacio no lineal pierde información\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ERROR DE RECONSTRUCCIÓN (MSE)\n",
    "# ============================================\n",
    "# Medimos qué tan diferente es la preimagen del original\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# MSE ≈ 32.79 - error considerable\n",
    "# Esto confirma que la reconstrucción desde kernel no lineal\n",
    "# no es perfecta (a diferencia de PCA lineal)\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otras técnicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NUEVO DATASET SWISS ROLL\n",
    "# ============================================\n",
    "# Generamos un nuevo rollo suizo para comparar otras técnicas\n",
    "\n",
    "# Usamos una semilla diferente (41) para variedad\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LLE (LOCALLY LINEAR EMBEDDING)\n",
    "# ============================================\n",
    "# Técnica que preserva relaciones locales entre vecinos\n",
    "\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "# Parámetros:\n",
    "# - n_components=2: reducir a 2 dimensiones\n",
    "# - n_neighbors=10: cada punto considera sus 10 vecinos más cercanos\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
    "\n",
    "# Aplicamos LLE\n",
    "# LLE reconstruye cada punto como combinación lineal de sus vecinos\n",
    "# y busca una proyección 2D que preserve estas relaciones\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLE funciona midiendo primero cómo cada instancia de entrenamiento se relaciona linealmente con sus vecinos más cercanos y luego buscando una representación de baja dimensión del conjunto de entrenamiento donde se preserven mejor estas relaciones locales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN: LLE DESENROLLA EL SWISS ROLL\n",
    "# ============================================\n",
    "# LLE es excelente para desenrollar manifolds\n",
    "\n",
    "plt.title(\"Unrolled swiss roll using LLE\", fontsize=14)\n",
    "\n",
    "# Graficamos los datos proyectados coloreados por t\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "\n",
    "plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "plt.ylabel(\"$z_2$\", fontsize=18)\n",
    "plt.axis([-0.065, 0.055, -0.1, 0.12])\n",
    "plt.grid(True)\n",
    "\n",
    "# OBSERVACIÓN: LLE desenrolla perfectamente el rollo\n",
    "# Preserva la estructura local (puntos cercanos siguen cercanos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLE funciona muy bien para desenrollar *manifolds*, pero escala mal a conjuntos de datos muy grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MDS (MULTIDIMENSIONAL SCALING)\n",
    "# ============================================\n",
    "# Preserva las distancias entre TODOS los puntos (no solo vecinos)\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "# MDS intenta mantener las distancias par a par del espacio original\n",
    "# Es más costoso computacionalmente que LLE\n",
    "mds = MDS(n_components=2, random_state=42)\n",
    "X_reduced_mds = mds.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# ISOMAP (ISOMETRIC MAPPING)\n",
    "# ============================================\n",
    "# Similar a MDS pero usa distancias geodésicas (sobre el manifold)\n",
    "\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "# Isomap construye un grafo de vecinos y calcula distancias\n",
    "# a lo largo del manifold (no distancias euclidianas directas)\n",
    "# Luego aplica MDS sobre estas distancias geodésicas\n",
    "isomap = Isomap(n_components=2)\n",
    "X_reduced_isomap = isomap.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# t-SNE (T-DISTRIBUTED STOCHASTIC NEIGHBOR EMBEDDING)\n",
    "# ============================================\n",
    "# Técnica muy popular para visualización, preserva grupos/clusters\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# t-SNE es excelente para visualización pero:\n",
    "# - Muy lento para datasets grandes\n",
    "# - No garantiza preservar distancias globales\n",
    "# - Preserva muy bien la estructura de clusters\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_reduced_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPARACIÓN DE LAS 3 TÉCNICAS\n",
    "# ============================================\n",
    "# Visualizamos MDS, Isomap y t-SNE lado a lado\n",
    "\n",
    "titles = [\"MDS\", \"Isomap\", \"t-SNE\"]\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "for subplot, title, X_reduced in zip((131, 132, 133), titles,\n",
    "                                     (X_reduced_mds, X_reduced_isomap, X_reduced_tsne)):\n",
    "    plt.subplot(subplot)\n",
    "    plt.title(title, fontsize=14)\n",
    "    \n",
    "    # Graficamos cada resultado coloreado por t\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=t, cmap=plt.cm.hot)\n",
    "    plt.xlabel(\"$z_1$\", fontsize=18)\n",
    "    if subplot == 131:\n",
    "        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n",
    "    plt.grid(True)\n",
    "\n",
    "# COMPARACIÓN:\n",
    "# - MDS: preserva distancias globales, desenrolla parcialmente\n",
    "# - Isomap: muy similar a LLE, desenrolla bien usando distancias geodésicas  \n",
    "# - t-SNE: agrupa bien pero distorsiona distancias, menos \"desenrollado\"\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
