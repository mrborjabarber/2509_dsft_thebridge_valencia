{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "peUFBQDh_Yo_"
   },
   "source": [
    "# **Chapter 5 – Support Vector Machines**\n",
    "\n",
    "_This notebook contains all the sample code and solutions to the exercises in chapter 5._"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OZVA5ffX_YpB"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eixSLX0m_YpB"
   },
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayuPpKv5_YpC"
   },
   "outputs": [],
   "source": "# ============================================\n# CONFIGURACIÓN INICIAL DEL NOTEBOOK\n# ============================================\n\n# Importamos las bibliotecas fundamentales para el análisis\nimport sys\nimport sklearn  # Biblioteca de machine learning\nimport numpy as np  # Manejo de arrays y operaciones numéricas\nimport os\n\n# Establecemos una semilla aleatoria para reproducibilidad de resultados\nnp.random.seed(42)\n\n# ============================================\n# CONFIGURACIÓN DE MATPLOTLIB PARA GRÁFICOS\n# ============================================\n# Habilitamos el modo inline para que los gráficos se muestren en el notebook\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n# Configuramos el tamaño de fuente para mejorar la legibilidad de los gráficos\nmpl.rc('axes', labelsize=14)  # Tamaño de etiquetas de los ejes\nmpl.rc('xtick', labelsize=12)  # Tamaño de las marcas del eje X\nmpl.rc('ytick', labelsize=12)  # Tamaño de las marcas del eje Y"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "95v-QLEa_YpG"
   },
   "source": [
    "# Large margin classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2Szjy-6V_YpH"
   },
   "source": [
    "The next few code cells generate the first figures in chapter 5. The first actual code sample comes after:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EjTbRbt_YpI",
    "outputId": "d855be49-512f-417d-9742-0510ce657c9a"
   },
   "outputs": [],
   "source": "# ============================================\n# CARGA DEL DATASET IRIS Y PREPARACIÓN\n# ============================================\n\nfrom sklearn.svm import SVC  # Support Vector Classifier\nfrom sklearn import datasets\n\n# Cargamos el famoso dataset Iris\niris = datasets.load_iris()\n\n# Seleccionamos solo dos características para facilitar la visualización 2D:\n# - Columna 2: Longitud del pétalo (petal length)\n# - Columna 3: Ancho del pétalo (petal width)\nX = iris[\"data\"][:, (2, 3)]\ny = iris[\"target\"]  # Las etiquetas de clase (0=setosa, 1=versicolor, 2=virginica)\n\n# ============================================\n# FILTRADO DE CLASES PARA CLASIFICACIÓN BINARIA\n# ============================================\n# SVM funciona mejor con problemas binarios, así que filtramos solo dos clases:\n# - Clase 0: Iris Setosa\n# - Clase 1: Iris Versicolor\nsetosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\n\n# ============================================\n# CREACIÓN DEL MODELO SVM CON KERNEL LINEAL\n# ============================================\n# Creamos un clasificador SVM con las siguientes características:\n# - kernel='linear': Usamos un kernel lineal (frontera de decisión recta)\n# - C = 1*10**10: Un valor MUY alto de C que penaliza fuertemente los errores\n#   (esto crea un \"hard margin\" - margen duro sin violaciones)\nsvm_clf = SVC(kernel='linear', C = 1*10**10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# ENTRENAMIENTO DEL MODELO SVM\n# ============================================\n# Ajustamos (entrenamos) el modelo SVM con nuestros datos\n# El modelo aprenderá a encontrar el hiperplano óptimo que separa las dos clases\n# maximizando el margen entre ellas\nsvm_clf.fit(X, y)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlFQexfx_YpM",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "2cbfd52f-6d3f-4667-aef0-3a99c4a96f23",
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================\n# CREACIÓN DE MODELOS \"MALOS\" PARA COMPARACIÓN\n# ============================================\n# Creamos algunos modelos de clasificación subóptimos para demostrar\n# por qué SVM es superior (encuentra el mejor margen)\nx0 = np.linspace(0, 5.5, 200)\npred_1 = 5*x0 - 20    # Primera frontera de decisión candidata\npred_2 = x0 - 1.8     # Segunda frontera de decisión candidata\npred_3 = 0.1 * x0 + 0.5  # Tercera frontera de decisión candidata\n\n# ============================================\n# FUNCIÓN PARA VISUALIZAR LA FRONTERA DE DECISIÓN SVM\n# ============================================\ndef plot_svc_decision_boundary(svm_clf, xmin, xmax):\n    \"\"\"\n    Esta función dibuja la frontera de decisión del SVM y sus márgenes.\n    \n    Parámetros:\n    - svm_clf: modelo SVM entrenado\n    - xmin, xmax: rango del eje x para la visualización\n    \"\"\"\n    # Extraemos los coeficientes del hiperplano (w0, w1) y el intercepto (b)\n    w = svm_clf.coef_[0]      # Vector de pesos [w0, w1]\n    b = svm_clf.intercept_[0]  # Término independiente (bias)\n\n    # La frontera de decisión es: w0*x0 + w1*x1 + b = 0\n    # Despejamos x1: x1 = -w0/w1 * x0 - b/w1\n    x0 = np.linspace(xmin, xmax, 200)\n    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n\n    # Calculamos los márgenes (las líneas paralelas a la frontera)\n    # El margen tiene un ancho de 2/||w||\n    margin = 1/w[1]\n    gutter_up = decision_boundary + margin    # Margen superior\n    gutter_down = decision_boundary - margin  # Margen inferior\n\n    # Destacamos los vectores de soporte (support vectors)\n    # Son los puntos más cercanos a la frontera de decisión\n    svs = svm_clf.support_vectors_\n    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n    \n    # Dibujamos la frontera de decisión (línea sólida) y los márgenes (líneas punteadas)\n    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n    plt.plot(x0, gutter_down, \"k--\", linewidth=2)\n\n# ============================================\n# CREACIÓN DE GRÁFICOS COMPARATIVOS\n# ============================================\nplt.figure(figsize=(12,2.7))\n\n# GRÁFICO IZQUIERDO: Múltiples fronteras de decisión posibles (modelos \"malos\")\nplt.subplot(121)\nplt.plot(x0, pred_1, \"g--\", linewidth=2)  # Frontera candidata 1\nplt.plot(x0, pred_2, \"m-\", linewidth=2)   # Frontera candidata 2\nplt.plot(x0, pred_3, \"r-\", linewidth=2)   # Frontera candidata 3\n# Graficamos los puntos de datos\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\n\n# GRÁFICO DERECHO: Frontera de decisión óptima encontrada por SVM\nplt.subplot(122)\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.axis([0, 5.5, 0, 2]);"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IfE9PMH1_YpP"
   },
   "source": [
    "# Sensitivity to feature scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO1bHNnA_YpQ",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "4dbfdc61-5f99-43e9-8c63-9a7b49c60138",
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================\n# DEMOSTRACIÓN: SENSIBILIDAD A ESCALAS DE CARACTERÍSTICAS\n# ============================================\n# Este ejemplo muestra por qué es CRÍTICO escalar las características antes de usar SVM\n\n# Creamos un dataset sintético con escalas muy diferentes:\n# - x0: valores entre 1 y 5\n# - x1: valores entre 20 y 80 (mucho más grandes)\nXs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\nys = np.array([0, 0, 1, 1])  # Etiquetas de clase\n\n# Entrenamos un SVM con datos SIN ESCALAR\nsvm_clf = SVC(kernel=\"linear\", C=100)\nsvm_clf.fit(Xs, ys)\n\n# ============================================\n# GRÁFICO COMPARATIVO: ANTES Y DESPUÉS DEL ESCALADO\n# ============================================\nplt.figure(figsize=(12,3.2))\n\n# GRÁFICO IZQUIERDO: Datos sin escalar\nplt.subplot(121)\nplt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")  # Clase 1\nplt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")  # Clase 0\nplot_svc_decision_boundary(svm_clf, 0, 6)\nplt.xlabel(\"$x_0$\", fontsize=20)\nplt.ylabel(\"$x_1$  \", fontsize=20, rotation=0)\nplt.title(\"Unscaled\", fontsize=16)\nplt.axis([0, 6, 0, 90])\n# NOTA: La frontera de decisión se ve casi vertical porque x1 domina (escala mayor)\n\n# Aplicamos StandardScaler para normalizar las características\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(Xs)  # Transforma los datos: (x - media) / desv_std\n\n# Entrenamos un nuevo SVM con datos ESCALADOS\nsvm_clf.fit(X_scaled, ys)\n\n# GRÁFICO DERECHO: Datos escalados (StandardScaler)\nplt.subplot(122)\nplt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")  # Clase 1\nplt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")  # Clase 0\nplot_svc_decision_boundary(svm_clf, -2, 2)\nplt.xlabel(\"$x_0$\", fontsize=20)\nplt.title(\"Scaled\", fontsize=16)\nplt.axis([-2, 2, -2, 2])\n# NOTA: Ahora la frontera de decisión tiene una orientación más equilibrada\n# porque ambas características tienen la misma escala (media=0, std=1)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TBe-G-xa_YpU"
   },
   "source": [
    "# Sensitivity to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIL-Iwy2_YpU",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "3a57927c-4b6d-44ed-c199-24213fd6a039",
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================\n# DEMOSTRACIÓN: SENSIBILIDAD A OUTLIERS (VALORES ATÍPICOS)\n# ============================================\n# Este ejemplo muestra cómo los outliers pueden afectar la frontera de decisión del SVM\n\n# Creamos dos outliers (puntos atípicos) de clase 0 en una zona dominada por clase 1\nX_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\ny_outliers = np.array([0, 0])\n\n# Creamos dos datasets diferentes añadiendo outliers:\n# Dataset 1: datos originales + primer outlier\nXo1 = np.concatenate([X, X_outliers[:1]], axis=0)\nyo1 = np.concatenate([y, y_outliers[:1]], axis=0)\n\n# Dataset 2: datos originales + segundo outlier\nXo2 = np.concatenate([X, X_outliers[1:]], axis=0)\nyo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n\n# Entrenamos un SVM con C muy alto (hard margin - margen duro)\n# Esto hace que el modelo sea MUY sensible a outliers\nsvm_clf2 = SVC(kernel=\"linear\", C=10**9)\nsvm_clf2.fit(Xo2, yo2)\n\n# ============================================\n# GRÁFICOS COMPARATIVOS\n# ============================================\nplt.figure(figsize=(12,2.7))\n\n# GRÁFICO IZQUIERDO: Outlier que hace IMPOSIBLE la separación lineal perfecta\nplt.subplot(121)\nplt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")  # Clase Versicolor\nplt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")  # Clase Setosa\nplt.text(0.3, 1.0, \"Impossible!\", fontsize=24, color=\"red\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\n# Anotamos dónde está el outlier problemático\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[0][0], X_outliers[0][1]),\n             xytext=(2.5, 1.7),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])\n\n# GRÁFICO DERECHO: Outlier que DISTORSIONA la frontera de decisión\nplt.subplot(122)\nplt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")  # Clase Versicolor\nplt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")  # Clase Setosa\nplot_svc_decision_boundary(svm_clf2, 0, 5.5)\nplt.xlabel(\"Petal length\", fontsize=14)\n# Anotamos dónde está el outlier que distorsiona el modelo\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[1][0], X_outliers[1][1]),\n             xytext=(3.2, 0.08),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])\n# NOTA: La frontera de decisión se desvía para intentar clasificar correctamente el outlier\n# Esto puede llevar a un modelo con mala generalización (overfitting)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "klrJvnv9_YpX"
   },
   "source": [
    "# Large margin *vs* margin violations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nc4RI8PO_YpY"
   },
   "source": [
    "This is the first code example in chapter 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# EJEMPLO PRÁCTICO: PIPELINE COMPLETO CON SVM\n# ============================================\n# Este es el primer ejemplo de código completo del capítulo 5\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC  # Versión optimizada para SVM lineal\n\n# Cargamos el dataset Iris completo\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n\n# Creamos un problema de clasificación binaria:\n# ¿Es la flor Iris virginica (clase 2) o no?\ny = (iris[\"target\"] == 2).astype(np.float64)  # 1 si es virginica, 0 si no\n\n# ============================================\n# CREACIÓN DE UN PIPELINE CON MEJORES PRÁCTICAS\n# ============================================\n# Un Pipeline encadena múltiples pasos de procesamiento:\nsvm_clf = Pipeline([\n    # Paso 1: Escalado de características (CRÍTICO para SVM)\n    (\"scaler\", StandardScaler()),\n    \n    # Paso 2: Clasificador SVM lineal\n    # - C = 1: Parámetro de regularización (equilibrio entre margen y violaciones)\n    # - loss='hinge': Función de pérdida estándar para SVM\n    # - random_state=42: Para reproducibilidad\n    (\"linear_svc\", LinearSVC(C = 1, loss='hinge', random_state=42))\n])\n\n# Entrenamos el pipeline completo (escala + entrena)\nsvm_clf.fit(X, y)\n# NOTA: El Pipeline automáticamente aplica el escalado y luego entrena el SVM"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctf4TVJz_Ype"
   },
   "source": [
    "Now let's generate the graph comparing different regularization settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIjo_Ds4_Ypf",
    "outputId": "7c5b8c4c-f859-4733-c122-970e997c79c2"
   },
   "outputs": [],
   "source": "# ============================================\n# COMPARACIÓN DEL PARÁMETRO C (REGULARIZACIÓN)\n# ============================================\n# El parámetro C controla el trade-off entre:\n# - Margen amplio (mejor generalización)\n# - Menos violaciones del margen (mejor precisión en entrenamiento)\n\nscaler = StandardScaler()\n\n# ============================================\n# MODELO 1: C = 1 (Regularización FUERTE)\n# ============================================\n# C bajo = Permite más violaciones del margen\n# Resultado: Margen MÁS AMPLIO pero menos preciso en los datos de entrenamiento\nsvm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42, max_iter = 10000)\n\n# ============================================\n# MODELO 2: C = 100 (Regularización DÉBIL)\n# ============================================\n# C alto = Penaliza fuertemente las violaciones del margen\n# Resultado: Margen MÁS ESTRECHO pero más preciso en los datos de entrenamiento\nsvm_clf2 = LinearSVC(C=100, loss=\"hinge\", random_state=42, max_iter = 10000)\n\n# Creamos pipelines para ambos modelos\nscaled_svm_clf1 = Pipeline([\n        (\"scaler\", scaler),\n        (\"linear_svc\", svm_clf1),\n    ])\n\nscaled_svm_clf2 = Pipeline([\n        (\"scaler\", scaler),\n        (\"linear_svc\", svm_clf2),\n    ])\n\n# Entrenamos ambos modelos con los mismos datos\nscaled_svm_clf1.fit(X, y)\nscaled_svm_clf2.fit(X, y)\n# NOTA: Veremos cómo C afecta la frontera de decisión y el ancho del margen"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KDxSYiG_Yph",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================\n# CONVERSIÓN A PARÁMETROS SIN ESCALAR\n# ============================================\n# Para visualizar correctamente, necesitamos convertir los parámetros del modelo\n# (que fueron entrenados con datos escalados) de vuelta a la escala original\n\n# Calculamos el bias (intercepto) en la escala original\nb1 = svm_clf1.decision_function([-scaler.mean_ / scaler.scale_])\nb2 = svm_clf2.decision_function([-scaler.mean_ / scaler.scale_])\n\n# Calculamos los pesos (coeficientes) en la escala original\n# Dividimos por scale_ para \"deshacer\" el escalado\nw1 = svm_clf1.coef_[0] / scaler.scale_\nw2 = svm_clf2.coef_[0] / scaler.scale_\n\n# Asignamos los parámetros convertidos de vuelta a los modelos\nsvm_clf1.intercept_ = np.array([b1])\nsvm_clf2.intercept_ = np.array([b2])\nsvm_clf1.coef_ = np.array([w1])\nsvm_clf2.coef_ = np.array([w2])\n\n# ============================================\n# IDENTIFICACIÓN MANUAL DE VECTORES DE SOPORTE\n# ============================================\n# LinearSVC no identifica automáticamente los vectores de soporte,\n# así que los encontramos manualmente\n\n# Convertimos las etiquetas a {-1, +1} (formato estándar de SVM)\nt = y * 2 - 1\n\n# Los vectores de soporte son los puntos que:\n# - Están en el margen: t * (X·w + b) = 1\n# - Violan el margen: t * (X·w + b) < 1\n# Aquí buscamos todos los puntos con t * (X·w + b) < 1\nsupport_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\nsupport_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\n\n# Guardamos los vectores de soporte en los modelos\nsvm_clf1.support_vectors_ = X[support_vectors_idx1]\nsvm_clf2.support_vectors_ = X[support_vectors_idx2]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31iDGXvg_Ypk",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "5a4fc362-162e-41e4-a1be-a2c799e93767",
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================\n# VISUALIZACIÓN: COMPARACIÓN DE C=1 vs C=100\n# ============================================\n# Este gráfico muestra el impacto del parámetro C en la frontera de decisión\n\nplt.figure(figsize=(12,3.2))\n\n# GRÁFICO IZQUIERDO: C = 1 (Regularización FUERTE)\nplt.subplot(121)\n# Dibujamos los puntos de datos\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\n# Dibujamos la frontera de decisión\nplot_svc_decision_boundary(svm_clf1, 4, 6)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.title(\"$C = {}$\".format(svm_clf1.C), fontsize=16)\nplt.axis([4, 6, 0.8, 2.8])\n# NOTA: Con C=1 vemos un margen MÁS AMPLIO (líneas punteadas más separadas)\n# Algunos puntos pueden estar dentro del margen (violaciones permitidas)\n\n# GRÁFICO DERECHO: C = 100 (Regularización DÉBIL)\nplt.subplot(122)\n# Dibujamos los puntos de datos\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n# Dibujamos la frontera de decisión\nplot_svc_decision_boundary(svm_clf2, 4, 6)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.title(\"$C = {}$\".format(svm_clf2.C), fontsize=16)\nplt.axis([4, 6, 0.8, 2.8])\n# NOTA: Con C=100 vemos un margen MÁS ESTRECHO (líneas punteadas más juntas)\n# El modelo penaliza más las violaciones, ajustándose más a los datos de entrenamiento\n# ¡Cuidado con el overfitting!"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkGRGZU5_Ypm"
   },
   "source": [
    "# Non-linear classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cok48GTd_Ypn",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "c13ef886-0e0e-44e6-8ece-28eda052ae07",
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================\n# INTRODUCCIÓN A CLASIFICACIÓN NO LINEAL\n# ============================================\n# A veces los datos NO son linealmente separables en el espacio original\n# Solución: Transformarlos a un espacio de mayor dimensión\n\n# Creamos un dataset 1D simple (no separable linealmente)\nX1D = np.linspace(-4, 4, 9).reshape(-1, 1)\n# Añadimos una característica polinómica: x2 = x1²\nX2D = np.c_[X1D, X1D**2]  # Ahora tenemos [x1, x1²]\ny = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n\nplt.figure(figsize=(11, 4))\n\n# ============================================\n# GRÁFICO IZQUIERDO: Espacio original 1D (NO separable linealmente)\n# ============================================\nplt.subplot(121)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')  # Línea horizontal en y=0\n# Dibujamos los puntos sobre el eje x\nplt.plot(X1D[:, 0][y==0], np.zeros(4), \"bs\")  # Clase 0 en azul\nplt.plot(X1D[:, 0][y==1], np.zeros(5), \"g^\")  # Clase 1 en verde\nplt.gca().get_yaxis().set_ticks([])\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.axis([-4.5, 4.5, -0.2, 0.2])\n# NOTA: En 1D, no hay una línea recta que separe perfectamente las dos clases\n\n# ============================================\n# GRÁFICO DERECHO: Espacio transformado 2D (SÍ separable linealmente)\n# ============================================\nplt.subplot(122)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\n# Dibujamos los puntos en el espacio 2D [x1, x1²]\nplt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], \"bs\")  # Clase 0\nplt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], \"g^\")  # Clase 1\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\nplt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\n# Dibujamos una línea horizontal que separa las clases\nplt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\nplt.axis([-4.5, 4.5, -1, 17])\n# NOTA: En 2D (con la característica x1²), las clases SÍ son linealmente separables\n# Esta es la idea clave del \"kernel trick\" en SVM\n\nplt.subplots_adjust(right=1);"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3kfeWmx_Ypq",
    "outputId": "8b0c1c18-18fc-473e-9a3c-cc7f56c4e526"
   },
   "outputs": [],
   "source": "# ============================================\n# CREACIÓN DE UN DATASET NO LINEAL: \"MOONS\"\n# ============================================\n# make_moons genera un dataset con forma de dos lunas entrelazadas\n# Este dataset NO es linealmente separable\n\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\n\n# Generamos 100 muestras con algo de ruido\nX, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n\n# Función auxiliar para visualizar el dataset\ndef plot_dataset(X, y, axes):\n    \"\"\"\n    Dibuja un dataset 2D con dos clases\n    \"\"\"\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")  # Clase 0 en cuadrados azules\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")  # Clase 1 en triángulos verdes\n    plt.axis(axes)\n    plt.grid(True, which='both')\n    plt.xlabel(r\"$x_1$\", fontsize=20)\n    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n\n# Visualizamos el dataset\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n# NOTA: Las dos clases tienen forma de \"lunas\" y están entrelazadas\n# No es posible separarlas con una línea recta (SVM lineal no funcionará bien)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2wCYFGx_Ypu",
    "outputId": "3efc3c61-3aeb-44e1-e7b2-5b85e5215ab1"
   },
   "outputs": [],
   "source": "# ============================================\n# ENFOQUE 1: CARACTERÍSTICAS POLINÓMICAS + SVM LINEAL\n# ============================================\n# Una forma de manejar datos no lineales es:\n# 1. Añadir características polinómicas (x², x³, x1*x2, etc.)\n# 2. Aplicar SVM lineal en el espacio de características ampliado\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\npolynomial_svm_clf = Pipeline([\n        # Paso 1: Crear características polinómicas hasta grado 3\n        # Por ejemplo, de [x1, x2] crea: [1, x1, x2, x1², x1*x2, x2², x1³, ...]\n        (\"poly_features\", PolynomialFeatures(degree=3)),\n        \n        # Paso 2: Escalar las características (CRÍTICO)\n        (\"scaler\", StandardScaler()),\n        \n        # Paso 3: Aplicar SVM lineal en el espacio de características ampliado\n        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", max_iter = 10000, random_state=42))\n    ])\n\n# Entrenamos el pipeline completo\npolynomial_svm_clf.fit(X, y)\n# NOTA: Aunque usamos LinearSVC, la frontera de decisión será NO LINEAL\n# en el espacio original porque trabajamos con características polinómicas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqona-Hq_Ypw",
    "outputId": "1da2cd71-3bd1-4387-a037-79483411aad5",
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================\n# FUNCIÓN PARA VISUALIZAR PREDICCIONES DEL MODELO\n# ============================================\ndef plot_predictions(clf, axes):\n    \"\"\"\n    Dibuja las regiones de decisión y los contornos de un clasificador\n    \n    Parámetros:\n    - clf: clasificador entrenado\n    - axes: límites del gráfico [xmin, xmax, ymin, ymax]\n    \"\"\"\n    # Creamos una malla de puntos para evaluar el modelo\n    x0s = np.linspace(axes[0], axes[1], 100)  # 100 puntos en el eje x\n    x1s = np.linspace(axes[2], axes[3], 100)  # 100 puntos en el eje y\n    x0, x1 = np.meshgrid(x0s, x1s)  # Creamos la malla 2D\n    \n    # Convertimos la malla a formato de array (cada fila es un punto [x0, x1])\n    X = np.c_[x0.ravel(), x1.ravel()]\n    \n    # Predecimos la clase para cada punto de la malla\n    y_pred = clf.predict(X).reshape(x0.shape)\n    \n    # Calculamos el valor de la función de decisión (distancia al hiperplano)\n    y_decision = clf.decision_function(X).reshape(x0.shape)\n    \n    # Dibujamos las regiones de decisión con colores de fondo\n    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n    \n    # Dibujamos los contornos de la función de decisión\n    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n\n# ============================================\n# VISUALIZACIÓN DEL RESULTADO\n# ============================================\n# Dibujamos las regiones de decisión del modelo\nplot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n# Superponemos los puntos de datos originales\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5]);\n# NOTA: Observa cómo la frontera de decisión es CURVA (no lineal)\n# a pesar de usar LinearSVC internamente"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7njdFDX_Ypz",
    "outputId": "d5cf76cc-b172-4235-e0fd-69022d206c38"
   },
   "outputs": [],
   "source": "# ============================================\n# ENFOQUE 2: KERNEL POLINÓMICO (KERNEL TRICK)\n# ============================================\n# En lugar de crear características polinómicas explícitamente,\n# usamos el \"kernel trick\" que es MUCHO más eficiente\n\nfrom sklearn.svm import SVC\n\npoly_kernel_svm_clf = Pipeline([\n        # Paso 1: Escalar las características\n        (\"scaler\", StandardScaler()),\n        \n        # Paso 2: SVM con kernel polinómico\n        # - kernel=\"poly\": Usa el kernel polinómico (no necesita PolynomialFeatures)\n        # - degree=3: Grado del polinomio\n        # - coef0=1: Término independiente (controla la influencia de términos de alto grado)\n        # - C=5: Parámetro de regularización\n        (\"svm_clf\", SVC(kernel = \"poly\", degree=3, coef0=1, C=5))\n    ])\n\n# Entrenamos el modelo\npoly_kernel_svm_clf.fit(X, y)\n# NOTA: El \"kernel trick\" calcula productos escalares en el espacio de alta dimensión\n# SIN crear explícitamente las características polinómicas\n# ¡Esto ahorra MUCHA memoria y tiempo de cómputo!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4D4IoYg_Yp1",
    "outputId": "26fb3dbf-6ce8-4eec-c254-66533e9e3e2a"
   },
   "outputs": [],
   "source": "# ============================================\n# DEMOSTRACIÓN: OVERFITTING CON HIPERPARÁMETROS EXTREMOS\n# ============================================\n# Creamos un modelo con hiperparámetros muy altos para mostrar overfitting\n\npoly100_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        \n        # SVM con parámetros extremos:\n        # - degree=10: Grado DEMASIADO ALTO del polinomio\n        # - coef0=100: Término independiente MUY ALTO\n        # - C=5: Misma regularización que antes\n        (\"svm_clf\", SVC(kernel = \"poly\", degree=10, coef0=100, C=5))\n    ])\n\n# Entrenamos el modelo con parámetros extremos\npoly100_kernel_svm_clf.fit(X, y)\n# NOTA: Este modelo probablemente tendrá OVERFITTING\n# Se ajustará demasiado a los datos de entrenamiento y generalizará mal"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0DzoAXk_Yp5",
    "outputId": "b94da4dc-9701-4785-8a58-a4d4d368bd1c"
   },
   "outputs": [],
   "source": "# ============================================\n# COMPARACIÓN VISUAL: BUENOS vs MALOS HIPERPARÁMETROS\n# ============================================\n\nplt.figure(figsize=(11, 4))\n\n# GRÁFICO IZQUIERDO: Hiperparámetros RAZONABLES (d=3, r=1, C=5)\nplt.subplot(121)\nplot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\nplt.title(r\"$d=3, r=1, C=5$\", fontsize=18)\n# NOTA: La frontera de decisión es suave y sigue la forma general de los datos\n# Este modelo probablemente generalizará bien a datos nuevos\n\n# GRÁFICO DERECHO: Hiperparámetros EXTREMOS (d=10, r=100, C=5)\nplt.subplot(122)\nplot_predictions(poly100_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\nplt.title(r\"$d=10, r=100, C=5$\", fontsize=18)\n# NOTA: La frontera de decisión es MUY irregular y compleja\n# Este modelo tiene OVERFITTING - se ajusta demasiado al ruido en los datos\n# Probablemente tendrá mal rendimiento con datos nuevos\n# \n# LECCIÓN: Es importante elegir hiperparámetros adecuados\n# mediante validación cruzada o búsqueda de hiperparámetros"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LXaZ3GUR_YqF"
   },
   "source": [
    "# Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qONUHlFt_YqF"
   },
   "outputs": [],
   "source": "# ============================================\n# REGRESIÓN CON SVM (SUPPORT VECTOR REGRESSION - SVR)\n# ============================================\n# SVM no solo sirve para clasificación, también para regresión\n\n# Configuramos la semilla aleatoria para reproducibilidad\nnp.random.seed(42)\n\n# Generamos un dataset sintético para regresión\nm = 50  # 50 muestras\nX = 2 * np.random.rand(m, 1)  # Valores aleatorios entre 0 y 2\n\n# Generamos la variable objetivo con una relación lineal + ruido\n# y = 4 + 3*X + ruido\ny = (4 + 3 * X + np.random.randn(m, 1)).ravel()\n# NOTA: .ravel() convierte el array 2D en 1D (requerido por algunos estimadores)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6eu6Rp8_YqI",
    "outputId": "e7048317-5be1-4255-9ab0-498ab45c0fbb"
   },
   "outputs": [],
   "source": "# ============================================\n# CREACIÓN Y ENTRENAMIENTO DE UN MODELO SVR LINEAL\n# ============================================\n\nfrom sklearn.svm import LinearSVR\n\n# Creamos un modelo de regresión SVM lineal\n# - epsilon=1.5: Define el ancho del \"tubo\" de tolerancia\n#   Los puntos dentro del tubo (error < epsilon) NO afectan al modelo\n#   Solo los puntos FUERA del tubo son penalizados\n# - random_state=42: Para reproducibilidad\nsvm_reg = LinearSVR(epsilon=1.5, random_state=42)\n\n# Entrenamos el modelo\nsvm_reg.fit(X, y)\n# NOTA: A diferencia de la regresión lineal tradicional que minimiza el error cuadrático,\n# SVR busca una línea donde la mayoría de puntos estén dentro de un \"tubo\" de ancho epsilon"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNkCdFFl_YqK"
   },
   "outputs": [],
   "source": "# ============================================\n# COMPARACIÓN DE DIFERENTES VALORES DE EPSILON\n# ============================================\n# Creamos dos modelos SVR con diferentes anchos de tubo\n\n# MODELO 1: epsilon = 1.5 (tubo ANCHO)\nsvm_reg1 = LinearSVR(epsilon=1.5, random_state=42)\nsvm_reg1.fit(X, y)\n\n# MODELO 2: epsilon = 0.5 (tubo ESTRECHO)\nsvm_reg2 = LinearSVR(epsilon=0.5, random_state=42)\nsvm_reg2.fit(X, y)\n\n# ============================================\n# IDENTIFICACIÓN DE VECTORES DE SOPORTE\n# ============================================\n# En SVR, los vectores de soporte son los puntos FUERA del tubo epsilon\ndef find_support_vectors(svm_reg, X, y):\n    \"\"\"\n    Encuentra los vectores de soporte para un modelo SVR\n    Son los puntos cuyo error es >= epsilon\n    \"\"\"\n    y_pred = svm_reg.predict(X)  # Predicciones del modelo\n    # Los vectores de soporte están FUERA del tubo (|error| >= epsilon)\n    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)\n    return np.argwhere(off_margin)\n\n# Identificamos los vectores de soporte para ambos modelos\nsvm_reg1.support_ = find_support_vectors(svm_reg1, X, y)\nsvm_reg2.support_ = find_support_vectors(svm_reg2, X, y)\n\n# Variables auxiliares para la visualización\neps_x1 = 1\neps_y_pred = svm_reg1.predict([[eps_x1]])\n# NOTA: Con epsilon grande, MENOS puntos serán vectores de soporte\n# Con epsilon pequeño, MÁS puntos serán vectores de soporte"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPJ5wupO_YqN",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "e01b2de9-0def-4845-f649-314b135a1ed3",
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================\n# FUNCIÓN PARA VISUALIZAR REGRESIÓN SVM\n# ============================================\ndef plot_svm_regression(svm_reg, X, y, axes):\n    \"\"\"\n    Dibuja la línea de regresión y el tubo epsilon de un modelo SVR\n    \n    Parámetros:\n    - svm_reg: modelo SVR entrenado\n    - X, y: datos de entrenamiento\n    - axes: límites del gráfico [xmin, xmax, ymin, ymax]\n    \"\"\"\n    # Creamos puntos para graficar la línea de regresión\n    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n    y_pred = svm_reg.predict(x1s)\n    \n    # Dibujamos la línea de regresión (predicción)\n    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n    \n    # Dibujamos los límites del tubo epsilon\n    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")  # Límite superior\n    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")  # Límite inferior\n    \n    # Destacamos los vectores de soporte (puntos fuera del tubo)\n    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n    \n    # Dibujamos todos los puntos de datos\n    plt.plot(X, y, \"bo\")\n    \n    plt.xlabel(r\"$x_1$\", fontsize=18)\n    plt.legend(loc=\"upper left\", fontsize=18)\n    plt.axis(axes)\n\n# ============================================\n# COMPARACIÓN VISUAL: EPSILON = 1.5 vs EPSILON = 0.5\n# ============================================\nplt.figure(figsize=(9, 4))\n\n# GRÁFICO IZQUIERDO: epsilon = 1.5 (tubo ANCHO)\nplt.subplot(121)\nplot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])\nplt.title(r\"$\\epsilon = {}$\".format(svm_reg1.epsilon), fontsize=18)\nplt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n\n# Dibujamos una flecha mostrando el tamaño de epsilon\nplt.annotate(\n        '', xy=(eps_x1, eps_y_pred), xycoords='data',\n        xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),\n        textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}\n    )\nplt.text(0.91, 5.6, r\"$\\epsilon$\", fontsize=20)\n# NOTA: Con tubo ANCHO, menos puntos son vectores de soporte (puntos rojos)\n\n# GRÁFICO DERECHO: epsilon = 0.5 (tubo ESTRECHO)\nplt.subplot(122)\nplot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])\nplt.title(r\"$\\epsilon = {}$\".format(svm_reg2.epsilon), fontsize=18)\n# NOTA: Con tubo ESTRECHO, MÁS puntos son vectores de soporte\n# El modelo es más sensible a los errores pequeños"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq9AAH1k_YqS"
   },
   "outputs": [],
   "source": "# ============================================\n# GENERACIÓN DE DATOS NO LINEALES PARA REGRESIÓN\n# ============================================\n# Ahora crearemos datos con una relación NO LINEAL (cuadrática)\n\nnp.random.seed(42)\nm = 100  # 100 muestras\nX = 2 * np.random.rand(m, 1) - 1  # Valores aleatorios entre -1 y 1\n\n# Generamos datos con una relación CUADRÁTICA + ruido\n# y = 0.2 + 0.1*X + 0.5*X² + ruido\ny = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(m, 1)/10).ravel()\n# NOTA: Esta es una relación NO LINEAL (parabólica)\n# Un modelo lineal no podrá capturar bien esta relación"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "T_9UnhDD_YqU"
   },
   "source": [
    "**Note**: to be future-proof, we set `gamma=\"scale\"`, as this will be the default value in Scikit-Learn 0.22."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vsC0tLV_YqU",
    "outputId": "c9383d55-e3dc-454a-c499-fe6448640c5d"
   },
   "outputs": [],
   "source": "# ============================================\n# REGRESIÓN NO LINEAL CON KERNEL POLINÓMICO\n# ============================================\n# Para capturar la relación cuadrática, usamos SVR con kernel polinómico\n\nfrom sklearn.svm import SVR\n\n# Creamos un modelo SVR con kernel polinómico\nsvm_poly_reg = SVR(kernel=\"poly\",     # Kernel polinómico (para relaciones no lineales)\n                   degree=2,          # Grado 2 (cuadrático) - coincide con nuestros datos\n                   C=100,             # Regularización (C alto = menos regularización)\n                   epsilon=0.1,       # Ancho del tubo de tolerancia\n                   gamma=\"scale\")     # Parámetro gamma (influencia de cada punto de entrenamiento)\n\n# Entrenamos el modelo\nsvm_poly_reg.fit(X, y)\n# NOTA: Este modelo puede capturar relaciones NO LINEALES\n# El kernel polinómico de grado 2 es perfecto para datos cuadráticos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3i-8t4Y_YqX",
    "outputId": "c072cc54-264b-4dba-f6ea-6041e5eca204"
   },
   "outputs": [],
   "source": "# ============================================\n# COMPARACIÓN DEL PARÁMETRO C EN REGRESIÓN NO LINEAL\n# ============================================\n\nfrom sklearn.svm import SVR\n\n# MODELO 1: C = 100 (Regularización DÉBIL)\n# Penaliza mucho las violaciones del tubo epsilon\n# El modelo se ajustará más a los datos de entrenamiento\nsvm_poly_reg1 = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1, gamma=\"scale\")\nsvm_poly_reg1.fit(X, y)\n\n# MODELO 2: C = 0.01 (Regularización FUERTE)\n# Permite más violaciones del tubo epsilon\n# El modelo será más simple y generalizará mejor (pero puede ser menos preciso)\nsvm_poly_reg2 = SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1, gamma=\"scale\")\nsvm_poly_reg2.fit(X, y)\n\n# NOTA: El parámetro C controla el trade-off entre:\n# - Simplicidad del modelo (C bajo)\n# - Precisión en los datos de entrenamiento (C alto)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stBMXBCl_YqZ",
    "outputId": "2595f8bd-7114-41c4-b0af-1753a4bfa708"
   },
   "outputs": [],
   "source": "# ============================================\n# VISUALIZACIÓN FINAL: COMPARACIÓN DE C EN SVR POLINÓMICO\n# ============================================\n\nplt.figure(figsize=(9, 4))\n\n# GRÁFICO IZQUIERDO: C=100, epsilon=0.1 (Regularización DÉBIL)\nplt.subplot(121)\nplot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\nplt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(\n    svm_poly_reg1.degree, \n    svm_poly_reg1.C, \n    svm_poly_reg1.epsilon), fontsize=18)\nplt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n# NOTA: Con C=100, el modelo se ajusta MUY bien a los datos\n# La curva sigue de cerca los puntos de entrenamiento\n# Menos puntos rojos (vectores de soporte) porque el tubo se ajusta bien\n\n# GRÁFICO DERECHO: C=0.01, epsilon=0.1 (Regularización FUERTE)\nplt.subplot(122)\nplot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\nplt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(\n    svm_poly_reg2.degree, \n    svm_poly_reg2.C, \n    svm_poly_reg2.epsilon), fontsize=18)\n# NOTA: Con C=0.01, el modelo es MÁS SIMPLE\n# La curva es menos precisa pero puede generalizar mejor\n# Más puntos rojos (vectores de soporte) porque permite más violaciones\n#\n# CONCLUSIÓN: Elige C según tus necesidades:\n# - C alto: Mayor precisión en entrenamiento (riesgo de overfitting)\n# - C bajo: Mayor generalización (puede tener underfitting)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "suppor_vector_machines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "nav_menu": {},
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}