{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MQGm-heq1Tua"
   },
   "source": [
    "# **Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta clase aprenderemos sobre uno de los algoritmos mas fundamentales y versatiles en machine learning: los arboles de decision. \n",
    "Vamos con un poco de teoria  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduccion: Que es un Arbol de Decision?\n",
    "\n",
    "Un **arbol de decision** es un modelo de machine learning que toma decisiones mediante una serie de preguntas organizadas en forma de arbol. Es similar a como tomariamos decisiones en la vida real.\n",
    "\n",
    "### Ejemplo cotidiano:\n",
    "\n",
    "Imagina que quieres decidir si salir a correr:\n",
    "- **Pregunta 1:** ¿Esta lloviendo?\n",
    "  - Si → No salgo\n",
    "  - No → Siguiente pregunta\n",
    "- **Pregunta 2:** ¿Tengo tiempo libre?\n",
    "  - Si → Salgo a correr\n",
    "  - No → No salgo\n",
    "\n",
    "Este proceso de preguntas secuenciales es exactamente como funciona un arbol de decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Componentes de un Arbol de Decision\n",
    "\n",
    "### Elementos principales:\n",
    "\n",
    "1. **Nodo raiz (Root Node):** El primer nodo del arbol, donde comienza la decision\n",
    "2. **Nodos internos (Internal Nodes):** Representan preguntas o condiciones sobre los atributos\n",
    "3. **Ramas (Branches):** Conectan los nodos y representan el resultado de una decision\n",
    "4. **Nodos hoja (Leaf Nodes):** Los nodos finales que contienen la prediccion o decision final\n",
    "\n",
    "### Terminologia importante:\n",
    "\n",
    "- **Split (Division):** El proceso de dividir un nodo en sub-nodos\n",
    "- **Profundidad (Depth):** La longitud del camino mas largo desde la raiz hasta una hoja\n",
    "- **Pureza (Purity):** Que tan homogeneos son los datos en un nodo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--  \n",
    "\n",
    "\n",
    "### 3. Tipos de Arboles de Decision\n",
    "\n",
    "### 3.1 Arboles de Clasificacion\n",
    "\n",
    "Se utilizan cuando la variable objetivo es **categorica** (discreta).\n",
    "\n",
    "**Ejemplos:**\n",
    "- Predecir si un correo es spam o no spam\n",
    "- Clasificar especies de flores\n",
    "- Determinar si un cliente comprara un producto (si/no)\n",
    "\n",
    "### 3.2 Arboles de Regresion\n",
    "\n",
    "Se utilizan cuando la variable objetivo es **continua** (numerica).\n",
    "\n",
    "**Ejemplos:**\n",
    "- Predecir el precio de una casa\n",
    "- Estimar la temperatura de manana\n",
    "- Calcular el salario de un empleado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--  \n",
    "\n",
    "### 4. Como Funciona: El Proceso de Division\n",
    "\n",
    "El algoritmo construye el arbol mediante divisiones sucesivas de los datos. Pero, ¿como decide donde dividir?\n",
    "\n",
    "### 4.1 Criterios para Arboles de Clasificacion\n",
    "\n",
    "#### Indice de Gini\n",
    "\n",
    "Mide la \"impureza\" de un nodo. Un nodo es puro si todos los ejemplos pertenecen a la misma clase.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Gini = 1 - Σ(p_i)²\n",
    "```\n",
    "\n",
    "Donde `p_i` es la probabilidad de cada clase.\n",
    "\n",
    "- **Gini = 0:** Nodo completamente puro (perfecta clasificacion)\n",
    "- **Gini = 0.5:** Maxima impureza (clases equilibradas)\n",
    "\n",
    "#### Entropia (Ganancia de Informacion)\n",
    "\n",
    "Mide el desorden o incertidumbre en los datos.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "Entropia = -Σ(p_i * log₂(p_i))\n",
    "```\n",
    "\n",
    "- **Entropia = 0:** Nodo puro\n",
    "- **Entropia alta:** Mucha incertidumbre\n",
    "\n",
    "### 4.2 Criterio para Arboles de Regresion\n",
    "\n",
    "#### Error Cuadratico Medio (MSE)\n",
    "\n",
    "Para problemas de regresion, se busca minimizar la varianza de los valores en cada nodo.\n",
    "\n",
    "**Formula:**\n",
    "```\n",
    "MSE = (1/n) * Σ(y_i - ȳ)²\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- `y_i` son los valores reales\n",
    "- `ȳ` es la media de los valores en el nodo\n",
    "- `n` es el numero de observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "GyO_LmBX1Tuc"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5Qv5M-b_1Tud"
   },
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9NzCNFg1Tud"
   },
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para el análisis\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "\n",
    "# Configuramos una semilla aleatoria para reproducibilidad de resultados\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuramos matplotlib para mostrar gráficos en el notebook\n",
    "# %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# Configuramos el tamaño de las etiquetas en los gráficos\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Definimos las rutas donde guardar las figuras generadas\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"decision_trees\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "# Creamos el directorio si no existe\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    \"\"\"\n",
    "    Función para guardar figuras con formato y resolución específica\n",
    "    \"\"\"\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "C9v2CRxV1Tuk"
   },
   "source": [
    "# Training and visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cmfxZUi1Tul",
    "outputId": "3e5a601c-ef12-4e71-e690-30d741bdbad3"
   },
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para trabajar con árboles de decisión\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Cargamos el dataset de flores Iris\n",
    "iris = load_iris()\n",
    "# Seleccionamos solo las columnas de longitud y anchura del pétalo (columnas 2 y 3)\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "# Variable objetivo: tipo de flor\n",
    "y = iris.target\n",
    "\n",
    "# Creamos un clasificador de árbol de decisión\n",
    "# max_depth=3: profundidad máxima del árbol para evitar overfitting\n",
    "# random_state=42: semilla para reproducibilidad\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3,\n",
    "                                  random_state=42)\n",
    "# Entrenamos el modelo con los datos\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultamos los nombres de las características del dataset Iris\n",
    "# Esto nos ayuda a entender qué representa cada columna\n",
    "iris['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultamos los nombres de las clases (tipos de flores) que puede predecir el modelo\n",
    "# Estas son las 3 especies de flores en el dataset Iris\n",
    "iris['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la estructura del árbol de decisión entrenado\n",
    "plt.figure(figsize=(10,6))\n",
    "# plot_tree: función de sklearn para visualizar árboles de decisión\n",
    "sklearn.tree.plot_tree(tree_clf,\n",
    "               # Solo usamos las características de pétalo (columnas 2 y 3)\n",
    "               feature_names=iris.feature_names[2:],\n",
    "               # Nombres de las clases para las hojas del árbol\n",
    "               class_names=iris.target_names,\n",
    "               # filled=True: colorea los nodos según la clase mayoritaria\n",
    "               filled = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicación\n",
    "\n",
    "Este diagrama muestra un **modelo de clasificación** que intenta predecir la **especie de una flor del conjunto de datos Iris** (uno de los datasets clásicos en data science).  \n",
    "Las tres clases posibles son:\n",
    "- Setosa\n",
    "- Versicolor\n",
    "- Virginica\n",
    "\n",
    "El modelo usa las variables de entrada:\n",
    "- Petal length (largo del pétalo)\n",
    "- Petal width (ancho del pétalo)\n",
    "\n",
    "para decidir a qué clase pertenece cada flor.\n",
    "\n",
    "\n",
    "\n",
    "### Cómo leerlo\n",
    "Cada **nodo** (caja) representa una **pregunta o condición** sobre una variable.  \n",
    "Ejemplo:  \n",
    "`petal length (cm) <= 2.45`  \n",
    "\n",
    "Si la condición es **verdadera (True)**, se sigue la **rama izquierda**;  \n",
    "si es **falsa (False)**, se sigue la **rama derecha**.\n",
    "\n",
    "   \n",
    "### Significado de los valores dentro de cada nodo\n",
    "1. **Condición (petal length <= ...)**: el criterio de división.  \n",
    "2. **gini**: mide la **impureza** del nodo (qué tan mezcladas están las clases).  \n",
    "   - 0 → todas las muestras son de una sola clase (puro)  \n",
    "   - 0.5 → mezcla equilibrada de dos clases  \n",
    "3. **samples**: cuántas muestras hay en ese nodo.  \n",
    "4. **value = [x, y, z]**: cuántas observaciones de cada clase hay en ese nodo (en este caso [setosa, versicolor, virginica]).  \n",
    "5. **class = ...**: la clase mayoritaria en ese nodo (la predicción del modelo ahí).\n",
    "\n",
    "  \n",
    "\n",
    "### Colores\n",
    "- Los colores indican la **clase predominante** en ese nodo.  \n",
    "  - Naranja → setosa  \n",
    "  - Verde → versicolor  \n",
    "  - Violeta → virginica  \n",
    "- Cuanto más **intenso** el color, más puro es el nodo (más baja la impureza Gini).\n",
    "\n",
    "  \n",
    "\n",
    "### Funcionamiento del árbol\n",
    "1. El árbol empieza en la **raíz** (arriba):  \n",
    "   - Si el pétalo mide ≤ 2.45 cm → la flor es **setosa**.  \n",
    "   - Si no → pasa a evaluar otras condiciones.\n",
    "2. A partir de ahí, el árbol sigue dividiendo los datos según el **criterio que mejor separa las clases**.\n",
    "3. Al llegar a las hojas (nodos sin más ramas), se obtiene la **predicción final**.\n",
    "\n",
    "  \n",
    "\n",
    "Resumen:  \n",
    "“Este árbol de decisión clasifica las flores Iris según las medidas de sus pétalos.  \n",
    "Cada nodo hace una pregunta, y dependiendo de la respuesta, se sigue una rama.  \n",
    "El valor de *gini* nos indica cuán mezcladas están las clases en ese punto, y al final cada hoja representa una predicción.  \n",
    "Es un modelo muy interpretable, ideal para entender cómo toma decisiones un algoritmo.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAFvUu3T1Tuv",
    "outputId": "e2ed9f46-8807-42e3-8c5a-ccc7a0251b91"
   },
   "outputs": [],
   "source": [
    "#¿como separa las clases? o como visualizar las fronteras de decisión de un clasificador\n",
    "\n",
    "# Importamos ListedColormap para crear mapas de colores personalizados\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3], iris=True, legend=False, plot_training=True):\n",
    "    \"\"\"\n",
    "    Función para visualizar las fronteras de decisión de un clasificador\n",
    "    \"\"\"\n",
    "    # Creamos una malla de puntos para evaluar el clasificador en todo el espacio\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    # Preparamos los datos para hacer predicciones en toda la malla\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    # Predecimos la clase para cada punto de la malla\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    # Definimos colores personalizados para las regiones de decisión\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff'])\n",
    "    # Dibujamos las regiones coloreadas según las predicciones\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if not iris:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    # Si plot_training=True, dibujamos también los puntos de entrenamiento\n",
    "    if plot_training:\n",
    "        plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\n",
    "        plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"gs\", label=\"Iris versicolor\")\n",
    "        plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"b^\", label=\"Iris virginica\")\n",
    "        plt.axis(axes)\n",
    "    # Configuramos las etiquetas de los ejes\n",
    "    if iris:\n",
    "        plt.xlabel(\"Petal length\", fontsize=14)\n",
    "        plt.ylabel(\"Petal width\", fontsize=14)\n",
    "    else:\n",
    "        plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "        plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)\n",
    "    if legend:\n",
    "        plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "# Creamos una visualización de las fronteras de decisión del árbol\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "# Dibujamos líneas que muestran cómo el árbol divide el espacio\n",
    "plt.plot([2.45, 2.45], [0, 3], \"k-\", linewidth=2)  # Primera división\n",
    "plt.plot([2.45, 7.5], [1.75, 1.75], \"k--\", linewidth=2)  # Segunda división\n",
    "plt.plot([4.95, 4.95], [0, 1.75], \"k:\", linewidth=2)  # Tercera división\n",
    "plt.plot([4.85, 4.85], [1.75, 3], \"k:\", linewidth=2)  # Cuarta división\n",
    "# Añadimos texto para indicar la profundidad de cada división\n",
    "plt.text(1.40, 1.0, \"Depth=1\", fontsize=15)\n",
    "plt.text(3.2, 1.80, \"Depth=2\", fontsize=13)\n",
    "plt.text(4.05, 0.5, \"(Depth=3)\", fontsize=11)\n",
    "\n",
    "# Guardamos la figura\n",
    "save_fig(\"decision_tree_decision_boundaries_plot\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regiones de decisión de un árbol de decisión\n",
    "\n",
    "Este gráfico muestra cómo un **árbol de decisión** divide el espacio de características para clasificar las flores del conjunto de datos Iris según sus medidas.  \n",
    "En este caso, se usan dos variables:\n",
    "- **Petal length (largo del pétalo)**\n",
    "- **Petal width (ancho del pétalo)**\n",
    "\n",
    "Cada punto representa una flor, y el color indica su clase:\n",
    "- Amarillo: setosa  \n",
    "- Verde: versicolor  \n",
    "- Azul: virginica\n",
    "\n",
    "  \n",
    "\n",
    "### Ejes y regiones\n",
    "- El eje **x** muestra el largo del pétalo.  \n",
    "- El eje **y** muestra el ancho del pétalo.  \n",
    "- Cada región coloreada corresponde a una **clase predicha** por el árbol.  \n",
    "- Las **líneas negras** representan las **fronteras de decisión** creadas por el modelo.\n",
    "\n",
    "  \n",
    "\n",
    "### Profundidad del árbol (Depth)\n",
    "- **Depth = 1:** El árbol hace la primera división (vertical). Separa las setosas del resto.  \n",
    "- **Depth = 2:** Segunda división (horizontal). Separa las versicolor de las virginica.  \n",
    "- **Depth = 3:** Nivel más profundo (en este ejemplo), ajustando mejor los límites entre clases.\n",
    "\n",
    "La **profundidad del árbol** indica cuántas divisiones (nodos) se han realizado desde la raíz.  \n",
    "Un árbol más profundo puede captar más patrones, pero también corre el riesgo de **sobreajustar** (overfitting).\n",
    "\n",
    "  \n",
    "\n",
    "### Interpretación general\n",
    "El gráfico muestra cómo el árbol de decisión **aprende fronteras rectas y jerárquicas** para separar las clases en función de las variables seleccionadas.  \n",
    "Aunque es un modelo simple, ofrece una **interpretación clara y visual** del proceso de clasificación.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "mE4rKy9B1Tux"
   },
   "source": [
    "# Predicting classes and class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffYy1rsq1Tuy",
    "outputId": "a2167a52-0e90-44a1-cf13-5dc2da6ab436"
   },
   "outputs": [],
   "source": [
    "# Predicción de probabilidades para un nuevo punto de datos\n",
    "# El método predict_proba devuelve las probabilidades de pertenencia a cada clase\n",
    "# Para un punto con pétalo de longitud 4 cm y anchura 1.9 cm\n",
    "# Resultado: [0% setosa, 33.3% versicolor, 66.7% virginica]\n",
    "tree_clf.predict_proba([[4, 1.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t7gv1o9g1Tu1",
    "outputId": "0209ff8d-65c9-4991-997a-a7b16a95864f"
   },
   "outputs": [],
   "source": [
    "# Predicción de la clase más probable para el mismo punto\n",
    "# Devuelve la clase con mayor probabilidad (2 = virginica)\n",
    "tree_clf.predict([[4, 1.9]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXVXoNSr1Tu5"
   },
   "source": [
    "# Sensitivity to training set details\n",
    "El principal problema de los **árboles de decisión** es que son muy sensibles a pequeñas variaciones en los **datos de entrenamiento**.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dSMJx8D1Tu5",
    "outputId": "273e3246-ecbb-4297-f1db-7d0478201d25"
   },
   "outputs": [],
   "source": [
    "# Identificamos la flor Iris versicolor más ancha en el dataset\n",
    "# Buscamos la muestra con la anchura máxima (1.8) entre las versicolor (y==1)\n",
    "X[(X[:, 1]==X[:, 1][y==1].max()) & (y==1)] # widest Iris versicolor flower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xr7lIoKY1Tu9",
    "outputId": "35efa189-e25d-488c-ae1e-d4a0fddb96cf"
   },
   "outputs": [],
   "source": [
    "# Creamos un dataset modificado removiendo la flor versicolor más ancha\n",
    "# Esto demuestra la sensibilidad de los árboles de decisión a pequeños cambios\n",
    "# Filtramos: mantenemos las que NO son anchura 1.8 O que son virginica (y==2)\n",
    "not_widest_versicolor = (X[:, 1]!=1.8) | (y==2)\n",
    "X_tweaked = X[not_widest_versicolor]\n",
    "y_tweaked = y[not_widest_versicolor]\n",
    "\n",
    "# Entrenamos un nuevo árbol con profundidad máxima 2 en el dataset modificado\n",
    "tree_clf_tweaked = DecisionTreeClassifier(max_depth=2, random_state=40)\n",
    "tree_clf_tweaked.fit(X_tweaked, y_tweaked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3ICJntW1TvC",
    "outputId": "2fe177c0-3f0b-41aa-fc1f-39a4f9879011"
   },
   "outputs": [],
   "source": [
    "# Visualizamos las fronteras de decisión del árbol modificado\n",
    "# Comparado con el árbol original, vemos cómo cambió completamente la estructura\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_decision_boundary(tree_clf_tweaked, X_tweaked, y_tweaked, legend=False)\n",
    "# Dibujamos las líneas de división del nuevo árbol\n",
    "plt.plot([0, 7.5], [0.8, 0.8], \"k-\", linewidth=2)  # Primera división horizontal\n",
    "plt.plot([0, 7.5], [1.75, 1.75], \"k--\", linewidth=2)  # Segunda división horizontal\n",
    "# Etiquetamos las profundidades\n",
    "plt.text(1.0, 0.9, \"Depth=1\", fontsize=15)\n",
    "plt.text(1.0, 1.80, \"Depth=2\", fontsize=13)\n",
    "\n",
    "save_fig(\"decision_tree_instability_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pFC3rMy61TvH",
    "outputId": "75b6d06d-c4d9-4395-bfd4-4f8f0a7d94fa"
   },
   "outputs": [],
   "source": [
    "# Creamos un dataset artificial con forma de medias lunas para demostrar overfitting\n",
    "# make_moons genera datos en forma de dos medias lunas entrelazadas\n",
    "from sklearn.datasets import make_moons\n",
    "Xm, ym = make_moons(n_samples=100, noise=0.25, random_state=53)\n",
    "\n",
    "# Entrenamos dos árboles con diferentes restricciones\n",
    "# Árbol 1: Sin restricciones (propenso a overfitting)\n",
    "deep_tree_clf1 = DecisionTreeClassifier(random_state=42)\n",
    "# Árbol 2: Con restricción mínima de 4 muestras por hoja (más generalizable)\n",
    "deep_tree_clf2 = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
    "deep_tree_clf1.fit(Xm, ym)\n",
    "deep_tree_clf2.fit(Xm, ym)\n",
    "\n",
    "# Visualizamos las diferencias en las fronteras de decisión\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(deep_tree_clf1, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"No restrictions\", fontsize=16)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(deep_tree_clf2, Xm, ym, axes=[-1.5, 2.5, -1, 1.5], iris=False)\n",
    "plt.title(\"min_samples_leaf = {}\".format(deep_tree_clf2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "save_fig(\"min_samples_leaf_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparación de árboles de decisión con y sin restricciones\n",
    "\n",
    "La siguiente figura muestra cómo cambia la **frontera de decisión** de un modelo de **árbol de decisión** al aplicar restricciones en su entrenamiento.\n",
    "\n",
    "  \n",
    "\n",
    "### Descripción general\n",
    "\n",
    "Se comparan dos modelos:\n",
    "\n",
    "- **Izquierda:** Árbol sin restricciones (`No restrictions`)\n",
    "- **Derecha:** Árbol con una restricción en el número mínimo de muestras por hoja (`min_samples_leaf = 4`)\n",
    "\n",
    "  \n",
    "\n",
    "### Interpretación de los gráficos\n",
    "\n",
    "### Árbol sin restricciones\n",
    "- El modelo se ajusta **perfectamente a los datos de entrenamiento**.\n",
    "- La frontera de decisión es **muy irregular y fragmentada**.\n",
    "- Indica **sobreajuste (overfitting)**: el modelo memoriza los datos en lugar de aprender patrones generales.\n",
    "\n",
    "### Árbol con `min_samples_leaf = 4`\n",
    "- Cada hoja del árbol debe contener al menos **4 muestras**.\n",
    "- La frontera de decisión se vuelve **más suave y simple**.\n",
    "- El modelo **generaliza mejor**, reduciendo el sobreajuste.\n",
    "\n",
    "  \n",
    "\n",
    "### Detalles del gráfico\n",
    "\n",
    "- Los ejes \\( x_1 \\) y \\( x_2 \\) representan las características de entrada.\n",
    "- Los **puntos amarillos y verdes** son los datos reales de dos clases distintas.\n",
    "- Las **zonas coloreadas** muestran las regiones donde el modelo predice cada clase.\n",
    "- Las **líneas oscuras** son las fronteras de decisión aprendidas por el árbol.\n",
    "\n",
    "  \n",
    "\n",
    "### Conclusión\n",
    "\n",
    "A la izquierda, el árbol de decisión fue entrenado con los **hiperparámetros por defecto** (es decir, sin restricciones),  \n",
    "y a la derecha fue entrenado con **min_samples_leaf = 4**.  \n",
    "\n",
    "Es bastante evidente que el modelo de la izquierda está **sobreajustando (overfitting)**,  \n",
    "mientras que el modelo de la derecha probablemente **generalice mejor**.\n",
    "\n",
    "Aplicar una restricción como `min_samples_leaf` en los árboles de decisión ayuda a:\n",
    "- Evitar el sobreajuste.\n",
    "- Crear modelos más simples y estables.\n",
    "- Mejorar la capacidad de generalización en datos nuevos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgP5P15K1TvM",
    "outputId": "e0a77ab3-7809-4182-9268-352b8a0ed458"
   },
   "outputs": [],
   "source": [
    "# Demostramos la sensibilidad de los árboles de decisión a la rotación\n",
    "# Rotamos los datos del iris 20 grados usando una matriz de rotación\n",
    "angle = np.pi / 180 * 20  # 20 grados convertidos a radianes\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xr = X.dot(rotation_matrix)  # Aplicamos la rotación a los datos\n",
    "\n",
    "# Entrenamos un nuevo árbol con los datos rotados\n",
    "tree_clf_r = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_r.fit(Xr, y)\n",
    "\n",
    "# Visualizamos las fronteras de decisión con datos rotados\n",
    "# Notaremos que el árbol tiene dificultades con datos rotados\n",
    "plt.figure(figsize=(8, 3))\n",
    "plot_decision_boundary(tree_clf_r, Xr, y, axes=[0.5, 7.5, -1.0, 1], iris=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZBuCMK01TvO",
    "outputId": "f5606e3f-df26-4cbf-8b45-db0cfafc6252"
   },
   "outputs": [],
   "source": [
    "# Creamos un ejemplo más claro de sensibilidad a rotación\n",
    "# Generamos datos aleatorios simples: clase basada en si x > 0\n",
    "np.random.seed(6)\n",
    "Xs = np.random.rand(100, 2) - 0.5  # Datos aleatorios centrados en 0\n",
    "ys = (Xs[:, 0] > 0).astype(np.float32) * 2  # Clase: 0 si x<0, 2 si x>0\n",
    "\n",
    "# Rotamos los datos 45 grados\n",
    "angle = np.pi / 4  # 45 grados\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n",
    "Xsr = Xs.dot(rotation_matrix)\n",
    "\n",
    "# Entrenamos árboles en datos originales y rotados\n",
    "tree_clf_s = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_s.fit(Xs, ys)\n",
    "tree_clf_sr = DecisionTreeClassifier(random_state=42)\n",
    "tree_clf_sr.fit(Xsr, ys)\n",
    "\n",
    "# Comparamos las fronteras de decisión\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(tree_clf_s, Xs, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(tree_clf_sr, Xsr, ys, axes=[-0.7, 0.7, -0.7, 0.7], iris=False)\n",
    "\n",
    "save_fig(\"sensitivity_to_rotation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Qué muestra la figura\n",
    "- **Izquierda (sin rotar):**  \n",
    "  El árbol separa perfecto con un **corte vertical** en `x1 = 0`.  \n",
    "  Como los árboles usan **divisiones alineadas a ejes** (umbral en una sola característica), la frontera es una línea vertical simple.\n",
    "\n",
    "- **Derecha (rotado 45°):**  \n",
    "  La frontera verdadera ahora es **diagonal**.  \n",
    "  El árbol solo puede aproximarla con **escalones axis-aligned** (rectángulos), por lo que la separación es más tosca y puede requerir más profundidad/hojas para acercarse a la diagonal.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "85i9ATWM1TvR"
   },
   "source": [
    "# Regression trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyPWjPFt1TvS"
   },
   "outputs": [],
   "source": [
    "# Creamos un dataset cuadrático con ruido para demostrar árboles de regresión\n",
    "# Los árboles de regresión predicen valores numéricos en lugar de clases\n",
    "np.random.seed(42)\n",
    "m = 200  # 200 muestras\n",
    "X = np.random.rand(m, 1)  # Variable x entre 0 y 1\n",
    "y = 4 * (X - 0.5) ** 2  # Función cuadrática\n",
    "y = y + np.random.randn(m, 1) / 10  # Añadimos ruido gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7VetGJg1TvU",
    "outputId": "4641b81c-fd2b-4483-84e5-a7cbb265a2e8"
   },
   "outputs": [],
   "source": [
    "# Entrenamos un árbol de regresión con profundidad máxima de 2\n",
    "# Los árboles de regresión minimizan el error cuadrático medio en cada hoja\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepto de `max_depth` en árboles de decisión\n",
    "\n",
    "El parámetro **`max_depth`** controla la **profundidad máxima** que puede tener un árbol de decisión, es decir, el número máximo de niveles de divisiones desde la raíz hasta una hoja.\n",
    "\n",
    "### Función\n",
    "- Limita la **complejidad** del modelo.  \n",
    "- Cada nivel adicional permite al árbol capturar más patrones, pero también aumenta el riesgo de **sobreajuste**.\n",
    "\n",
    "### Efectos\n",
    "- **Poca profundidad (`max_depth` pequeño):**  \n",
    "  Modelo simple que puede no capturar bien los datos → **subajuste**.  \n",
    "- **Mucha profundidad (`max_depth` grande):**  \n",
    "  Modelo muy flexible que puede ajustarse al ruido → **sobreajuste**.\n",
    "\n",
    "### Conclusión\n",
    "Elegir un valor adecuado de `max_depth` es clave para equilibrar **precisión y generalización** en los árboles de decisión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HHT2VVch1TvX",
    "outputId": "378c5ec2-823d-4ff0-bebd-17e07a9ebfe8"
   },
   "outputs": [],
   "source": [
    "# Comparamos árboles de regresión con diferentes profundidades\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Árbol con profundidad máxima 2 (más simple, menos overfitting)\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42, max_depth=2)\n",
    "# Árbol con profundidad máxima 3 (más complejo, mayor capacidad)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, max_depth=3)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[0, 1, -0.2, 1], ylabel=\"$y$\"):\n",
    "    \"\"\"\n",
    "    Función para visualizar las predicciones de un árbol de regresión\n",
    "    \"\"\"\n",
    "    # Creamos puntos de evaluación espaciados uniformemente\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    # Predecimos para cada punto\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "    if ylabel:\n",
    "        plt.ylabel(ylabel, fontsize=18, rotation=0)\n",
    "    # Graficamos datos originales y predicciones del árbol\n",
    "    plt.plot(X, y, \"b.\")  # Datos originales en azul\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")  # Predicciones en rojo\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.subplot(121)\n",
    "plot_regression_predictions(tree_reg1, X, y)\n",
    "# Dibujamos las líneas de división del árbol de profundidad 2\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "plt.text(0.21, 0.65, \"Depth=1\", fontsize=15)\n",
    "plt.text(0.01, 0.2, \"Depth=2\", fontsize=13)\n",
    "plt.text(0.65, 0.8, \"Depth=2\", fontsize=13)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"max_depth=2\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_regression_predictions(tree_reg2, X, y, ylabel=None)\n",
    "# Dibujamos las líneas de división del árbol de profundidad 3\n",
    "for split, style in ((0.1973, \"k-\"), (0.0917, \"k--\"), (0.7718, \"k--\")):\n",
    "    plt.plot([split, split], [-0.2, 1], style, linewidth=2)\n",
    "# Divisiones adicionales de profundidad 3\n",
    "for split in (0.0458, 0.1298, 0.2873, 0.9040):\n",
    "    plt.plot([split, split], [-0.2, 1], \"k:\", linewidth=1)\n",
    "plt.text(0.3, 0.5, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\", fontsize=14)\n",
    "\n",
    "save_fig(\"tree_regression_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "La figura muestra cómo cambia la predicción de un **árbol de regresión** según la profundidad máxima (`max_depth`).\n",
    "\n",
    "### Descripción\n",
    "\n",
    "- Los **puntos azules** son los datos reales.\n",
    "- La **línea roja** representa la predicción del árbol (\\(\\hat{y}\\)).\n",
    "- Las **líneas verticales** indican los puntos de división del árbol.\n",
    "\n",
    "### Interpretación\n",
    "\n",
    "- **Izquierda (`max_depth=2`)**:  \n",
    "  Pocas divisiones → modelo simple → **subajuste**.\n",
    "\n",
    "- **Derecha (`max_depth=3`)**:  \n",
    "  Más divisiones → mejor ajuste a los datos → **mayor flexibilidad**, pero con riesgo de sobreajuste si se sigue aumentando la profundidad.\n",
    "\n",
    "### Conclusión\n",
    "Aumentar `max_depth` mejora el ajuste, pero un árbol demasiado profundo puede perder capacidad de generalización.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la estructura del árbol de regresión de profundidad 3\n",
    "# Esto nos muestra cómo el árbol toma decisiones numéricas en cada nodo\n",
    "plt.figure(figsize=(10,10))\n",
    "sklearn.tree.plot_tree(tree_reg2,\n",
    "               feature_names=[\"x1\"],  # Solo tenemos una característica\n",
    "               filled = True);  # Coloreamos los nodos según su valor predicho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VtV9Jjw1Tvg",
    "outputId": "a0d42e39-8854-4d5a-c8a5-b07e341c151c"
   },
   "outputs": [],
   "source": [
    "# Comparamos regularización en árboles de regresión\n",
    "# Árbol 1: Sin restricciones (overfitting)\n",
    "tree_reg1 = DecisionTreeRegressor(random_state=42)\n",
    "# Árbol 2: Con restricción mínima de 10 muestras por hoja (regularizado)\n",
    "tree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\n",
    "tree_reg1.fit(X, y)\n",
    "tree_reg2.fit(X, y)\n",
    "\n",
    "# Generamos predicciones para visualización\n",
    "x1 = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "y_pred1 = tree_reg1.predict(x1)\n",
    "y_pred2 = tree_reg2.predict(x1)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(X, y, \"b.\")  # Datos de entrenamiento\n",
    "plt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")  # Predicciones\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", fontsize=18, rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=18)\n",
    "plt.title(\"No restrictions\", fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(X, y, \"b.\")  # Datos de entrenamiento\n",
    "plt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")  # Predicciones\n",
    "plt.axis([0, 1, -0.2, 1.1])\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n",
    "\n",
    "save_fig(\"tree_regression_regularization_plot\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "decision_trees.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "nav_menu": {
   "height": "309px",
   "width": "468px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
