{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Cab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulación de un Taxi Autónomo\n",
    "\n",
    "Diseñemos una simulación de un taxi autónomo. El objetivo principal es demostrar, en un entorno simplificado, cómo se pueden utilizar técnicas de aprendizaje por refuerzo para desarrollar un enfoque eficiente y seguro para abordar este problema.\n",
    "\n",
    "El trabajo del Smartcab es recoger al pasajero en una ubicación y dejarlo en otra. Aquí hay algunas cosas que nos encantaría que nuestro Smartcab se encargara:\n",
    "\n",
    "- Dejar al pasajero en la ubicación correcta.\n",
    "- Ahorrar tiempo al pasajero tomando el tiempo mínimo posible para dejarlo.\n",
    "- Cuidar la seguridad del pasajero y las normas de tráfico.\n",
    "\n",
    "Hay diferentes aspectos que deben considerarse aquí al modelar una solución de aprendizaje por refuerzo para este problema: recompensas, estados y acciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recompensa y Penalización\n",
    "\n",
    "Dado que el agente (el conductor imaginario) está motivado por las recompensas y aprenderá a controlar el taxi mediante experiencias de prueba en el entorno, necesitamos decidir las recompensas y/o penalizaciones y su magnitud en consecuencia. Aquí hay algunos puntos a considerar:\n",
    "\n",
    "* El agente debería recibir una recompensa positiva alta por una entrega exitosa porque este comportamiento es altamente deseado.\n",
    "* El agente debería ser penalizado si intenta dejar al pasajero en ubicaciones incorrectas.\n",
    "* El agente debería recibir una ligera penalización negativa por no llegar al destino después de cada paso de tiempo. \"Ligera\" negativa porque preferiríamos que nuestro agente llegue tarde en lugar de hacer movimientos incorrectos tratando de llegar al destino lo más rápido posible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space\n",
    "\n",
    "En Aprendizaje por Refuerzo, el agente encuentra un estado y luego toma una acción de acuerdo con el estado en el que se encuentra.\n",
    "\n",
    "El Espacio de Estados es el conjunto de todas las situaciones posibles en las que nuestro taxi podría encontrarse. El estado debe contener información útil que el agente necesita para tomar la acción correcta.\n",
    "\n",
    "Digamos que tenemos un área de entrenamiento para nuestro Smartcab donde lo estamos enseñando a transportar personas en un estacionamiento a cuatro ubicaciones diferentes (R, G, Y, B):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Reinforcement_Learning_Taxi_Env.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de Estados\n",
    "\n",
    "Supongamos que Smartcab es el único vehículo en este estacionamiento. Podemos dividir el estacionamiento en una cuadrícula de 5x5, lo que nos da 25 ubicaciones de taxi posibles. Estas 25 ubicaciones son una parte de nuestro espacio de estados. Observa que el estado de ubicación actual de nuestro taxi es la coordenada (3, 1).\n",
    "\n",
    "También notarás que hay cuatro (4) ubicaciones donde podemos recoger y dejar a un pasajero: R, G, Y, B o [(0,0), (0,4), (4,0), (4,3)] en coordenadas (fila, columna). Nuestro pasajero ilustrado está en la ubicación Y y desea ir a la ubicación R.\n",
    "\n",
    "Cuando también tenemos en cuenta un (1) estado adicional del pasajero de estar dentro del taxi, podemos tomar todas las combinaciones de ubicaciones de pasajeros y ubicaciones de destino para llegar a un número total de estados para nuestro entorno de taxi; hay cuatro (4) destinos y cinco (4 + 1) ubicaciones de pasajeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cálculo del espacio de estados total del entorno Taxi\n# El entorno es una cuadrícula de 5x5 con 4 ubicaciones especiales (R, G, Y, B)\n# Fórmula: posiciones_taxi * ubicaciones_pasajero * ubicaciones_destino\n# - 5x5 = 25 posiciones posibles para el taxi en la cuadrícula\n# - 5 ubicaciones posibles del pasajero: en R, G, Y, B, o dentro del taxi (4+1)\n# - 4 ubicaciones de destino posibles: R, G, Y, o B\n# Total de estados posibles:\n5*5*5*4  # = 500 estados únicos posibles"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acciones en el Entorno del Taxi\n",
    "\n",
    "El agente encuentra uno de los 500 estados y toma una acción. La acción en nuestro caso puede ser moverse en una dirección o decidir recoger/dejar a un pasajero.\n",
    "\n",
    "En otras palabras, tenemos seis acciones posibles:\n",
    "1. sur\n",
    "2. norte\n",
    "3. este\n",
    "4. oeste\n",
    "5. recoger\n",
    "6. dejar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de Acciones\n",
    "\n",
    "Este es el espacio de acciones: el conjunto de todas las acciones que nuestro agente puede tomar en un estado dado.\n",
    "\n",
    "Observarás en la ilustración anterior que el taxi no puede realizar ciertas acciones en ciertos estados debido a las paredes. En el código del entorno, simplemente proporcionaremos una penalización de -1 por cada colisión con una pared y el taxi no se moverá en absoluto. Esto simplemente acumulará penalizaciones, haciendo que el taxi considere ir alrededor de la pared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n# =============================================================================\n\n# Gymnasium: Librería moderna de entornos de Reinforcement Learning (antes OpenAI Gym)\n# Es la versión actualizada y mantenida de Gym, compatible con NumPy moderno\nimport gymnasium as gym\n\n# Time: Para controlar pausas y tiempos de espera en las animaciones\nimport time\n\n# Matplotlib (comentado): Para visualizar gráficas de rendimiento\n# import matplotlib.pyplot as plt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CREACIÓN DEL ENTORNO TAXI\n# =============================================================================\n\n# gym.make() crea una instancia del entorno \"Taxi-v3\"\n# - \"Taxi-v3\": Entorno predefinido de un taxi que debe recoger y dejar pasajeros\n# - render_mode=\"ansi\": Modo de renderizado en texto ASCII para visualizar en consola\nenv = gym.make(\"Taxi-v3\", render_mode=\"ansi\")\n\n# env.reset() reinicia el entorno a un estado inicial aleatorio\n# Devuelve dos valores:\n# - state: El estado inicial (un número entre 0-499)\n# - info: Diccionario con información adicional del entorno\nstate, info = env.reset()\n\n# env.render() genera una representación visual del estado actual\n# En este caso, muestra la cuadrícula del taxi en formato ASCII\nprint(env.render())\n\n# Líneas comentadas para posibles usos futuros:\n# time.sleep(10)  # Pausar la ejecución 10 segundos\n# env.close()     # Cerrar el entorno y liberar recursos"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfaz del Entorno Gymnasium\n",
    "\n",
    "La interfaz central de Gymnasium es `env`, que es la interfaz de entorno unificada. Los siguientes son los métodos de `env` que serían bastante útiles para nosotros:\n",
    "\n",
    "* `env.reset`: Reinicia el entorno y devuelve un estado inicial aleatorio y un diccionario de información.\n",
    "* `env.step(action)`: Avanza el entorno en un paso de tiempo. Devuelve\n",
    "    - `observation`: Observaciones del entorno\n",
    "    - `reward`: Si tu acción fue beneficiosa o no\n",
    "    - `terminated`: Indica si hemos recogido y dejado con éxito a un pasajero, también llamado un episodio\n",
    "    - `truncated`: Si el episodio se trunca debido a un límite de tiempo\n",
    "    - `info`: Información adicional como rendimiento y latencia con fines de depuración\n",
    "* `env.render`: Renderiza un fotograma del entorno (útil para visualizar el entorno)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hay 4 ubicaciones (etiquetadas con diferentes letras), y nuestro trabajo es recoger al pasajero en una ubicación y dejarlo en otra. Recibimos +20 puntos por una entrega exitosa y perdemos 1 punto por cada paso de tiempo que toma. También hay una penalización de 10 puntos por acciones de recogida y entrega ilegales.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EXPLORACIÓN DEL ESPACIO DE ESTADOS Y ACCIONES\n# =============================================================================\n\n# Reiniciamos el entorno para obtener un nuevo estado aleatorio\nstate, info = env.reset()\n\n# Mostramos cómo se ve el entorno en este nuevo estado\nprint(env.render())\n\n# env.action_space: Define todas las acciones posibles que puede tomar el agente\n# En Taxi-v3 hay 6 acciones: 0=sur, 1=norte, 2=este, 3=oeste, 4=recoger, 5=dejar\nprint(\"Action Space {}\".format(env.action_space))\n\n# env.observation_space: Define todos los estados posibles del entorno\n# En Taxi-v3 hay 500 estados únicos (Discrete(500))\nprint(\"State Space {}\".format(env.observation_space))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* El cuadrado lleno representa el taxi, que es amarillo sin pasajero y verde con pasajero.\n",
    "* El símbolo \"|\" representa una pared que el taxi no puede cruzar.\n",
    "* R, G, Y, B son las posibles ubicaciones de recogida y destino. La letra azul representa la ubicación actual de recogida del pasajero, y la letra morada es el destino actual.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según lo verificado por las impresiones, tenemos un Espacio de Acciones de tamaño 6 y un Espacio de Estados de tamaño 500. Como verás, nuestro algoritmo de aprendizaje por refuerzo no necesitará más información que estas dos cosas. Todo lo que necesitamos es una forma de identificar un estado de manera única asignando un número único a cada estado posible, y el aprendizaje por refuerzo aprende a elegir un número de acción de 0 a 5 donde:\n",
    "\n",
    "0 = sur\n",
    "1 = norte\n",
    "2 = este\n",
    "3 = oeste\n",
    "4 = recoger\n",
    "5 = dejar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que los 500 estados corresponden a una codificación de la ubicación del taxi, la ubicación del pasajero y la ubicación de destino.\n",
    "\n",
    "El Aprendizaje por Refuerzo aprenderá un mapeo de estados a la acción óptima a realizar en ese estado mediante la exploración, es decir, el agente explora el entorno y toma acciones basadas en las recompensas definidas en el entorno.\n",
    "\n",
    "La acción óptima para cada estado es la acción que tiene la recompensa acumulativa a largo plazo más alta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De hecho, podemos tomar nuestra ilustración anterior, codificar su estado y dárselo al entorno para que lo renderice en Gym. Recuerda que tenemos el taxi en la fila 3, columna 1, nuestro pasajero está en la ubicación 2 y nuestro destino está en la ubicación 0. Usando el método de codificación de estado Taxi-v3, podemos hacer lo siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CODIFICACIÓN MANUAL DE UN ESTADO ESPECÍFICO\n# =============================================================================\n\n# env.unwrapped.encode() convierte coordenadas legibles en un número de estado (0-499)\n# Parámetros:\n# - taxi_row: Fila donde está el taxi (0-4)\n# - taxi_col: Columna donde está el taxi (0-4)\n# - passenger_index: Ubicación del pasajero (0=R, 1=G, 2=Y, 3=B, 4=en taxi)\n# - destination_index: Destino deseado (0=R, 1=G, 2=Y, 3=B)\n\n# Creamos un estado donde:\n# - Taxi en fila 1, columna 3\n# - Pasajero en G (índice 1)\n# - Destino en Y (índice 2)\nstate = env.unwrapped.encode(1, 3, 1, 2)\nprint(\"State:\", state)\n\n# Establecemos manualmente este estado en el entorno\n# env.unwrapped.s permite acceder al estado interno del entorno\nenv.unwrapped.s = state\n\n# Visualizamos el estado que acabamos de configurar\nprint(env.render())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos utilizando las coordenadas de nuestra ilustración para generar un número correspondiente a un estado entre 0 y 499, lo cual resulta ser 324 para el estado de nuestra ilustración.\n",
    "\n",
    "Luego podemos establecer manualmente el estado del entorno utilizando env.unwrapped.s con ese número codificado. Puedes experimentar con los números y verás que el taxi, el pasajero y el destino se mueven.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reward Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se crea el entorno Taxi, también se crea una tabla de recompensas inicial llamada `P`. Podemos pensar en ella como una matriz que tiene el número de estados como filas y el número de acciones como columnas.\n",
    "\n",
    "Dado que cada estado está en esta matriz, podemos ver los valores de recompensa predeterminados asignados al estado de nuestra ilustración:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURACIÓN DE UN ESTADO ESPECÍFICO PARA ANÁLISIS\n# =============================================================================\n\n# Codificamos el estado del ejemplo de la ilustración:\n# - Taxi en fila 3, columna 1\n# - Pasajero en G (índice 1)\n# - Destino en R (índice 0)\nstate = env.unwrapped.encode(3, 1, 1, 0)\nprint(\"State:\", state)  # Debería ser el estado 324\n\n# Establecemos este estado manualmente en el entorno\nenv.unwrapped.s = state\n\n# Renderizamos para ver cómo se ve visualmente\nprint(env.render())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TABLA DE TRANSICIONES DEL ENTORNO (P)\n# =============================================================================\n\n# env.unwrapped.P es un diccionario que contiene toda la dinámica del entorno\n# Para cada estado, muestra qué pasa al tomar cada acción\n# Estructura: {acción: [(probabilidad, próximo_estado, recompensa, terminado)]}\n\n# Veamos qué pasa en el estado 324 para cada una de las 6 acciones posibles:\nenv.unwrapped.P[324]\n\n# Interpretación del resultado:\n# 0 (sur): ir al estado 424, recompensa -1, no termina\n# 1 (norte): ir al estado 224, recompensa -1, no termina\n# 2 (este): ir al estado 344, recompensa -1, no termina\n# 3 (oeste): quedarse en 324 (pared), recompensa -1, no termina\n# 4 (recoger): quedarse en 324, recompensa -10 (ilegal), no termina\n# 5 (dejar): quedarse en 324, recompensa -10 (ilegal), no termina"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este diccionario tiene la estructura {acción: [(probabilidad, próximo estado, recompensa, hecho)]}.\n",
    "\n",
    "Algunas cosas a tener en cuenta:\n",
    "\n",
    "* Los números del 0 al 5 corresponden a las acciones (sur, norte, este, oeste, recoger, dejar) que el taxi puede realizar en nuestro estado actual en la ilustración.\n",
    "* En este entorno, la probabilidad siempre es 1.0.\n",
    "* El próximo estado es el estado en el que estaríamos si tomamos la acción en este índice del diccionario.\n",
    "* Todas las acciones de movimiento tienen una recompensa de -1 y las acciones de recoger/dejar tienen una recompensa de -10 en este estado en particular. Si estamos en un estado donde el taxi tiene un pasajero y está encima del destino correcto, veríamos una recompensa de 20 en la acción de dejar (5).\n",
    "* done se utiliza para indicarnos cuándo hemos dejado con éxito a un pasajero en la ubicación correcta. Cada entrega exitosa es el final de un episodio.\n",
    "\n",
    "Ten en cuenta que si nuestro agente elige explorar la acción dos (2) en este estado, estaría yendo hacia el Este, hacia una pared. El código fuente ha hecho imposible mover realmente el taxi a través de una pared, así que si el taxi elige esa acción, simplemente seguirá acumulando penalizaciones de -1, lo que afecta a la recompensa a largo plazo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EXPERIMENTO: TAXI SIN INTELIGENCIA (ACCIONES ALEATORIAS)\n# =============================================================================\n\n# Creamos un nuevo entorno limpio\nenv = gym.make(\"Taxi-v3\", render_mode=\"ansi\")\nstate, info = env.reset()\n\n# Configuramos el estado inicial al estado 324 de ejemplo\nenv.unwrapped.s = 324\n\n# Variables para métricas\nepochs = 0          # Contador de pasos/movimientos\npenalties = 0       # Contador de penalizaciones\nreward = 0          # Recompensa actual\n\nframes = []         # Lista para guardar cada frame de la animación\n\nterminated = False  # Flag que indica si el episodio terminó (pasajero entregado)\n\n# Bucle principal: el taxi actúa hasta entregar al pasajero\nwhile not terminated:\n    # env.action_space.sample() elige una acción ALEATORIA (0-5)\n    # Esto simula un taxi sin inteligencia que actúa al azar\n    action = env.action_space.sample()\n    \n    # env.step(action) ejecuta la acción y devuelve:\n    # - state: nuevo estado después de la acción\n    # - reward: recompensa obtenida (+20 éxito, -1 movimiento, -10 acción ilegal)\n    # - terminated: True si se completó el objetivo\n    # - truncated: True si se alcanzó límite de tiempo\n    # - info: información adicional\n    state, reward, terminated, truncated, info = env.step(action)\n    \n    # Contamos penalizaciones (recogida/entrega ilegal)\n    if reward == -10:\n        penalties = penalties + 1\n    \n    # Guardamos el frame actual para poder animarlo después\n    frames.append({\n        'frame': env.render(),  # Visualización del estado\n        'state': state,          # Número del estado\n        'action': action,        # Acción tomada\n        'reward': reward         # Recompensa recibida\n    })\n\n    epochs += 1  # Incrementamos el contador de pasos\n\n# Mostramos resultados del experimento aleatorio\nprint(\"Timesteps taken: {}\".format(epochs))        # Cuántos pasos tardó\nprint(\"Penalties incurred: {}\".format(penalties))  # Cuántas penalizaciones recibió"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        time.sleep(.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# FUNCIÓN PARA ANIMAR LA SECUENCIA DE ACCIONES DEL TAXI\n# =============================================================================\n\nfrom IPython.display import clear_output  # Para limpiar la salida en notebooks\nfrom time import sleep  # Para pausar entre frames\n\ndef print_frames(frames):\n    \"\"\"\n    Función que anima la secuencia de movimientos del taxi\n    \n    Parámetros:\n    - frames: Lista de diccionarios con información de cada paso\n    \"\"\"\n    # Iteramos sobre cada frame guardado\n    for i, frame in enumerate(frames):\n        clear_output(wait=True)  # Limpiamos la salida anterior\n        \n        # Mostramos el estado visual del entorno\n        print(frame['frame'])\n        \n        # Mostramos información del paso actual\n        print(f\"Timestep: {i + 1}\")           # Número de paso (empezando en 1)\n        print(f\"State: {frame['state']}\")     # Estado numérico (0-499)\n        print(f\"Action: {frame['action']}\")   # Acción tomada (0-5)\n        print(f\"Reward: {frame['reward']}\")   # Recompensa recibida\n        \n        sleep(.2)  # Pausa de 0.2 segundos entre frames para ver la animación\n\n# Ejecutamos la animación con los frames capturados anteriormente\nprint_frames(frames)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar un algoritmo simple de aprendizaje por refuerzo llamado Q-learning, que le dará a nuestro agente algo de memoria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Básicamente, el Q-learning permite que el agente utilice las recompensas del entorno para aprender, con el tiempo, la mejor acción a tomar en un estado dado.\n",
    "\n",
    "En nuestro entorno Taxi, tenemos la tabla de recompensas, P, de la que el agente aprenderá. Lo hace al recibir una recompensa por tomar una acción en el estado actual, y luego actualiza un valor Q para recordar si esa acción fue beneficiosa.\n",
    "\n",
    "Los valores almacenados en la tabla Q se llaman valores Q, y se corresponden con una combinación (estado, acción).\n",
    "\n",
    "Un valor Q para una combinación particular de estado-acción es representativo de la \"calidad\" de una acción tomada desde ese estado. Mejores valores Q implican mejores posibilidades de obtener recompensas mayores.\n",
    "\n",
    "Por ejemplo, si el taxi se enfrenta a un estado que incluye un pasajero en su ubicación actual, es muy probable que el valor Q para recoger sea más alto en comparación con otras acciones, como dejar o ir hacia el norte.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estamos asignando (), o actualizando, el valor Q del estado y la acción actual del agente primero tomando un peso () del antiguo valor Q, y luego añadiendo el valor aprendido. El valor aprendido es una combinación de la recompensa por tomar la acción actual en el estado actual, y la recompensa máxima descontada del próximo estado en el que estaremos una vez que tomemos la acción actual.\n",
    "\n",
    "Básicamente, estamos aprendiendo la acción adecuada a tomar en el estado actual al observar la recompensa para la combinación estado/acción actual y las máximas recompensas para el próximo estado. Esto eventualmente hará que nuestro taxi considere la ruta con las mejores recompensas concatenadas.\n",
    "\n",
    "El valor Q de un par estado-acción es la suma de la recompensa instantánea y la recompensa futura descontada (del estado resultante). La forma en que almacenamos los valores Q para cada estado y acción es a través de una tabla Q.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla Q es una matriz donde tenemos una fila para cada estado (500) y una columna para cada acción (6). Se inicializa primero en 0, y luego los valores se actualizan después del entrenamiento. Ten en cuenta que la tabla Q tiene las mismas dimensiones que la tabla de recompensas, pero tiene un propósito completamente diferente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/q-matrix-initialized-to-learned_gQq0BFs.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desglosándolo en pasos, obtenemos:\n",
    "\n",
    "* Inicializar la tabla Q con todos los valores en cero.\n",
    "* Comenzar a explorar acciones: Para cada estado, seleccionar una de entre todas las acciones posibles para el estado actual (S).\n",
    "* Viajar al siguiente estado (S') como resultado de esa acción (a).\n",
    "* Para todas las acciones posibles desde el estado (S'), seleccionar aquella con el valor Q más alto.\n",
    "* Actualizar los valores de la tabla Q usando la ecuación.\n",
    "* Establecer el siguiente estado como el estado actual.\n",
    "* Si se alcanza el estado objetivo, entonces finalizar y repetir el proceso.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de suficiente exploración aleatoria de acciones, los valores Q tienden a converger, sirviendo a nuestro agente como una función de valor de acción que puede explotar para elegir la acción más óptima a partir de un estado dado.\n",
    "\n",
    "Existe un compromiso entre la exploración (elegir una acción al azar) y la explotación (elegir acciones basadas en los valores Q ya aprendidos). Queremos evitar que la acción siempre tome la misma ruta, y posiblemente sobreajuste, así que introduciremos otro parámetro llamado \"epsilon\" para atender a esto durante el entrenamiento.\n",
    "\n",
    "En lugar de simplemente seleccionar la mejor acción aprendida con valor Q, a veces favoreceremos la exploración adicional del espacio de acciones. Un valor epsilon más bajo resulta en episodios con más penalizaciones (en promedio), lo que es obvio porque estamos explorando y tomando decisiones al azar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a construirlo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# INICIALIZACIÓN DE LA TABLA Q (Q-TABLE)\n# =============================================================================\n\nimport numpy as np  # NumPy para operaciones con arrays\n\n# Creamos la Q-table: una matriz de CEROS\n# Dimensiones:\n# - Filas: 500 (un estado por fila, correspondiente a cada estado del entorno)\n# - Columnas: 6 (una acción por columna: sur, norte, este, oeste, recoger, dejar)\n# \n# Cada celda [estado, acción] contendrá el valor Q, que representa:\n# \"Qué tan bueno es tomar esa acción en ese estado\"\n# \n# Al inicio todos los valores son 0 porque el agente no sabe nada\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n\n# Mostramos la tabla Q inicial\nq_table"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos las dimensiones de la Q-table\n# Debería ser (500, 6): 500 estados x 6 acciones\nq_table.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos el número total de elementos en la Q-table\n# 500 estados * 6 acciones = 3000 valores Q para aprender\nq_table.size"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos crear el algoritmo de entrenamiento que actualizará esta tabla Q a medida que el agente explore el entorno a lo largo de miles de episodios.\n",
    "\n",
    "En la primera parte de while not done, decidimos si elegir una acción al azar o explotar los valores Q ya calculados. Esto se hace simplemente utilizando el valor de epsilon y comparándolo con la función random.uniform(0, 1), que devuelve un número arbitrario entre 0 y 1.\n",
    "\n",
    "Ejecutamos la acción elegida en el entorno para obtener el próximo estado y la recompensa por realizar la acción. Después de eso, calculamos el valor Q máximo para las acciones correspondientes al próximo estado, y con eso, podemos actualizar fácilmente nuestro valor Q al nuevo valor_q:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# ENTRENAMIENTO DEL AGENTE CON Q-LEARNING\n# =============================================================================\n\n# %%time  # Comando mágico de Jupyter para medir tiempo de ejecución (descomentarlo si deseas)\n\nimport random  # Para generar números aleatorios\nfrom IPython.display import clear_output  # Para limpiar la salida durante entrenamiento\n\n# Reinicializamos la Q-table a ceros\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n\n# ========== HIPERPARÁMETROS DEL ALGORITMO Q-LEARNING ==========\n# Alpha (α): Tasa de aprendizaje - qué tanto confiamos en nueva información vs vieja\n# Un valor de 0.1 significa: 10% nueva info, 90% info antigua\nalpha = 0.1\n\n# Gamma (γ): Factor de descuento - importancia de recompensas futuras vs inmediatas\n# Un valor de 0.6 significa: las recompensas futuras valen 60% de las inmediatas\ngamma = 0.6\n\n# Epsilon (ε): Probabilidad de exploración vs explotación\n# Un valor de 0.1 significa: 10% exploración aleatoria, 90% usar conocimiento\nepsilon = 0.1\n\n# Listas para guardar métricas (opcional, no se usan actualmente)\nall_epochs = []\nall_penalties = []\n\n# ========== BUCLE PRINCIPAL DE ENTRENAMIENTO ==========\n# Entrenaremos por 50,000 episodios (cada episodio = recoger y dejar un pasajero)\nfor i in range(1, 50001):\n    # Reseteamos el entorno para un nuevo episodio\n    state, info = env.reset()\n\n    # Variables para métricas de este episodio\n    epochs = 0      # Contador de pasos\n    penalties = 0   # Contador de penalizaciones\n    reward = 0      # Recompensa actual\n    terminated = False  # Flag de finalización\n\n    # Bucle del episodio: hasta que el pasajero sea entregado\n    while not terminated:\n        # ========== DECISIÓN: EXPLORAR O EXPLOTAR ==========\n        if random.uniform(0, 1) < epsilon:\n            # EXPLORACIÓN: Elegimos una acción aleatoria (10% del tiempo)\n            action = env.action_space.sample()\n        else:\n            # EXPLOTACIÓN: Elegimos la mejor acción según la Q-table (90% del tiempo)\n            # np.argmax() encuentra el índice de la acción con mayor valor Q\n            action = np.argmax(q_table[state])\n\n        # Ejecutamos la acción en el entorno\n        next_state, reward, terminated, truncated, info = env.step(action)\n\n        # ========== ACTUALIZACIÓN DE LA Q-TABLE (ECUACIÓN DE BELLMAN) ==========\n        # Obtenemos el valor Q actual para esta combinación estado-acción\n        old_value = q_table[state, action]\n        \n        # Encontramos el mejor valor Q posible en el próximo estado\n        # Esto representa la mejor recompensa futura esperada\n        next_max = np.max(q_table[next_state])\n        \n        # Calculamos el nuevo valor Q usando la ecuación de Q-learning:\n        # nuevo_Q = (1-α)*viejo_Q + α*(recompensa_actual + γ*mejor_Q_futuro)\n        # Esto combina la estimación antigua con la nueva información\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        \n        # Actualizamos la Q-table con el nuevo valor\n        q_table[state, action] = new_value\n\n        # Contamos penalizaciones para estadísticas\n        if reward == -10:\n            penalties += 1\n\n        # Avanzamos al siguiente estado\n        state = next_state\n        epochs += 1\n\n    # Cada 100 episodios mostramos el progreso\n    if i % 100 == 0:\n        clear_output(wait=True)\n        print(f\"Episode: {i}\")\n\nprint(\"Training finished.\\\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que la tabla Q se ha establecido durante 100,000 episodios, veamos cuáles son los valores Q en el estado de nuestra ilustración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# INSPECCIÓN DE LA TABLA DE TRANSICIONES PARA EL ESTADO 324\n# =============================================================================\n\n# Recordamos qué pasa en el estado 324 según la dinámica del entorno\n# Esto nos ayudará a comparar con lo que aprendió el agente en la Q-table\nenv.unwrapped.P[324]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos el espacio de acciones del entorno\n# Debería mostrar Discrete(6) indicando 6 acciones posibles\nenv.action_space"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# ANÁLISIS DE LOS VALORES Q APRENDIDOS PARA EL ESTADO 324\n# =============================================================================\n\n# Accedemos a la fila 324 de la Q-table para ver qué aprendió el agente\n# Cada valor representa \"qué tan buena\" es cada acción en este estado\n# np.round(..., 4) redondea a 4 decimales para mejor legibilidad\n\n# Los valores Q son negativos porque el taxi acumula recompensas negativas (-1 por movimiento)\n# hasta llegar al objetivo (+20). El valor Q representa la recompensa acumulada esperada.\n\n# Interpretación:\n# - Valores más altos (menos negativos) = mejores acciones\n# - Valores muy bajos (muy negativos) = malas acciones (como recoger/dejar en lugar incorrecto)\n\nnp.round(q_table[324], 4)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sur\n",
    "2. norte\n",
    "3. este\n",
    "4. oeste\n",
    "5. recoger\n",
    "6. dejar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# IDENTIFICAR LA MEJOR ACCIÓN APRENDIDA PARA EL ESTADO 324\n# =============================================================================\n\n# np.argmax() encuentra el índice (0-5) del valor más alto en la fila 324\n# Este índice corresponde a la mejor acción que el agente aprendió para este estado\n\n# Recordatorio de acciones:\n# 0 = sur, 1 = norte, 2 = este, 3 = oeste, 4 = recoger, 5 = dejar\n\nnp.argmax(q_table[324])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor máximo de Q es \"north\" o \"east\" (-2.489), ¡así que parece que Q-learning ha aprendido efectivamente la mejor acción a tomar en el estado de nuestra ilustración!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a evaluar el rendimiento de nuestro agente. Ya no necesitamos explorar acciones más, así que ahora la siguiente acción siempre se selecciona usando el mejor valor Q:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EVALUACIÓN DEL AGENTE ENTRENADO (CON Q-LEARNING)\n# =============================================================================\n\n# Ahora evaluaremos qué tan bien funciona el agente DESPUÉS del entrenamiento\n# El agente ya no explora, solo explota su conocimiento (usa la Q-table)\n\ntotal_epochs = 0      # Suma total de pasos de todos los episodios\ntotal_penalties = 0   # Suma total de penalizaciones\nepisodes = 10         # Número de episodios de prueba\nframes = []           # Para guardar animación\n\n# Probamos el agente en 10 episodios diferentes\nfor _ in range(episodes):\n    # Comenzamos un nuevo episodio con estado aleatorio\n    state, info = env.reset()\n    \n    # NOTA: Las siguientes líneas están comentadas pero permitirían\n    # establecer un estado inicial específico para pruebas consistentes\n    # state = env.unwrapped.encode(3, 1, 2, 0)\n    # env.unwrapped.s = state\n    \n    # Variables para este episodio\n    epochs = 0\n    penalties = 0\n    reward = 0\n    terminated = False\n    actions = []  # Lista para guardar la secuencia de acciones\n\n    # Bucle del episodio\n    while not terminated:\n        # ========== EXPLOTACIÓN PURA (SIN EXPLORACIÓN) ==========\n        # El agente SIEMPRE elige la mejor acción según la Q-table\n        # Ya no hay aleatoriedad, usa solo lo que aprendió\n        action = np.argmax(q_table[state])\n        actions.append(action)\n        \n        # Ejecutamos la acción elegida\n        state, reward, terminated, truncated, info = env.step(action)\n        \n        # Guardamos el frame para animación posterior\n        frames.append({\n            'frame': env.render(),\n            'state': state,\n            'action': action,\n            'reward': reward\n        })\n        \n        # Contamos penalizaciones (idealmente debería ser 0)\n        if reward == -10:\n            penalties += 1\n\n        epochs += 1\n    \n    print(\"Finalizada partida\", _)\n    \n    # Acumulamos métricas\n    total_penalties += penalties\n    total_epochs += epochs\n\n# ========== RESULTADOS DE LA EVALUACIÓN ==========\nprint(f\"Results after {episodes} episodes:\")\nprint(f\"Average timesteps per episode: {total_epochs / episodes}\")  # Debería ser bajo (~12-15)\nprint(f\"Average penalties per episode: {total_penalties / episodes}\")  # Debería ser 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Animamos la última evaluación del agente entrenado\n# Deberías ver movimientos inteligentes y eficientes hacia el objetivo\nprint_frames(frames)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos la secuencia de acciones del último episodio\n# Esto muestra la \"estrategia\" que siguió el agente\n# Cada número representa: 0=sur, 1=norte, 2=este, 3=oeste, 4=recoger, 5=dejar\nactions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver en la evaluación que el rendimiento del agente mejoró significativamente y no incurrió en penalizaciones, lo que significa que realizó las acciones correctas de recogida/dejar con 100 pasajeros diferentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Genera una acción aleatoria del espacio de acciones\n# Esto es equivalente a elegir un número aleatorio entre 0 y 5\nenv.action_space.sample()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EVALUACIÓN DEL AGENTE SIN ENTRENAMIENTO (ACCIONES ALEATORIAS)\n# =============================================================================\n\n# Para comparar, evaluamos un agente que actúa COMPLETAMENTE al azar\n# Esto nos muestra la importancia del aprendizaje por refuerzo\n\ntotal_epochs = 0\ntotal_penalties = 0\nepisodes = 100  # Usamos más episodios para obtener un promedio confiable\n\nfor _ in range(episodes):\n    # Reiniciamos el entorno\n    state, info = env.reset()\n    \n    # Establecemos un estado inicial fijo para comparación justa\n    state = env.unwrapped.encode(3, 1, 2, 0)\n    env.unwrapped.s = state\n    \n    # Variables del episodio\n    epochs = 0\n    penalties = 0\n    reward = 0\n    terminated = False\n    actions = []\n\n    # Bucle del episodio\n    while not terminated:\n        # ========== ACCIÓN COMPLETAMENTE ALEATORIA ==========\n        # El agente NO usa conocimiento, solo elige al azar\n        action = env.action_space.sample()\n        actions.append(action)\n        \n        # Ejecutamos la acción\n        state, reward, terminated, truncated, info = env.step(action)\n\n        # Contamos penalizaciones\n        if reward == -10:\n            penalties += 1\n\n        epochs += 1\n\n    # Acumulamos métricas\n    total_penalties += penalties\n    total_epochs += epochs\n\n# ========== COMPARACIÓN DE RESULTADOS ==========\nprint(f\"Results after {episodes} episodes:\")\nprint(f\"Average timesteps per episode: {total_epochs / episodes}\")  # Será MUCHO mayor (~2000+)\nprint(f\"Average penalties per episode: {total_penalties / episodes}\")  # Será alto (~700+)\n\n# CONCLUSIÓN: El agente aleatorio tarda ~150x más y comete ~1000x más errores\n# que el agente entrenado. Esto demuestra el poder del Q-Learning."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA (State-Action-Reward-State-Action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA es un algoritmo específico de aprendizaje por refuerzo que se utiliza para actualizar los valores de acción en función de la observación del siguiente estado y acción, además de la recompensa actual. En SARSA, se elige una acción (A) en un estado (S), se observa el siguiente estado (S') y se elige una nueva acción (A') basada en una política de toma de decisiones (que puede ser ε-greedy, por ejemplo). Luego, se actualizan los valores de acción utilizando la recompensa recibida y el valor de acción del siguiente estado y acción.\n",
    "\n",
    "\n",
    "\n",
    "La principal diferencia entre Q-Table y SARSA radica en cómo se actualizan los valores de acción.\n",
    "En la Q-Table, los valores se actualizan considerando el máximo valor de acción posible en el siguiente estado, independientemente de la acción tomada. En cambio, SARSA actualiza los valores de acción utilizando la acción real tomada en el siguiente estado.\n",
    "Por lo tanto, mientras que Q-Table es un método off-policy (actualiza los valores de acción considerando la mejor acción posible), SARSA es un método on-policy (actualiza los valores de acción considerando la acción real tomada).\n",
    "\n",
    "\n",
    "Q-Table:\n",
    "- Ventajas: Es simple de entender e implementar en entornos con un número limitado de estados y acciones.\n",
    "- Inconvenientes: No es escalable para entornos con un gran número de estados y acciones debido a la necesidad de almacenar y actualizar una tabla grande de valores de acción.\n",
    "\n",
    "SARSA:\n",
    "- Ventajas: Es más eficiente en términos de memoria y puede escalar mejor a entornos con un gran número de estados y acciones.\n",
    "- Inconvenientes: Puede ser más difícil de implementar y entender en comparación con Q-Table debido a la necesidad de seguir una política de toma de decisiones y actualizar los valores de acción de manera adecuada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# ENTRENAMIENTO CON SARSA (State-Action-Reward-State-Action)\n# =============================================================================\n\n# SARSA es similar a Q-Learning pero con una diferencia clave:\n# - Q-Learning es \"off-policy\": actualiza usando la MEJOR acción posible del siguiente estado\n# - SARSA es \"on-policy\": actualiza usando la acción que REALMENTE tomará en el siguiente estado\n\n# Reinicializamos la Q-table\nq_table = np.zeros([env.observation_space.n, env.action_space.n])\n\n# Hiperparámetros (los mismos que en Q-Learning para comparación)\nalpha = 0.1    # Tasa de aprendizaje\ngamma = 0.6    # Factor de descuento\nepsilon = 0.1  # Probabilidad de exploración\n\n# Métricas\nall_epochs = []\nall_penalties = []\n\n# Entrenamiento por 100,000 episodios\nfor i in range(1, 100001):\n    # Reiniciamos el entorno\n    state, info = env.reset()\n    \n    # ========== DIFERENCIA CLAVE: ELEGIMOS LA PRIMERA ACCIÓN ANTES DEL BUCLE ==========\n    # En SARSA necesitamos la acción inicial para poder actualizar con la acción real del siguiente paso\n    action = env.action_space.sample() if random.uniform(0, 1) < epsilon else np.argmax(q_table[state])\n\n    epochs = 0\n    penalties = 0\n    reward = 0\n    terminated = False\n    \n    while not terminated:\n        # Ejecutamos la acción actual\n        next_state, reward, terminated, truncated, info = env.step(action)\n        \n        # ========== ELEGIMOS LA PRÓXIMA ACCIÓN (que realmente tomaremos) ==========\n        next_action = env.action_space.sample() if random.uniform(0, 1) < epsilon else np.argmax(q_table[next_state])\n\n        # ========== ACTUALIZACIÓN SARSA ==========\n        old_value = q_table[state, action]\n        \n        # DIFERENCIA PRINCIPAL: En lugar de usar max(Q[next_state]), \n        # usamos Q[next_state, next_action] (la acción que realmente tomaremos)\n        next_value = q_table[next_state, next_action]\n        \n        # Ecuación de actualización SARSA:\n        # Q(s,a) = Q(s,a) + α * [r + γ*Q(s',a') - Q(s,a)]\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_value)\n        q_table[state, action] = new_value\n\n        # Contamos penalizaciones\n        if reward == -10:\n            penalties += 1\n\n        # ========== AVANZAMOS AL SIGUIENTE ESTADO Y ACCIÓN ==========\n        # Esto es clave en SARSA: usamos la acción que ya elegimos\n        state = next_state\n        action = next_action\n        epochs += 1\n\n    # Mostramos progreso cada 100 episodios\n    if i % 100 == 0:\n        clear_output(wait=True)\n        print(f\"Episode: {i}\")\n\nprint(\"Training finished.\\\\n\")\n\n# COMPARACIÓN Q-Learning vs SARSA:\n# - Q-Learning: Más agresivo, aprende la política óptima incluso si explora\n# - SARSA: Más conservador, aprende la política que realmente sigue (incluyendo exploración)\n# - En la práctica, Q-Learning suele converger más rápido en entornos determinísticos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q-Learning\n",
    "# action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "# # SARSA\n",
    "# next_action = np.argmax(q_table[next_state]) # Choose next action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha (α):\n",
    "El parámetro alpha controla la tasa de aprendizaje en los algoritmos de aprendizaje por refuerzo. Es una medida de cuánto confiamos en las nuevas actualizaciones de los valores de acción en comparación con los valores existentes.\n",
    "\n",
    "- Si alpha es alto, damos más peso a las nuevas recompensas para actualizar los valores de acción.\n",
    "- Si alpha es bajo, damos más peso a los valores de acción existentes y aprendemos más lentamente.\n",
    "\n",
    "Ejemplo: Imagina que estás aprendiendo a jugar al ajedrez. Si tienes un alpha alto, estarías cambiando tus estrategias rápidamente después de cada partida. Si tienes un alpha bajo, estarías más inclinado a mantener tus estrategias existentes durante más tiempo, incluso si no te están dando buenos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gamma (γ):\n",
    "El parámetro gamma es el factor de descuento en los algoritmos de aprendizaje por refuerzo. Controla cuánto valoramos las recompensas futuras en comparación con las recompensas inmediatas.\n",
    "- Un gamma cercano a 1 significa que valoramos mucho las recompensas futuras.\n",
    "- Un gamma cercano a 0 significa que solo valoramos las recompensas inmediatas.\n",
    "\n",
    "Ejemplo: Imagina que estás decidiendo si estudiar para un examen o salir con tus amigos. Si tienes un gamma alto, estarías más inclinado a estudiar porque valoras mucho las recompensas futuras (buenas calificaciones). Si tienes un gamma bajo, estarías más inclinado a salir con tus amigos porque solo valoras la recompensa inmediata (diversión)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon (ε):\n",
    "El parámetro epsilon controla la exploración frente a la explotación en los algoritmos de aprendizaje por refuerzo. Determina la probabilidad de elegir una acción al azar en lugar de la acción óptima según los valores de acción actuales.\n",
    "- Un epsilon alto significa que somos más propensos a explorar nuevas acciones.\n",
    "- Un epsilon bajo significa que somos más propensos a explotar las acciones conocidas.\n",
    "\n",
    "Ejemplo: Imagina que estás decidiendo qué película ver en Netflix. Si tienes un epsilon alto, es más probable que explores nuevas películas en lugar de ver tus favoritas. Si tienes un epsilon bajo, es más probable que veas tus películas favoritas una y otra vez sin explorar nuevas opciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}