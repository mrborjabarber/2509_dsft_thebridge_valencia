{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Introducción a Spark con Python, por Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook introducirá tres operaciones básicas pero esenciales de Spark. Dos de ellas son las *transformaciones* `map` y `filter`. La otra es la *acción* `collect`. Al mismo tiempo, introduciremos el concepto de *persistencia* en Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos pyspark y SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Nota: solamente hay una SparkSession activa por aplicación\n",
    "# spark = SparkSession.builder.appName(\"RDDBasics\").getOrCreate()\n",
    "\n",
    "# Obtenemos el SparkContext desde la SparkSession existente\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo los datos y creando el RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hicimos en nuestro primer notebook, usaremos el conjunto de datos reducido (10 por ciento) proporcionado para la KDD Cup 1999, que contiene casi medio millón de interacciones de red. El archivo se proporciona como un archivo Gzip que descargaremos localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la URL del archivo de datos\n",
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "\n",
    "# Importamos SparkFiles para gestionar la descarga y acceso a archivos\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Añadimos el archivo al contexto de Spark\n",
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos usar este archivo para crear nuestro RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el RDD leyendo el archivo de texto comprimido\n",
    "myRDD = sc.textFile(\"file://\" + SparkFiles.get(\"kddcup.data_10_percent.gz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La transformación `filter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta transformación se puede aplicar a los RDDs para mantener solo los elementos que satisfacen una cierta condición. Más concretamente, se evalúa una función en cada elemento del RDD original. El nuevo RDD resultante contendrá solo aquellos elementos que hagan que la función devuelva `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, imaginemos que queremos contar cuántas interacciones `normal.` tenemos en nuestro conjunto de datos. Podemos filtrar nuestro RDD `raw_data` de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos el RDD para mantener solo las líneas que contienen \"normal.\"\n",
    "# lambda x: \"normal.\" in x es una función anónima que devuelve True si \"normal.\" está en x\n",
    "# Esto es una TRANSFORMACIÓN (lazy), no se ejecuta hasta que haya una acción\n",
    "normal_myRDD = myRDD.filter(lambda x: \"normal.\" in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos contar cuántos elementos tenemos en el nuevo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos los elementos filtrados\n",
    "# count() es una ACCIÓN, aquí es cuando realmente se ejecuta el cálculo\n",
    "normal_myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos el total de elementos en el RDD original para comparar\n",
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos time para medir el tiempo de ejecución\n",
    "from time import time\n",
    "\n",
    "# Marcamos el tiempo inicial\n",
    "t0 = time()\n",
    "\n",
    "# Ejecutamos la acción count()\n",
    "normal_count = normal_myRDD.count()\n",
    "\n",
    "# Calculamos el tiempo transcurrido\n",
    "tt = time() - t0\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(\"There are {} 'normal' intercations\".format(normal_count))\n",
    "print(\"Count completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda del notebook 1 que tenemos un total de 494021 en nuestro conjunto de datos del 10 por ciento. Aquí podemos ver que 97278 contienen la etiqueta `normal.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observa que hemos medido el tiempo transcurrido para contar los elementos en el RDD. Lo hemos hecho porque queríamos señalar que los cálculos (distribuidos) reales en Spark ocurren cuando ejecutamos *acciones* y no *transformaciones*. En este caso, `count` es la acción que ejecutamos sobre el RDD. Podemos aplicar tantas transformaciones como queramos en nuestro RDD y no se realizará ningún cálculo hasta que llamemos a la primera acción que, en este caso, tarda unos segundos en completarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La transformación `map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al usar la transformación `map` en Spark, podemos aplicar una función a cada elemento en nuestro RDD. Las funciones lambda de Python son especialmente expresivas para esto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, queremos leer nuestro archivo de datos como un archivo formateado en CSV. Podemos hacer esto aplicando una función lambda a cada elemento en el RDD de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos algunas líneas del RDD original (son strings)\n",
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la transformación map para dividir cada línea por comas\n",
    "# lambda x: x.split(\",\") convierte cada string en una lista\n",
    "# Esto es una TRANSFORMACIÓN (lazy), no se ejecuta hasta que haya una acción\n",
    "csv_data = myRDD.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Marcamos el tiempo inicial\n",
    "t0 = time()\n",
    "\n",
    "# take(5) es una ACCIÓN, aquí es cuando se ejecuta la transformación map\n",
    "head_rows = csv_data.take(5)\n",
    "\n",
    "# Calculamos el tiempo transcurrido\n",
    "tt = time() - t0\n",
    "\n",
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos las primeras 3 filas parseadas (ahora son listas)\n",
    "head_rows[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuevamente, toda la acción ocurre una vez que llamamos a la primera *acción* de Spark (es decir, *take* en este caso). ¿Qué pasa si tomamos muchos elementos en lugar de solo los primeros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos muchos más elementos para ver la diferencia en tiempo de ejecución\n",
    "t0 = time()\n",
    "\n",
    "# Solicitamos 100,000 elementos parseados\n",
    "head_rows = csv_data.take(100000)\n",
    "\n",
    "tt = time() - t0\n",
    "print(\"parse completed in {} seconds\".format(round(tt, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que tarda más. La función `map` se aplica ahora de forma distribuida a muchos elementos del RDD, de ahí el mayor tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando `map` y funciones predefinidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por supuesto, podemos usar funciones predefinidas con `map`. Imaginemos que queremos tener cada elemento en el RDD como un par clave-valor donde la clave es la etiqueta (ej. *normal*) y el valor es toda la lista de elementos que representa la fila en el archivo formateado en CSV. Podríamos proceder de la siguiente manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos las líneas originales\n",
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una función personalizada para parsear y crear pares clave-valor\n",
    "def parse_interaction(line):\n",
    "    # Dividimos la línea por comas\n",
    "    elems = line.split(\",\")\n",
    "    \n",
    "    # Extraemos la etiqueta (última columna, posición 41)\n",
    "    tag = elems[41]\n",
    "    \n",
    "    # Devolvemos una tupla (clave, valor)\n",
    "    # Clave: la etiqueta (normal., attack., etc.)\n",
    "    # Valor: la lista completa de elementos\n",
    "    return (tag, elems)\n",
    "\n",
    "# Aplicamos la función personalizada con map\n",
    "key_csv_data = myRDD.map(parse_interaction)\n",
    "\n",
    "# Tomamos los primeros 5 elementos y mostramos los primeros 2\n",
    "head_rows = key_csv_data.take(5)\n",
    "print(head_rows[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso fue fácil, ¿verdad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En nuestro notebook sobre el trabajo con pares clave-valor, usaremos este tipo de RDDs para hacer agregaciones de datos (ej. contar por clave)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La acción `collect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos usado las acciones `count` y `take`. Otra acción básica que necesitamos aprender es `collect`. Básicamente, traerá todos los elementos del RDD **a la memoria** para que trabajemos con ellos. Por esta razón, debe usarse con cuidado, especialmente cuando se trabaja con RDDs grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo usando nuestros datos crudos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marcamos el tiempo inicial\n",
    "t0 = time()\n",
    "\n",
    "# collect() trae TODOS los datos del RDD al nodo driver (memoria local)\n",
    "# ¡PRECAUCIÓN! Puede causar problemas de memoria con datasets grandes\n",
    "all_raw_data = myRDD.collect()\n",
    "\n",
    "tt = time() - t0\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora all_raw_data es una lista de Python normal\n",
    "# Podemos acceder a los elementos usando indexación estándar\n",
    "all_raw_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso tardó más que cualquier otra acción que usamos antes, por supuesto. Cada nodo trabajador de Spark que tiene un fragmento del RDD debe ser coordinado para recuperar su parte, y luego *reducir* todo junto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como último ejemplo que combina todo lo anterior, queremos recopilar todas las interacciones `normal` como pares clave-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Parseamos en pares clave-valor usando la función definida anteriormente\n",
    "key_csv_data = myRDD.map(parse_interaction)\n",
    "\n",
    "# Paso 2: Filtramos solo las interacciones con clave \"normal.\"\n",
    "# x[0] es la clave (la etiqueta), x[1] sería el valor (la lista completa)\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# Paso 3: Recopilamos todos los datos filtrados\n",
    "t0 = time()\n",
    "all_normal = normal_key_interactions.collect()\n",
    "tt = time() - t0\n",
    "\n",
    "# Contamos los elementos en la lista resultante\n",
    "normal_count = len(all_normal)\n",
    "\n",
    "print(\"Data collected in  {} seconds\".format(round(tt, 3)))\n",
    "print(\"There are {} 'normal' interactions\".format(normal_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el número de particiones del RDD original\n",
    "print(\"Number of Partitions: \" + str(myRDD.getNumPartitions()))\n",
    "\n",
    "# Verificamos el número de particiones después de las transformaciones\n",
    "# Las transformaciones mantienen el número de particiones por defecto\n",
    "print(\"Number of Partitions: \" + str(key_csv_data.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el tipo de objeto RDD\n",
    "type(myRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este conteo coincide con el conteo anterior de interacciones `normal`. El nuevo procedimiento consume más tiempo. Esto se debe a que recuperamos todos los datos con `collect` y luego usamos la función `len` de Python en la lista resultante. Antes solo estábamos contando el número total de elementos en el RDD usando `count`."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
