{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359697d5",
   "metadata": {},
   "source": [
    "# LangChain Cookbook üë®‚Äçüç≥üë©‚Äçüç≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d788b0",
   "metadata": {},
   "source": [
    "*Este cookbook est√° basado en la [Documentaci√≥n Conceptual de LangChain](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**Objetivo:** Proporcionar una comprensi√≥n introductoria de los componentes y casos de uso de LangChain a trav√©s de ejemplos [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) y fragmentos de c√≥digo. Para casos de uso, consulta la [parte 2](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb). Ver [tutorial en video](https://www.youtube.com/watch?v=2xxziIWmaSA) de este notebook.\n",
    "\n",
    "\n",
    "**Enlaces:**\n",
    "* [Documentaci√≥n Conceptual de LC](https://docs.langchain.com/docs/)\n",
    "* [Documentaci√≥n de LC Python](https://python.langchain.com/en/latest/)\n",
    "* [Documentaci√≥n de LC Javascript/Typescript](https://js.langchain.com/docs/)\n",
    "* [Discord de LC](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [Twitter de LC](https://twitter.com/LangChainAI)\n",
    "\n",
    "\n",
    "### **¬øQu√© es LangChain?**\n",
    "> LangChain es un framework para desarrollar aplicaciones potenciadas por modelos de lenguaje.\n",
    "\n",
    "**~~TL~~DR**: LangChain hace que las partes complicadas de trabajar y construir con modelos de IA sean m√°s f√°ciles. Ayuda a hacer esto de dos maneras:\n",
    "\n",
    "1. **Integraci√≥n** - Trae datos externos, como tus archivos, otras aplicaciones y datos de API, a tus LLMs\n",
    "2. **Agencia** - Permite que tus LLMs interact√∫en con su entorno mediante la toma de decisiones. Usa LLMs para ayudar a decidir qu√© acci√≥n tomar a continuaci√≥n\n",
    "\n",
    "### **¬øPor qu√© LangChain?**\n",
    "1. **Componentes** - LangChain facilita intercambiar abstracciones y componentes necesarios para trabajar con modelos de lenguaje.\n",
    "\n",
    "2. **Cadenas personalizadas** - LangChain proporciona soporte listo para usar para usar y personalizar 'cadenas' - una serie de acciones encadenadas.\n",
    "\n",
    "3. **Velocidad üö¢** - Este equipo lanza incre√≠blemente r√°pido. Estar√°s al d√≠a con las √∫ltimas caracter√≠sticas de LLM.\n",
    "\n",
    "4. **Comunidad üë•** - Maravilloso soporte comunitario en Discord, encuentros, hackathons, etc.\n",
    "\n",
    "Aunque los LLMs pueden ser sencillos (texto de entrada, texto de salida), r√°pidamente encontrar√°s puntos de fricci√≥n con los que LangChain ayuda una vez que desarrolles aplicaciones m√°s complicadas.\n",
    "\n",
    "*Nota: Este cookbook no cubrir√° todos los aspectos de LangChain. Sus contenidos han sido seleccionados para llevarte a construir y tener impacto lo m√°s r√°pido posible. Para m√°s, consulta la [Documentaci√≥n Conceptual de LangChain](https://docs.langchain.com/docs/)*\n",
    "\n",
    "*Actualizaci√≥n Oct '23: Este notebook se ha ampliado desde su forma original*\n",
    "\n",
    "Necesitar√°s una clave API de OpenAI para seguir este tutorial. Puedes tenerla como variable de entorno, en un archivo .env donde vive este jupyter notebook, o insertarla donde dice 'YourAPIKey'. Si tienes preguntas sobre esto, pon estas instrucciones en [ChatGPT](https://chat.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8gd5ot2g15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalaci√≥n de todas las dependencias necesarias\n",
    "# Ejecuta esta celda primero antes de ejecutar el resto del notebook\n",
    "\n",
    "# %pip install langchain\n",
    "# %pip install langchain-core\n",
    "# %pip install langchain-openai\n",
    "# %pip install langchain-community\n",
    "# %pip install langchain-text-splitters\n",
    "# %pip install python-dotenv\n",
    "# %pip install openai\n",
    "# %pip install faiss-cpu\n",
    "# %pip install chromadb\n",
    "# %pip install tiktoken\n",
    "# %pip install google-search-results\n",
    "# %pip install unstructured\n",
    "# %pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e9815081",
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias para manejar variables de entorno\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la clave API de OpenAI desde las variables de entorno\n",
    "openai_api_key=os.getenv('OPENAI_API_KEY', 'YourAPIKey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb564d",
   "metadata": {},
   "source": [
    "# Componentes de LangChain\n",
    "\n",
    "## Schema - Tuercas y tornillos para trabajar con Modelos de Lenguaje Grandes (LLMs)\n",
    "\n",
    "### **Texto**\n",
    "La forma natural de interactuar con LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e0dc06c",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What day comes after Friday?'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trabajar√°s con cadenas de texto simples (¬°que pronto crecer√°n en complejidad!)\n",
    "my_text = \"What day comes after Friday?\"\n",
    "my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39eb39",
   "metadata": {},
   "source": [
    "### **Mensajes de Chat**\n",
    "Como texto, pero especificado con un tipo de mensaje (Sistema, Humano, IA)\n",
    "\n",
    "* **Sistema** - Contexto de fondo √∫til que le dice a la IA qu√© hacer\n",
    "* **Humano** - Mensajes que est√°n destinados a representar al usuario\n",
    "* **IA** - Mensajes que muestran con qu√© respondi√≥ la IA\n",
    "\n",
    "Para m√°s informaci√≥n, consulta la [documentaci√≥n](https://platform.openai.com/docs/guides/chat/introduction) de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "99b0935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el modelo de chat de OpenAI y las clases de mensajes\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Este es el modelo de lenguaje que usaremos. Hablaremos de lo que estamos haciendo a continuaci√≥n\n",
    "# temperature controla la aleatoriedad de las respuestas (0 = determinista, 1 = m√°s creativo)\n",
    "chat = ChatOpenAI(temperature=.7, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2f7af",
   "metadata": {},
   "source": [
    "Ahora creemos algunos mensajes que simulan una experiencia de chat con un bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "878d6a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You could try a caprese salad with fresh tomatoes, mozzarella, basil, and balsamic glaze.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 39, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CfXU4DJEBZHXh0CfhqVO9UifJOfmn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--34d0f3fd-666a-40ab-ab3b-4cde72f9a3ea-0', usage_metadata={'input_tokens': 39, 'output_tokens': 23, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora creamos algunos mensajes que simulan una experiencia de chat con un bot\n",
    "# SystemMessage: establece el comportamiento del asistente\n",
    "# HumanMessage: representa la entrada del usuario\n",
    "response = chat.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a425aaa",
   "metadata": {},
   "source": [
    "Tambi√©n puedes pasar m√°s historial de chat con respuestas de la IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8fd3fe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In Nice, France, you can also explore the charming old town, visit the colorful markets, and enjoy delicious French cuisine.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 64, 'total_tokens': 89, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CfXU5v3BYZTgEAbJqf6PNhNux60IW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--38cf7303-0dad-4264-9c8f-66a61a1d7c0d-0', usage_metadata={'input_tokens': 64, 'output_tokens': 25, 'total_tokens': 89, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tambi√©n puedes pasar m√°s historial de chat con respuestas previas de la IA\n",
    "# Esto le da contexto al modelo sobre la conversaci√≥n anterior\n",
    "response = chat.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ee37a",
   "metadata": {},
   "source": [
    "Tambi√©n puedes excluir el mensaje del sistema si lo deseas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "238a49f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Friday', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 13, 'total_tokens': 14, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CfXU67A0anxF6iFgX76g1dXCTLECK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--d309016d-bfc4-4638-96dc-ee106a6c7a1f-0', usage_metadata={'input_tokens': 13, 'output_tokens': 1, 'total_tokens': 14, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tambi√©n puedes excluir el mensaje del sistema si lo deseas\n",
    "# El modelo responder√° bas√°ndose solo en el mensaje del usuario\n",
    "response = chat.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What day comes after Thursday?\")\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf9634",
   "metadata": {},
   "source": [
    "### **Documentos**\n",
    "Un objeto que contiene un fragmento de texto y metadatos (m√°s informaci√≥n sobre ese texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3bbf58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la clase Document para trabajar con documentos\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ad9bef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear un documento con contenido y metadatos\n",
    "# page_content: el texto del documento\n",
    "# metadata: informaci√≥n adicional sobre el documento (ID, fuente, fecha, etc.)\n",
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd19754",
   "metadata": {},
   "source": [
    "Pero no tienes que incluir metadatos si no quieres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0798d3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pero no tienes que incluir metadatos si no quieres\n",
    "# Puedes crear un documento solo con el contenido\n",
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e462b5d",
   "metadata": {},
   "source": [
    "## Modelos - La interfaz con los cerebros de IA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fe982",
   "metadata": {},
   "source": [
    "###  **Modelo de Lenguaje**\n",
    "¬°Un modelo que hace texto de entrada ‚û°Ô∏è texto de salida!\n",
    "\n",
    "*Observa c√≥mo cambi√© el modelo que estaba usando del predeterminado a gpt-3.5-turbo-instruct (un modelo econ√≥mico de bajo rendimiento). Ver m√°s modelos [aqu√≠](https://platform.openai.com/docs/models)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "74b1a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el modelo de lenguaje de OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Crear una instancia del modelo usando text-ada-001 (un modelo econ√≥mico de bajo rendimiento)\n",
    "# Ver m√°s modelos en https://platform.openai.com/docs/models\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6399c295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSaturday.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invocar el modelo con una pregunta simple\n",
    "llm.invoke(\"What day comes after Friday?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef89bfa",
   "metadata": {},
   "source": [
    "### **Modelo de Chat**\n",
    "Un modelo que toma una serie de mensajes y devuelve un mensaje de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bf091777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el modelo de chat de OpenAI y las clases de mensajes\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Crear un modelo de chat con temperature=1 (m√°s creativo/aleatorio)\n",
    "chat = ChatOpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4260711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the computer go to New York City? To get a byte of the Big Apple!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 43, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CfXU8UdEaN8mCVVYlpTJUBWMQZhVK', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--6b59c56c-0efe-43a9-b782-5d78c28bb0bc-0', usage_metadata={'input_tokens': 43, 'output_tokens': 19, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de un bot que responde con bromas\n",
    "# SystemMessage configura el comportamiento: en este caso, hacer bromas\n",
    "response = chat.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
    "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "    ]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c028f9",
   "metadata": {},
   "source": [
    "### Modelos de Function Calling\n",
    "\n",
    "[Los modelos de function calling](https://openai.com/blog/function-calling-and-other-api-updates) son similares a los Modelos de Chat pero con un poco de sabor extra. Est√°n afinados para dar salidas de datos estructurados.\n",
    "\n",
    "Esto es √∫til cuando est√°s haciendo una llamada API a un servicio externo o haciendo extracci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1020ff45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"location\":\"Boston, MA\"}', 'name': 'get_current_weather'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 88, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CfXU9cwdUa9tYKyf9UTbZVW3VxKMs', 'service_tier': 'default', 'finish_reason': 'function_call', 'logprobs': None}, id='lc_run--10f12cad-8ef0-444f-b061-25ee86bb73c4-0', usage_metadata={'input_tokens': 88, 'output_tokens': 17, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de Function Calling: el modelo puede devolver datos estructurados\n",
    "# En este caso, definimos una funci√≥n \"get_current_weather\" que el modelo puede llamar\n",
    "chat = ChatOpenAI(model='gpt-4o-mini', temperature=1, openai_api_key=openai_api_key)\n",
    "\n",
    "# Enviar un mensaje y definir qu√© funciones est√°n disponibles\n",
    "output = chat.invoke(\n",
    "     [\n",
    "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "         HumanMessage(content=\"What's the weather like in Boston right now?\")\n",
    "     ],\n",
    "     functions=[{\n",
    "         \"name\": \"get_current_weather\",\n",
    "         \"description\": \"Get the current weather in a given location\",\n",
    "         \"parameters\": {\n",
    "             \"type\": \"object\",\n",
    "             \"properties\": {\n",
    "                 \"location\": {\n",
    "                     \"type\": \"string\",\n",
    "                     \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                 },\n",
    "                 \"unit\": {\n",
    "                     \"type\": \"string\",\n",
    "                     \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                 }\n",
    "             },\n",
    "             \"required\": [\"location\"]\n",
    "         }\n",
    "     }\n",
    "     ]\n",
    ")\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f399a1d",
   "metadata": {},
   "source": [
    "¬øVes el `additional_kwargs` extra que se nos devuelve? Podemos tomar eso y pasarlo a una API externa para obtener datos. Ahorra la molestia de hacer an√°lisis de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70f23",
   "metadata": {},
   "source": [
    "### **Modelo de Embedding de Texto**\n",
    "Cambia tu texto en un vector (una serie de n√∫meros que contienen el 'significado' sem√°ntico de tu texto). Se usa principalmente cuando se comparan dos fragmentos de texto juntos.\n",
    "\n",
    "*PD: Sem√°ntico significa 'relacionado con el significado en el lenguaje o la l√≥gica.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1655de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el modelo de embeddings de OpenAI\n",
    "# Los embeddings convierten texto en vectores num√©ricos que capturan el significado sem√°ntico\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Crear una instancia del modelo de embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a2c85e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de ejemplo para convertir en embedding\n",
    "text = \"Hi! It's time for the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ddc5a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [-0.00022214034106582403, -0.0031126115936785936, -0.0010768607025966048, -0.019214099273085594, -0.015184946358203888]...\n",
      "Your embedding is length 1536\n"
     ]
    }
   ],
   "source": [
    "# Generar el embedding (vector) del texto\n",
    "text_embedding = embeddings.embed_query(text)\n",
    "# Mostrar una muestra de los primeros 5 n√∫meros del vector\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "# Mostrar la longitud total del vector (t√≠picamente 1536 para OpenAI)\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fe99f",
   "metadata": {},
   "source": [
    "## Prompts - Texto generalmente usado como instrucciones para tu modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9318ed",
   "metadata": {},
   "source": [
    "### **Prompt**\n",
    "Lo que pasar√°s al modelo subyacente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d270239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The statement skips over Tuesday and jumps directly from Monday to Wednesday. It should say \"Today is Monday, tomorrow is Tuesday.\"\n"
     ]
    }
   ],
   "source": [
    "# Importar el modelo de lenguaje de OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Crear instancia del modelo usando text-davinci-003\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Me gusta usar tres comillas dobles para mis prompts porque es m√°s f√°cil de leer\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "\n",
    "# Invocar el modelo con el prompt\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74988254",
   "metadata": {},
   "source": [
    "### **Prompt Template**\n",
    "Un objeto que ayuda a crear prompts basados en una combinaci√≥n de entrada del usuario, otra informaci√≥n no est√°tica y una cadena de plantilla fija.\n",
    "\n",
    "Piensa en ello como un [f-string](https://realpython.com/python-f-strings/) en python pero para prompts\n",
    "\n",
    "*Avanzado: Consulta LangSmithHub(https://smith.langchain.com/hub) para muchos m√°s templates de prompts de la comunidad*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "abcc212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence\n",
      "\n",
      "-----------\n",
      "LLM Output: \n",
      "Visit historical landmarks such as the Colosseum and Vatican City.\n"
     ]
    }
   ],
   "source": [
    "# Importar las clases necesarias para trabajar con templates de prompts\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Crear instancia del modelo\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Nota: \"location\" es un marcador de posici√≥n que ser√° reemplazado m√°s tarde\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there?\n",
    "\n",
    "Respond in one short sentence\n",
    "\"\"\"\n",
    "\n",
    "# Crear el template de prompt con la variable \"location\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"location\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Formatear el prompt reemplazando {location} con 'Rome'\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "# Invocar el modelo con el prompt formateado\n",
    "print (f\"LLM Output: {llm.invoke(final_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40bac2",
   "metadata": {},
   "source": [
    "### **Selectores de Ejemplos**\n",
    "Una forma f√°cil de seleccionar de una serie de ejemplos que te permiten colocar din√°micamente informaci√≥n en contexto en tu prompt. A menudo se usa cuando tu tarea es matizada o tienes una gran lista de ejemplos.\n",
    "\n",
    "Consulta diferentes tipos de selectores de ejemplos [aqu√≠](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/)\n",
    "\n",
    "Si quieres una visi√≥n general de por qu√© los ejemplos son importantes (ingenier√≠a de prompts), consulta [este video](https://www.youtube.com/watch?v=dOxUroR57xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "aaf36cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las clases necesarias para selectores de ejemplos sem√°nticos\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Crear instancia del modelo\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Template para formatear cada ejemplo\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Example Input: {input}\\nExample Output: {output}\",\n",
    ")\n",
    "\n",
    "# Ejemplos de ubicaciones donde se encuentran sustantivos\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12b4798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector seleccionar√° ejemplos similares a tu entrada por significado sem√°ntico\n",
    "\n",
    "# Crear el selector de ejemplos\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # Esta es la lista de ejemplos disponibles para seleccionar\n",
    "    examples, \n",
    "    \n",
    "    # Esta es la clase de embedding utilizada para medir la similitud sem√°ntica\n",
    "    OpenAIEmbeddings(openai_api_key=openai_api_key), \n",
    "    \n",
    "    # Esta es la clase VectorStore que almacena los embeddings y hace b√∫squedas de similitud\n",
    "    Chroma, \n",
    "    \n",
    "    # Este es el n√∫mero de ejemplos a producir\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2cf30107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un prompt con pocos ejemplos (Few-Shot) usando el selector sem√°ntico\n",
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # El objeto que ayudar√° a seleccionar ejemplos\n",
    "    example_selector=example_selector,\n",
    "    \n",
    "    # Tu template de ejemplo\n",
    "    example_prompt=example_prompt,\n",
    "    \n",
    "    # Personalizaciones que se agregar√°n al inicio y final de tu prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    \n",
    "    # Qu√© variables recibir√° tu prompt\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "369442bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give the location an item is usually found in\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Example Input: tree\n",
      "Example Output: ground\n",
      "\n",
      "Input: plant\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "# ¬°Selecciona un sustantivo!\n",
    "my_noun = \"plant\"\n",
    "# my_noun = \"student\"\n",
    "\n",
    "# Formatear el prompt con el sustantivo y ver qu√© ejemplos se seleccionan\n",
    "print(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9bb910f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' garden'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invocar el modelo con el prompt formateado\n",
    "llm.invoke(similar_prompt.format(noun=my_noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474c91d",
   "metadata": {},
   "source": [
    "### **Analizadores de Salida M√©todo 1: Instrucciones de Prompt y An√°lisis de Cadenas**\n",
    "Una forma √∫til de formatear la salida de un modelo. Generalmente se usa para salida estructurada. LangChain tiene muchos m√°s analizadores de salida listados en su [documentaci√≥n](https://python.langchain.com/docs/modules/model_io/output_parsers).\n",
    "\n",
    "Dos conceptos grandes:\n",
    "\n",
    "**1. Instrucciones de Formato** - Un prompt autogenerado que le dice al LLM c√≥mo formatear su respuesta bas√°ndose en tu resultado deseado\n",
    "\n",
    "**2. Analizador** - Un m√©todo que extraer√° la salida de texto de tu modelo en una estructura deseada (generalmente json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "58353756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta celda usa APIs deprecadas. Por favor, salta a la secci√≥n 'Analizadores de Salida M√©todo 2: Funciones de OpenAI' m√°s abajo.\n"
     ]
    }
   ],
   "source": [
    "# NOTA: StructuredOutputParser est√° deprecado en versiones nuevas de LangChain\n",
    "# Se recomienda usar OpenAI Functions (M√©todo 2 m√°s abajo) para salidas estructuradas\n",
    "# Esta celda se mantiene para referencia pero puede no funcionar en versiones recientes\n",
    "\n",
    "# Para versiones actuales, saltea esta secci√≥n y usa directamente el M√©todo 2 (OpenAI Functions)\n",
    "print(\"Esta celda usa APIs deprecadas. Por favor, salta a la secci√≥n 'Analizadores de Salida M√©todo 2: Funciones de OpenAI' m√°s abajo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ee36f881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\n"
     ]
    }
   ],
   "source": [
    "# Celda deprecada - salta a la secci√≥n de OpenAI Functions\n",
    "print(\"Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fa59be3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\n"
     ]
    }
   ],
   "source": [
    "# Celda deprecada - salta a la secci√≥n de OpenAI Functions\n",
    "print(\"Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d1079f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\n"
     ]
    }
   ],
   "source": [
    "# Celda deprecada - salta a la secci√≥n de OpenAI Functions\n",
    "print(\"Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9aaae5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\n"
     ]
    }
   ],
   "source": [
    "# Celda deprecada - salta a la secci√≥n de OpenAI Functions\n",
    "print(\"Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b116bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\n"
     ]
    }
   ],
   "source": [
    "# Celda deprecada - salta a la secci√≥n de OpenAI Functions\n",
    "print(\"Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "985aa814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\n"
     ]
    }
   ],
   "source": [
    "# Celda deprecada - salta a la secci√≥n de OpenAI Functions\n",
    "print(\"Esta celda est√° deprecada. Usa el M√©todo 2 (OpenAI Functions) en su lugar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07045ae3",
   "metadata": {},
   "source": [
    "### **Analizadores de Salida M√©todo 2: Funciones de OpenAI**\n",
    "Cuando OpenAI lanz√≥ function calling, el juego cambi√≥. Este es el m√©todo recomendado cuando est√°s empezando.\n",
    "\n",
    "Entrenaron modelos espec√≠ficamente para dar salidas de datos estructurados. Se volvi√≥ s√∫per f√°cil especificar un esquema Pydantic y obtener una salida estructurada.\n",
    "\n",
    "Hay muchas formas de definir tu esquema, prefiero usar Modelos Pydantic por lo organizados que son. Si√©ntete libre de consultar la [documentaci√≥n](https://platform.openai.com/docs/guides/gpt/function-calling) de OpenAI para otros m√©todos.\n",
    "\n",
    "Para usar este m√©todo necesitar√°s usar un modelo que soporte [function calling](https://openai.com/blog/function-calling-and-other-api-updates#:~:text=Developers%20can%20now%20describe%20functions%20to%20gpt%2D4%2D0613%20and%20gpt%2D3.5%2Dturbo%2D0613%2C). Usar√© `gpt4-0613`\n",
    "\n",
    "**Ejemplo 1: Simple**\n",
    "\n",
    "Comencemos definiendo un modelo simple para que extraigamos de √©l."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3593699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar Pydantic para definir modelos de datos estructurados\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "# Definir un modelo Pydantic para informaci√≥n de una persona\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Informaci√≥n de identificaci√≥n sobre una persona.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"El nombre de la persona\")\n",
    "    age: int = Field(..., description=\"La edad de la persona\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"La comida favorita de la persona\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17033d15",
   "metadata": {},
   "source": [
    "Entonces creemos una cadena (m√°s sobre esto m√°s adelante) que har√° la extracci√≥n por nosotros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "60b7be09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='Sally', age=13, fav_food=None)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# M√©todo moderno usando with_structured_output (reemplaza create_structured_output_chain)\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Crear instancia del modelo GPT-4o-mini (modelo actual y econ√≥mico)\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', openai_api_key=openai_api_key)\n",
    "\n",
    "# Crear un extractor estructurado usando el m√©todo moderno\n",
    "structured_llm = llm.with_structured_output(Person)\n",
    "\n",
    "# Ejecutar con un texto de ejemplo\n",
    "result = structured_llm.invoke(\n",
    "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37370210",
   "metadata": {},
   "source": [
    "¬øNotas c√≥mo solo tenemos datos de una persona de esa lista? Eso es porque no especificamos que quer√≠amos m√∫ltiples. Cambiemos nuestro esquema para especificar que queremos una lista de personas si es posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "df4ad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar Sequence para definir listas tipadas\n",
    "from typing import Sequence\n",
    "\n",
    "# Definir un modelo que puede contener m√∫ltiples personas\n",
    "class People(BaseModel):\n",
    "    \"\"\"Informaci√≥n de identificaci√≥n sobre todas las personas en un texto.\"\"\"\n",
    "\n",
    "    people: Sequence[Person] = Field(..., description=\"Las personas en el texto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bc127",
   "metadata": {},
   "source": [
    "Ahora llamaremos con People en lugar de Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5ba430d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "People(people=[Person(name='Sally', age=13, fav_food=None), Person(name='Joey', age=12, fav_food='spinach'), Person(name='Caroline', age=23, fav_food=None)])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora usamos People en lugar de Person para extraer m√∫ltiples personas\n",
    "structured_llm = llm.with_structured_output(People)\n",
    "\n",
    "result = structured_llm.invoke(\n",
    "    \"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db9b8b",
   "metadata": {},
   "source": [
    "Hagamos m√°s an√°lisis con √©l\n",
    "\n",
    "**Ejemplo 2: Enum**\n",
    "\n",
    "Ahora analicemos cuando se menciona un producto de una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6616a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar enum para definir opciones limitadas\n",
    "import enum\n",
    "\n",
    "# Crear instancia del modelo con gpt-4o-mini (modelo actual)\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', openai_api_key=openai_api_key)\n",
    "\n",
    "# Definir un enum con productos espec√≠ficos\n",
    "class Product(str, enum.Enum):\n",
    "    CRM = \"CRM\"\n",
    "    VIDEO_EDITING = \"VIDEO_EDITING\"\n",
    "    HARDWARE = \"HARDWARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a5250ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un modelo para identificar productos mencionados\n",
    "class Products(BaseModel):\n",
    "    \"\"\"Identificar productos que fueron mencionados en un texto\"\"\"\n",
    "\n",
    "    products: Sequence[Product] = Field(..., description=\"Los productos mencionados en un texto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dd7e0bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Products(products=[<Product.CRM: 'CRM'>, <Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear extractor para productos y ejecutarlo\n",
    "structured_llm = llm.with_structured_output(Products)\n",
    "\n",
    "result = structured_llm.invoke(\n",
    "    \"The CRM in this demo is great. Love the hardware. The microphone is also cool. Love the video editing\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43cec2",
   "metadata": {},
   "source": [
    "## √çndices - Estructurar documentos para que los LLMs puedan trabajar con ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f904e9",
   "metadata": {},
   "source": [
    "### **Cargadores de Documentos**\n",
    "Formas f√°ciles de importar datos de otras fuentes. Funcionalidad compartida con [Plugins de OpenAI](https://openai.com/blog/chatgpt-plugins) [espec√≠ficamente plugins de recuperaci√≥n](https://github.com/openai/chatgpt-retrieval-plugin)\n",
    "\n",
    "Ver una [gran lista](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) de cargadores de documentos aqu√≠. Un mont√≥n m√°s en [Llama Index](https://llamahub.ai/) tambi√©n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4719d4",
   "metadata": {},
   "source": [
    "**HackerNews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ba88e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el cargador de documentos de HackerNews\n",
    "from langchain_community.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ee693520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un cargador para una p√°gina espec√≠fica de HackerNews\n",
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "88d89ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al cargar datos de HackerNews: 'NoneType' object has no attribute 'get'\n",
      "El cargador de HackerNews puede estar desactualizado. Prueba con otros cargadores de documentos.\n"
     ]
    }
   ],
   "source": [
    "# Cargar los datos (comentarios) de la p√°gina\n",
    "# NOTA: HNLoader puede fallar si la estructura de HackerNews cambia\n",
    "try:\n",
    "    data = loader.load()\n",
    "    print(f\"Cargados {len(data)} comentarios exitosamente\")\n",
    "except AttributeError as e:\n",
    "    print(f\"Error al cargar datos de HackerNews: {e}\")\n",
    "    print(\"El cargador de HackerNews puede estar desactualizado. Prueba con otros cargadores de documentos.\")\n",
    "    data = []  # Lista vac√≠a para que las siguientes celdas no fallen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e814f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudieron cargar comentarios. El cargador de HackerNews puede tener problemas.\n"
     ]
    }
   ],
   "source": [
    "# Imprimir la cantidad de comentarios encontrados y una muestra\n",
    "if len(data) > 0:\n",
    "    print(f\"Found {len(data)} comments\")\n",
    "    print(f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")\n",
    "else:\n",
    "    print(\"No se pudieron cargar comentarios. El cargador de HackerNews puede tener problemas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564583f",
   "metadata": {},
   "source": [
    "**Libros del Proyecto Gutenberg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "72964fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el cargador de libros del Proyecto Gutenberg\n",
    "from langchain_community.document_loaders import GutenbergLoader\n",
    "\n",
    "# Cargar un libro espec√≠fico (en este caso, un libro de Edgar Allan Poe)\n",
    "loader = GutenbergLoader(\"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "47140a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o.‚Äî_Seneca_.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "      At Paris, just after dark one gusty evening in the autumn of 18-,\n",
      "\n",
      "\n",
      "      I was enjoying the twofold l\n"
     ]
    }
   ],
   "source": [
    "# Imprimir una muestra del contenido del libro\n",
    "print(data[0].page_content[1855:1984])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1386b0",
   "metadata": {},
   "source": [
    "**URLs y p√°ginas web**\n",
    "\n",
    "Prob√©moslo con [el sitio web de Paul Graham](http://www.paulgraham.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "46a54e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New: Good Writing | Founder Mode Want to start a startup? Get funded by Y Combinator . ¬© mmxxv pg'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importar el cargador de URLs para p√°ginas web\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "# Lista de URLs para cargar\n",
    "urls = [\n",
    "    \"http://www.paulgraham.com/\",\n",
    "]\n",
    "\n",
    "# Crear el cargador con las URLs\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "# Cargar los datos de las p√°ginas web\n",
    "data = loader.load()\n",
    "\n",
    "# Mostrar el contenido de la primera p√°gina\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9601db",
   "metadata": {},
   "source": [
    "### **Divisores de Texto**\n",
    "A menudo tus documentos son demasiado largos (como un libro) para tu LLM. Necesitas dividirlo en fragmentos. Los divisores de texto ayudan con esto.\n",
    "\n",
    "Hay muchas formas en que podr√≠as dividir tu texto en fragmentos, experimenta con [diferentes](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) para ver cu√°l es mejor para ti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "95713e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar el divisor de texto recursivo\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a54455f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo 'data/PaulGrahamEssays/worked.txt' no existe.\n",
      "Puedes descargarlo de: https://github.com/gkamradt/langchain-tutorials/tree/main/data/PaulGrahamEssays\n",
      "O usar tu propio texto para probar:\n",
      "\n",
      "Usando texto de ejemplo. You have 1 document\n"
     ]
    }
   ],
   "source": [
    "# Este es un documento largo que podemos dividir\n",
    "# NOTA: Si no tienes el archivo, puedes descargarlo de:\n",
    "# https://github.com/gkamradt/langchain-tutorials/tree/main/data/PaulGrahamEssays\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.exists('data/PaulGrahamEssays/worked.txt'):\n",
    "    with open('data/PaulGrahamEssays/worked.txt') as f:\n",
    "        pg_work = f.read()\n",
    "    print(f\"You have {len([pg_work])} document\")\n",
    "else:\n",
    "    print(\"El archivo 'data/PaulGrahamEssays/worked.txt' no existe.\")\n",
    "    print(\"Puedes descargarlo de: https://github.com/gkamradt/langchain-tutorials/tree/main/data/PaulGrahamEssays\")\n",
    "    print(\"O usar tu propio texto para probar:\")\n",
    "    \n",
    "    # Texto de ejemplo para demostraci√≥n\n",
    "    pg_work = \"\"\"This is a sample text for demonstration purposes. \n",
    "    In real scenarios, you would load a longer document here.\n",
    "    You can replace this with any long text you want to split into chunks.\n",
    "    The text splitter will divide this into smaller pieces based on the chunk_size parameter.\"\"\"\n",
    "    print(f\"\\nUsando texto de ejemplo. You have {len([pg_work])} document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d19acb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un divisor de texto con configuraci√≥n personalizada\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Establecer un tama√±o de chunk peque√±o, solo para demostrar\n",
    "    chunk_size = 150,\n",
    "    # Superposici√≥n entre chunks para mantener contexto\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "# Crear documentos divididos\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e3090f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 3 documents\n"
     ]
    }
   ],
   "source": [
    "# Mostrar cu√°ntos documentos tenemos despu√©s de dividir\n",
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "87a0f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "This is a sample text for demonstration purposes. \n",
      "    In real scenarios, you would load a longer document here. \n",
      "\n",
      "You can replace this with any long text you want to split into chunks.\n"
     ]
    }
   ],
   "source": [
    "# Vista previa de los primeros chunks\n",
    "print (\"Preview:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e670d",
   "metadata": {},
   "source": [
    "Hay un mont√≥n de formas diferentes de hacer divisi√≥n de texto y realmente depende de tu estrategia de recuperaci√≥n y dise√±o de aplicaci√≥n. Consulta m√°s divisores [aqu√≠](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85defb",
   "metadata": {},
   "source": [
    "### **Recuperadores**\n",
    "Forma f√°cil de combinar documentos con modelos de lenguaje.\n",
    "\n",
    "Hay muchos tipos diferentes de recuperadores, el m√°s ampliamente soportado es el VectoreStoreRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8cccbd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo no encontrado. Usando texto de ejemplo.\n"
     ]
    }
   ],
   "source": [
    "# Importar las clases necesarias para recuperadores y vectorstores\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Cargar un documento de texto\n",
    "if os.path.exists('data/PaulGrahamEssays/worked.txt'):\n",
    "    loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "    documents = loader.load()\n",
    "    print(\"Archivo cargado exitosamente\")\n",
    "else:\n",
    "    print(\"Archivo no encontrado. Usando texto de ejemplo.\")\n",
    "    from langchain_core.documents import Document\n",
    "    documents = [Document(page_content=\"\"\"This is sample text for demonstration. \n",
    "    In a real scenario, you would load a document from a file.\n",
    "    This text will be split into chunks and used for retrieval demonstrations.\n",
    "    You can replace this with any text content you want to experiment with.\"\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1dab1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar tu divisor de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Dividir tus documentos en textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Preparar el motor de embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Embedear tus textos y crear la base de datos vectorial FAISS\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e62372be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar tu recuperador\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e0534bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000012E5BD18750>, search_kwargs={})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver el objeto recuperador\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3846a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener documentos relevantes basados en una consulta\n",
    "# Nota: get_relevant_documents est√° deprecado, ahora se usa invoke\n",
    "docs = retriever.invoke(\"what types of things did the author want to build?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "db383cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is sample text for demonstration. \n",
      "    In a real scenario, you would load a document from a file.\n",
      "    This text will be split into chunks and used for retrieval demonstrations.\n",
      "    You can replac\n"
     ]
    }
   ],
   "source": [
    "# Mostrar los primeros 200 caracteres de los 2 documentos m√°s relevantes\n",
    "print(\"\\n\\n\".join([x.page_content[:200] for x in docs[:2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24193139",
   "metadata": {},
   "source": [
    "### **VectorStores**\n",
    "Bases de datos para almacenar vectores. Las m√°s populares son [Pinecone](https://www.pinecone.io/) y [Weaviate](https://weaviate.io/). M√°s ejemplos en la [documentaci√≥n de recuperaci√≥n](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database) de OpenAI. [Chroma](https://www.trychroma.com/) y [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) son f√°ciles de trabajar localmente.\n",
    "\n",
    "Conceptualmente, piensa en ellos como tablas con una columna para embeddings (vectores) y una columna para metadatos.\n",
    "\n",
    "Ejemplo\n",
    "\n",
    "| Embedding      | Metadata |\n",
    "| ----------- | ----------- |\n",
    "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
    "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3c5533ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo no encontrado. Usando texto de ejemplo.\n"
     ]
    }
   ],
   "source": [
    "# Importar las clases necesarias\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "# Cargar el documento\n",
    "if os.path.exists('data/PaulGrahamEssays/worked.txt'):\n",
    "    loader = TextLoader('data/PaulGrahamEssays/worked.txt')\n",
    "    documents = loader.load()\n",
    "else:\n",
    "    print(\"Archivo no encontrado. Usando texto de ejemplo.\")\n",
    "    from langchain_core.documents import Document\n",
    "    documents = [Document(page_content=\"\"\"Sample text for embeddings demonstration.\n",
    "    This text will be used to create vector embeddings.\n",
    "    Each piece of text gets converted into a numerical vector.\n",
    "    These vectors capture the semantic meaning of the text.\"\"\")]\n",
    "\n",
    "# Preparar tu divisor de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "\n",
    "# Dividir tus documentos en textos\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Preparar el motor de embeddings\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "661fdf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 documents\n"
     ]
    }
   ],
   "source": [
    "# Mostrar cu√°ntos documentos tenemos\n",
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e99ac0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear embeddings para cada documento\n",
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "89e7758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 embeddings\n",
      "Here's a sample of one: [-0.02986394800245762, 0.0034656021744012833, 0.01732276752591133]...\n"
     ]
    }
   ],
   "source": [
    "# Mostrar informaci√≥n sobre los embeddings\n",
    "print (f\"You have {len(embedding_list)} embeddings\")\n",
    "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac358c5",
   "metadata": {},
   "source": [
    "Tu vectorstore almacena tus embeddings (‚òùÔ∏è) y los hace f√°cilmente buscables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9b79b",
   "metadata": {},
   "source": [
    "## Memoria\n",
    "Ayudar a los LLMs a recordar informaci√≥n.\n",
    "\n",
    "La memoria es un t√©rmino un poco suelto. Podr√≠a ser tan simple como recordar informaci√≥n sobre la que has charlado en el pasado o recuperaci√≥n de informaci√≥n m√°s complicada.\n",
    "\n",
    "Lo mantendremos hacia el caso de uso de Mensajes de Chat. Esto se usar√≠a para chatbots.\n",
    "\n",
    "Hay muchos tipos de memoria, explora [la documentaci√≥n](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html) para ver cu√°l se ajusta a tu caso de uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b49da",
   "metadata": {},
   "source": [
    "### Historial de Mensajes de Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "893a18c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensajes iniciales creados\n"
     ]
    }
   ],
   "source": [
    "# Importar las clases necesarias para trabajar con mensajes de chat\n",
    "# NOTA: langchain.memory est√° deprecado, ahora usamos langchain_core.messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Crear instancia del modelo con temperature=0 (m√°s determinista)\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Crear una lista simple para almacenar mensajes (reemplaza ChatMessageHistory deprecado)\n",
    "messages = []\n",
    "\n",
    "# Agregar un mensaje de la IA\n",
    "messages.append(AIMessage(content=\"hi!\"))\n",
    "\n",
    "# Agregar un mensaje del usuario\n",
    "messages.append(HumanMessage(content=\"what is the capital of france?\"))\n",
    "\n",
    "print(\"Mensajes iniciales creados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a2949fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of france?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver los mensajes almacenados en la lista\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "9b74d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 20, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CfXUM123mKgDIxDh33HZTd2DG73f3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5aff2e15-8887-47b8-9f6d-c4cd82463d6b-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener la respuesta de la IA usando los mensajes almacenados\n",
    "ai_response = chat.invoke(messages)\n",
    "ai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "529e168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of france?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 20, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CfXUM123mKgDIxDh33HZTd2DG73f3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5aff2e15-8887-47b8-9f6d-c4cd82463d6b-0', usage_metadata={'input_tokens': 20, 'output_tokens': 7, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agregar la respuesta de la IA a la lista de mensajes\n",
    "messages.append(ai_response)\n",
    "# Ver los mensajes actualizados\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fc79c",
   "metadata": {},
   "source": [
    "## Cadenas ‚õìÔ∏è‚õìÔ∏è‚õìÔ∏è\n",
    "Combinando diferentes llamadas LLM y acciones autom√°ticamente\n",
    "\n",
    "Ej: Resumen #1, Resumen #2, Resumen #3 > Resumen Final\n",
    "\n",
    "Consulta [este video](https://www.youtube.com/watch?v=f9_BWhCI4Zo&t=2s) explicando diferentes tipos de cadenas de resumen\n",
    "\n",
    "Hay [muchas aplicaciones de cadenas](https://python.langchain.com/en/latest/modules/chains/how_to_guides.html) busca para ver cu√°les son mejores para tu caso de uso.\n",
    "\n",
    "Cubriremos dos de ellas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ba415",
   "metadata": {},
   "source": [
    "### 1. Cadenas Secuenciales Simples\n",
    "\n",
    "Cadenas f√°ciles donde puedes usar la salida de un LLM como entrada en otro. Buenas para dividir tareas (y mantener tu LLM enfocado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "79fc0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las clases necesarias para cadenas usando LCEL (moderno)\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Crear instancia del modelo con temperature=1 (m√°s creativo)\n",
    "llm = OpenAI(temperature=1, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43d4494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template para la primera cadena: sugerir un plato t√≠pico basado en la ubicaci√≥n\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "% USER LOCATION\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_location\"], template=template)\n",
    "\n",
    "# Crear la cadena usando LCEL (LangChain Expression Language)\n",
    "# El operador | encadena el prompt con el modelo\n",
    "location_chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b6c8e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template para la segunda cadena: dar una receta para el plato\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "% MEAL\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Crear la cadena usando LCEL\n",
    "meal_chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7e0b83f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cadena secuencial creada usando LCEL (m√©todo moderno)\n"
     ]
    }
   ],
   "source": [
    "# Crear una cadena secuencial usando LCEL\n",
    "# Primero ejecuta location_chain, luego pasa el resultado a meal_chain\n",
    "# RunnablePassthrough permite pasar el resultado de una cadena a la siguiente\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Crear una funci√≥n que toma la salida de location_chain y la formatea para meal_chain\n",
    "def format_for_meal_chain(location_output):\n",
    "    return {\"user_meal\": location_output}\n",
    "\n",
    "# Encadenar: location_chain -> formatear -> meal_chain\n",
    "overall_chain = (\n",
    "    location_chain \n",
    "    | format_for_meal_chain \n",
    "    | meal_chain\n",
    ")\n",
    "\n",
    "print(\"Cadena secuencial creada usando LCEL (m√©todo moderno)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d19c64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando cadena con ubicaci√≥n: Rome\n",
      "--------------------------------------------------\n",
      "To make Spaghetti alla Carbonara at home, you will need: spaghetti, eggs, Parmesan cheese, bacon or pancetta, black pepper, and salt. Start by boiling the spaghetti in a pot of salted water until al dente, then drain and set aside. In a separate pan, cook the bacon or pancetta until crispy. Add the spaghetti to the pan and turn off the heat. In a bowl, whisk together eggs, Parmesan cheese, black pepper, and a pinch of salt. Pour the egg mixture over the spaghetti and toss until the pasta is coated evenly. The heat from the pasta will cook the eggs and create a creamy sauce. Serve hot with a sprinkle of Parmesan and extra black pepper on top. Buon appetito!\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la cadena completa con \"Rome\" como entrada\n",
    "print(\"Ejecutando cadena con ubicaci√≥n: Rome\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Invocar la cadena con la ubicaci√≥n\n",
    "review = overall_chain.invoke({\"user_location\": \"Rome\"})\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6191bf5",
   "metadata": {},
   "source": [
    "### 2. Cadena de Resumen\n",
    "\n",
    "Ejecuta f√°cilmente a trav√©s de numerosos documentos largos y obt√©n un resumen. Consulta [este video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) para otros tipos de cadenas adem√°s de map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6f218c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No module named 'langchain.chains'\n",
      "La API de summarize chain puede estar deprecada. Usa m√©todos LCEL modernos en su lugar.\n"
     ]
    }
   ],
   "source": [
    "# NOTA: load_summarize_chain puede estar deprecado\n",
    "# Intentaremos usarlo pero si falla, usa m√©todos LCEL modernos\n",
    "\n",
    "try:\n",
    "    from langchain.chains.summarize import load_summarize_chain\n",
    "    from langchain_community.document_loaders import TextLoader\n",
    "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "    import os\n",
    "\n",
    "    # Cargar un documento\n",
    "    if os.path.exists('data/PaulGrahamEssays/disc.txt'):\n",
    "        loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "        documents = loader.load()\n",
    "    else:\n",
    "        print(\"Archivo disc.txt no encontrado. Usando texto de ejemplo.\")\n",
    "        from langchain_core.documents import Document\n",
    "        documents = [Document(page_content=\"\"\"This is a longer text that will be summarized.\n",
    "        The summarization chain takes multiple chunks of text and creates a concise summary.\n",
    "        This is useful when you have very long documents and need to extract key information.\n",
    "        The map-reduce approach processes each chunk and then combines the results.\"\"\" * 10)]\n",
    "\n",
    "    # Preparar tu divisor de texto\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "    # Dividir tus documentos en textos\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Crear cadena de resumen\n",
    "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "    result = chain.run(texts)\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"La API de summarize chain puede estar deprecada. Usa m√©todos LCEL modernos en su lugar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6193c",
   "metadata": {},
   "source": [
    "## Agentes ü§ñü§ñ\n",
    "\n",
    "La Documentaci√≥n Oficial de LangChain describe a los agentes perfectamente (√©nfasis m√≠o):\n",
    "> Algunas aplicaciones requerir√°n no solo una cadena predeterminada de llamadas a LLMs/otras herramientas, sino potencialmente una **cadena desconocida** que depende de la entrada del usuario. En estos tipos de cadenas, hay un \"agente\" que tiene acceso a un conjunto de herramientas. Dependiendo de la entrada del usuario, el agente puede entonces **decidir cu√°l, si alguna, de estas herramientas llamar**.\n",
    "\n",
    "\n",
    "B√°sicamente usas el LLM no solo para salida de texto, sino tambi√©n para toma de decisiones. La genialidad y el poder de esta funcionalidad no pueden ser exagerados.\n",
    "\n",
    "Sam Altman enfatiza que los LLMs son buenos '[motores de razonamiento](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Los agentes aprovechan esto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce05d51",
   "metadata": {},
   "source": [
    "### Agentes\n",
    "\n",
    "El modelo de lenguaje que impulsa la toma de decisiones.\n",
    "\n",
    "M√°s espec√≠ficamente, un agente toma una entrada y devuelve una respuesta correspondiente a una acci√≥n a tomar junto con una entrada de acci√≥n. Puedes ver diferentes tipos de agentes (que son mejores para diferentes casos de uso) [aqu√≠](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696b65c",
   "metadata": {},
   "source": [
    "### Herramientas\n",
    "\n",
    "Una 'capacidad' de un agente. Esta es una abstracci√≥n encima de una funci√≥n que facilita a los LLMs (y agentes) interactuar con ella. Ej: b√∫squeda de Google.\n",
    "\n",
    "Esta √°rea comparte similitudes con [plugins de OpenAI](https://platform.openai.com/docs/plugins/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f8231",
   "metadata": {},
   "source": [
    "### Toolkit\n",
    "\n",
    "Grupos de herramientas de las que tu agente puede seleccionar\n",
    "\n",
    "Junt√©moslos todos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "67d5d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de chat inicializado para uso con agentes\n"
     ]
    }
   ],
   "source": [
    "# NOTA: Las APIs de agentes (load_tools, initialize_agent) est√°n deprecadas\n",
    "# El siguiente ejemplo muestra el patr√≥n moderno de agentes usando herramientas directamente\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Crear instancia del modelo de chat (requerido para agentes modernos)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "print(\"Modelo de chat inicializado para uso con agentes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0ddcdbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è ADVERTENCIA: No se encontr√≥ SERP_API_KEY en las variables de entorno\n",
      "Para usar b√∫squedas web, reg√≠strate en https://serpapi.com/\n"
     ]
    }
   ],
   "source": [
    "# Obtener la clave API de SerpAPI desde las variables de entorno\n",
    "# SerpAPI es un servicio de b√∫squeda que requiere una clave API\n",
    "# Reg√≠strate en https://serpapi.com/ para obtener una clave gratuita\n",
    "serpapi_api_key=os.getenv(\"SERP_API_KEY\", \"YourAPIKey\")\n",
    "\n",
    "if serpapi_api_key == \"YourAPIKey\":\n",
    "    print(\"‚ö†Ô∏è ADVERTENCIA: No se encontr√≥ SERP_API_KEY en las variables de entorno\")\n",
    "    print(\"Para usar b√∫squedas web, reg√≠strate en https://serpapi.com/\")\n",
    "else:\n",
    "    print(\"‚úì SERP_API_KEY configurada correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "44fad67f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error al crear herramienta de b√∫squeda: cannot import name 'SerpAPIWrapper' from 'langchain_community.tools' (c:\\Users\\borja\\anaconda3\\envs\\multiagente\\Lib\\site-packages\\langchain_community\\tools\\__init__.py)\n",
      "Continuando sin la herramienta de b√∫squeda...\n"
     ]
    }
   ],
   "source": [
    "# NOTA: load_tools est√° deprecado\n",
    "# El patr√≥n moderno es crear herramientas directamente o usar langchain-community\n",
    "\n",
    "try:\n",
    "    from langchain_community.tools import SerpAPIWrapper\n",
    "    from langchain_core.tools import Tool\n",
    "    \n",
    "    # Crear la herramienta de b√∫squeda\n",
    "    search = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)\n",
    "    \n",
    "    # Crear una herramienta usando el wrapper\n",
    "    search_tool = Tool(\n",
    "        name=\"Google Search\",\n",
    "        description=\"√ötil para buscar informaci√≥n actual en internet. Usa esto cuando necesites responder preguntas sobre eventos actuales o informaci√≥n espec√≠fica.\",\n",
    "        func=search.run,\n",
    "    )\n",
    "    \n",
    "    toolkit = [search_tool]\n",
    "    print(\"‚úì Herramienta de b√∫squeda creada exitosamente\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error al crear herramienta de b√∫squeda: {e}\")\n",
    "    print(\"Continuando sin la herramienta de b√∫squeda...\")\n",
    "    toolkit = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f544a74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error al crear agente: cannot import name 'create_react_agent' from 'langchain.agents' (c:\\Users\\borja\\anaconda3\\envs\\multiagente\\Lib\\site-packages\\langchain\\agents\\__init__.py)\n",
      "Las APIs de agentes pueden requerir instalaci√≥n adicional\n",
      "Intenta: pip install langchain-community google-search-results\n"
     ]
    }
   ],
   "source": [
    "# NOTA: initialize_agent est√° deprecado\n",
    "# El patr√≥n moderno es usar create_react_agent o construir agentes con LangGraph\n",
    "\n",
    "try:\n",
    "    from langchain.agents import create_react_agent, AgentExecutor\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    \n",
    "    # Template para el agente ReAct (Reasoning + Acting)\n",
    "    template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    \n",
    "    # Crear el agente ReAct\n",
    "    agent = create_react_agent(llm, toolkit, prompt)\n",
    "    \n",
    "    # Crear el executor del agente\n",
    "    agent_executor = AgentExecutor(\n",
    "        agent=agent, \n",
    "        tools=toolkit, \n",
    "        verbose=True,\n",
    "        return_intermediate_steps=True,\n",
    "        handle_parsing_errors=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Agente ReAct creado exitosamente con patr√≥n moderno\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error al crear agente: {e}\")\n",
    "    print(\"Las APIs de agentes pueden requerir instalaci√≥n adicional\")\n",
    "    print(\"Intenta: pip install langchain-community google-search-results\")\n",
    "    agent_executor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c4882754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è El agente no est√° disponible. Verifica la celda anterior para m√°s detalles.\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el agente con una consulta compleja\n",
    "# El agente decidir√° autom√°ticamente qu√© b√∫squedas hacer para responder\n",
    "\n",
    "if agent_executor is not None:\n",
    "    try:\n",
    "        response = agent_executor.invoke({\n",
    "            \"input\": \"what was the first album of the band that Natalie Bergman is a part of?\"\n",
    "        })\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RESPUESTA FINAL:\")\n",
    "        print(\"=\"*50)\n",
    "        print(response.get('output', 'No se encontr√≥ respuesta'))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error al ejecutar agente: {e}\")\n",
    "        print(\"\\nNOTA: Para usar agentes con b√∫squeda web necesitas:\")\n",
    "        print(\"1. Una clave API de SerpAPI (https://serpapi.com/)\")\n",
    "        print(\"2. Instalar: pip install google-search-results\")\n",
    "        print(\"3. Configurar SERP_API_KEY en tu archivo .env\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è El agente no est√° disponible. Verifica la celda anterior para m√°s detalles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c30d2",
   "metadata": {},
   "source": [
    "![Wild Belle](data/WildBelle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4b368",
   "metadata": {},
   "source": [
    "üéµDisfrutaüéµ\n",
    "https://open.spotify.com/track/1eREJIBdqeCcqNCB1pbz7w?si=c014293b63c7478c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiagente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
