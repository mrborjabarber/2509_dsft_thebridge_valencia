{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro Aprendizaje por Refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considera el escenario de enseñarle nuevos trucos a un perro. El perro no entiende nuestro lenguaje, así que no podemos decirle qué hacer. En cambio, seguimos una estrategia diferente. Emulamos una situación (o una señal), y el perro intenta responder de muchas maneras diferentes. Si la respuesta del perro es la deseada, lo recompensamos con golosinas. Ahora, adivina qué, la próxima vez que el perro se enfrenta a la misma situación, ejecuta una acción similar con aún más entusiasmo esperando más comida. Esto es como aprender \"qué hacer\" a partir de experiencias positivas. De manera similar, los perros tienden a aprender qué no hacer cuando se enfrentan a experiencias negativas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entendiendo cómo funciona el Aprendizaje por Refuerzo\n",
    "\n",
    "En un sentido más amplio, así es como funciona el Aprendizaje por Refuerzo:\n",
    "\n",
    "* Tu perro es un \"agente\" que está expuesto al entorno. El entorno podría ser tu casa, contigo.\n",
    "* Las situaciones que encuentran son análogas a un estado. Un ejemplo de un estado podría ser tu perro de pie y tú usando una palabra específica con cierto tono en tu sala de estar.\n",
    "* Nuestros agentes reaccionan realizando una acción para pasar de un \"estado\" a otro \"estado\"; por ejemplo, tu perro pasa de estar de pie a sentarse.\n",
    "* Después de la transición, pueden recibir una recompensa o una penalización a cambio. ¡Les das una golosina! O un \"No\" como penalización.\n",
    "* La política es la estrategia de elegir una acción dada un estado en espera de mejores resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Aprendizaje por Refuerzo se encuentra entre el espectro del Aprendizaje Supervisado y el No Supervisado, y hay algunas cosas importantes que tener en cuenta:\n",
    "\n",
    "#### No siempre es beneficioso ser codicioso\n",
    "\n",
    "**Ser codicioso no siempre funciona:**\n",
    "\n",
    "* Hay cosas que son fáciles de hacer para obtener una gratificación instantánea, y hay cosas que proporcionan recompensas a largo plazo. El objetivo no es ser codicioso buscando las recompensas inmediatas, sino optimizar las recompensas máximas durante todo el entrenamiento.\n",
    "\n",
    "#### La secuencia importa en el Aprendizaje por Refuerzo\n",
    "\n",
    "**La secuencia importa en el Aprendizaje por Refuerzo:**\n",
    "\n",
    "* La recompensa del agente no solo depende del estado actual, sino de toda la historia de estados. A diferencia del aprendizaje supervisado y no supervisado, el tiempo es importante aquí.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El proceso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Reinforcement-Learning-Animation.gif\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cierto sentido, el Aprendizaje por Refuerzo es la ciencia de tomar decisiones óptimas utilizando experiencias. Desglosándolo, el proceso de Aprendizaje por Refuerzo involucra estos pasos simples:\n",
    "\n",
    "1. Observación del entorno.\n",
    "2. Decidir cómo actuar utilizando alguna estrategia.\n",
    "3. Actuar en consecuencia.\n",
    "4. Recibir una recompensa o penalización.\n",
    "5. Aprender de las experiencias y refinar nuestra estrategia.\n",
    "6. Iterar hasta encontrar una estrategia óptima.\n",
    "\n",
    "Ahora vamos a entender el Aprendizaje por Refuerzo desarrollando un agente para aprender a jugar un juego automáticamente por sí mismo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comenzando con Gymnasium\n",
    "\n",
    "Gymnasium es un conjunto de herramientas para desarrollar y comparar algoritmos de aprendizaje por refuerzo. Es la versión mantenida y moderna de OpenAI Gym, compatible con NumPy 2.0 y otras dependencias actuales.\n",
    "\n",
    "La biblioteca Gymnasium es una colección de problemas de prueba, o entornos, que puedes usar para desarrollar tus algoritmos de aprendizaje por refuerzo. Estos entornos tienen una interfaz compartida, lo que te permite escribir algoritmos generales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# INSTALACIÓN DE LIBRERÍAS NECESARIAS\n",
    "# =============================================================================\n",
    "\n",
    "# Gymnasium: Versión moderna y mantenida de OpenAI Gym\n",
    "# Compatible con NumPy 2.0 y Python moderno\n",
    "# pip install gymnasium\n",
    "\n",
    "# Pygame: Para renderizar gráficamente los entornos\n",
    "# pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entornos\n",
    "\n",
    "Aquí tienes un ejemplo mínimo para comenzar a ejecutar algo. Esto ejecutará una instancia del entorno CartPole-v0 durante 1000 pasos de tiempo, representando el entorno en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Consiste en equilibrar un poste (péndulo) montado en la parte superior de un carrito móvil:\n",
    " * El **objetivo** es mantener el poste en posición vertical mientras el carrito se mueve hacia adelante y hacia atrás en una pista. \n",
    " * El **agente** (o jugador) tiene dos acciones disponibles en cada paso de tiempo: empujar el carrito hacia la izquierda o hacia la derecha. \n",
    " * El **desafío** radica en tomar decisiones adecuadas para evitar que el poste caiga mientras se mueve el carrito. \n",
    " \n",
    " Este problema es un ejemplo comúnmente utilizado para probar algoritmos de aprendizaje por refuerzo debido a su simplicidad y naturaleza desafiante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentación oficial de Gymnasium:\n",
    "\n",
    "https://gymnasium.farama.org/api/env/\n",
    "\n",
    "**Nota:** Gymnasium es el sucesor mantenido de OpenAI Gym, compatible con NumPy 2.0 y versiones modernas de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEMOSTRACIÓN BÁSICA: ENTORNO MOUNTAINCAR CON ACCIONES ALEATORIAS\n",
    "# =============================================================================\n",
    "\n",
    "# Importamos Gymnasium (la versión moderna de Gym)\n",
    "import gymnasium as gym\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Suprimimos advertencias para una salida más limpia\n",
    "\n",
    "# import time  # Opcional: para añadir pausas entre frames\n",
    "\n",
    "# Creamos el entorno MountainCar-v0\n",
    "# - MountainCar: Un coche debe subir una montaña usando impulso\n",
    "# - render_mode=\"human\": Muestra la ventana gráfica en tiempo real\n",
    "env = gym.make('MountainCar-v0', render_mode=\"human\")\n",
    "\n",
    "# Reseteamos el entorno al estado inicial\n",
    "# En Gymnasium, reset() devuelve (observación, info)\n",
    "state, info = env.reset()\n",
    "\n",
    "# Ejecutamos 300 pasos de simulación\n",
    "for i in range(300):\n",
    "    # Renderizamos el frame actual (ya está en modo \"human\", así que se muestra automáticamente)\n",
    "    env.render()\n",
    "    \n",
    "    # time.sleep(0.1)  # Descomentar para ralentizar la animación\n",
    "    \n",
    "    # Tomamos una acción ALEATORIA del espacio de acciones\n",
    "    # env.action_space.sample() devuelve un número aleatorio entre 0 y 2\n",
    "    # 0 = acelerar izquierda, 1 = no acelerar, 2 = acelerar derecha\n",
    "    action = env.action_space.sample()\n",
    "    \n",
    "    # Ejecutamos la acción en el entorno\n",
    "    # En Gymnasium, step() devuelve 5 valores:\n",
    "    # - observation: nuevo estado\n",
    "    # - reward: recompensa obtenida\n",
    "    # - terminated: si el episodio terminó exitosamente\n",
    "    # - truncated: si el episodio fue truncado por límite de tiempo\n",
    "    # - info: información adicional\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Si el episodio terminó o fue truncado, paramos\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "# Cerramos el entorno y liberamos recursos\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentando con diferentes entornos\n",
    "\n",
    "Si deseas ver otros entornos en acción, intenta reemplazar `MountainCar-v0` con otros entornos como:\n",
    "- `CartPole-v1`: Equilibrar un péndulo sobre un carro móvil\n",
    "- `LunarLander-v2`: Aterrizar una nave espacial suavemente\n",
    "- `Acrobot-v1`: Balancear un robot de dos eslabones\n",
    "\n",
    "Todos los entornos descienden de la clase base `Env` de Gymnasium y comparten la misma interfaz.\n",
    "\n",
    "**Nota sobre Gymnasium:** Si encuentras código antiguo que usa `import gym`, simplemente reemplázalo por `import gymnasium as gym` en la mayoría de los casos. Gymnasium es compatible con la API de Gym pero está activamente mantenido y soporta dependencias modernas como NumPy 2.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones\n",
    "\n",
    "Si queremos hacer algo mejor que tomar acciones aleatorias en cada paso, probablemente sería bueno saber realmente qué están haciendo nuestras acciones en el entorno.\n",
    "\n",
    "La función `step` del entorno devuelve exactamente lo que necesitamos. De hecho, `step` devuelve cinco valores en Gymnasium. Estos son:\n",
    "\n",
    "* `observation` (object): un objeto específico del entorno que representa tu observación del entorno. Por ejemplo, datos de píxeles de una cámara, ángulos de articulación y velocidades de articulación de un robot, o el estado del tablero en un juego de mesa.\n",
    "* `reward` (float): cantidad de recompensa lograda por la acción anterior. La escala varía entre entornos, pero el objetivo es siempre aumentar tu recompensa total.\n",
    "* `terminated` (bool): si el episodio ha terminado exitosamente (el agente alcanzó el objetivo). La mayoría (pero no todas) de las tareas están divididas en episodios bien definidos, y `terminated` siendo True indica que el episodio ha terminado de forma natural.\n",
    "* `truncated` (bool): True si el episodio se trunca debido a un límite de tiempo o una razón que no está definida como parte de la tarea MDP.\n",
    "* `info` (dict): información de diagnóstico útil para la depuración. A veces puede ser útil para el aprendizaje (por ejemplo, podría contener las probabilidades crudas detrás del último cambio de estado del entorno). Sin embargo, no se permite usar esto para el aprendizaje en las evaluaciones oficiales de tu agente.\n",
    "\n",
    "Esto es simplemente una implementación del clásico \"bucle agente-entorno\". En cada paso de tiempo, el agente elige una acción, y el entorno devuelve una observación y una recompensa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREACIÓN DEL ENTORNO MOUNTAINCAR (SIN VISUALIZACIÓN)\n",
    "# =============================================================================\n",
    "\n",
    "# Creamos el entorno sin render_mode para trabajar con él más rápido\n",
    "# Esto es útil cuando solo queremos analizar el espacio de acciones y estados\n",
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INSPECCIÓN DEL ESPACIO DE ACCIONES\n",
    "# =============================================================================\n",
    "\n",
    "# env.action_space nos dice qué acciones puede tomar el agente\n",
    "# Discrete(3) significa que hay 3 acciones discretas posibles: 0, 1, 2\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de Acciones\n",
    "\n",
    "Hay 3 acciones discretas determinísticas:\n",
    "\n",
    "| Num | Observación          | Valor | Unidad       |\n",
    "|-----|----------------------|-------|--------------|\n",
    "| 0   | Acelerar a la izquierda | Inf   | posición (m) |\n",
    "| 1   | No acelerar              | Inf   | posición (m) |\n",
    "| 2   | Acelerar a la derecha    | Inf   | posición (m) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INSPECCIÓN DEL ESPACIO DE OBSERVACIONES\n",
    "# =============================================================================\n",
    "\n",
    "# env.observation_space.shape nos dice las dimensiones del estado\n",
    "# (2,) significa que el estado es un vector de 2 elementos:\n",
    "# - Elemento 0: Posición del coche en el eje X\n",
    "# - Elemento 1: Velocidad del coche\n",
    "print(env.observation_space.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de Observación\n",
    "\n",
    "La observación es un ndarray con forma (2,), donde los elementos corresponden a lo siguiente:\n",
    "\n",
    "| Num | Observación                    | Mínimo | Máximo | Unidad        |\n",
    "|-----|--------------------------------|--------|--------|---------------|\n",
    "| 0   | posición del coche a lo largo del eje x | -Inf   | Inf    | posición (m) |\n",
    "| 1   | velocidad del coche             | -Inf   | Inf    | velocidad (m) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEMOSTRACIÓN: SELECCIÓN ALEATORIA DE ELEMENTOS\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# np.random.choice() selecciona aleatoriamente un elemento de un array\n",
    "# Esto es útil cuando quieres elegir entre varias opciones\n",
    "np.random.choice(np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intento 0\n",
      "Accion 0\n",
      "Observación [-0.47985008  0.        ]\n",
      "Accion 10\n",
      "Observación [-0.50268555 -0.00379564]\n",
      "Accion 20\n",
      "Observación [-0.54379946 -0.00186527]\n",
      "Accion 30\n",
      "Observación [-5.352816e-01  5.029060e-04]\n",
      "Accion 40\n",
      "Observación [-0.5236577   0.00205248]\n",
      "Accion 50\n",
      "Observación [-5.0771111e-01  4.1508456e-04]\n",
      "Accion 60\n",
      "Observación [-0.50030196  0.0011197 ]\n",
      "Accion 70\n",
      "Observación [-0.49893996 -0.0008852 ]\n",
      "Accion 80\n",
      "Observación [-0.5294612  -0.00496277]\n",
      "Accion 90\n",
      "Observación [-0.5606577  -0.00228768]\n",
      "Intento 1\n",
      "Accion 0\n",
      "Observación [-0.5880612  0.       ]\n",
      "Accion 10\n",
      "Observación [-0.568869    0.00218481]\n",
      "Accion 20\n",
      "Observación [-0.49628976  0.00951739]\n",
      "Accion 30\n",
      "Observación [-0.4364022   0.00313993]\n",
      "Accion 40\n",
      "Observación [-0.45278108 -0.00421663]\n",
      "Accion 50\n",
      "Observación [-0.5052383 -0.0068862]\n",
      "Accion 60\n",
      "Observación [-0.58757293 -0.00525783]\n",
      "Accion 70\n",
      "Observación [-0.6142896   0.00292763]\n",
      "Accion 80\n",
      "Observación [-0.5483299   0.00657334]\n",
      "Accion 90\n",
      "Observación [-0.45727062  0.0116558 ]\n",
      "Intento 2\n",
      "Accion 0\n",
      "Observación [-0.5986761  0.       ]\n",
      "Accion 10\n",
      "Observación [-0.5733774   0.00510003]\n",
      "Accion 20\n",
      "Observación [-0.5254152   0.00614087]\n",
      "Accion 30\n",
      "Observación [-0.49018052  0.00077069]\n",
      "Accion 40\n",
      "Observación [-0.5123816  -0.00314435]\n",
      "Accion 50\n",
      "Observación [-0.5705503  -0.00625105]\n",
      "Accion 60\n",
      "Observación [-0.6403491  -0.00543538]\n",
      "Accion 70\n",
      "Observación [-6.8049169e-01 -2.9004362e-04]\n",
      "Accion 80\n",
      "Observación [-0.63034844  0.01119857]\n",
      "Accion 90\n",
      "Observación [-0.4810161   0.01531835]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BUCLE COMPLETO DE ENTRENAMIENTO: 3 EPISODIOS CON ACCIONES ALEATORIAS\n",
    "# =============================================================================\n",
    "\n",
    "# Creamos el entorno con visualización humana\n",
    "env = gym.make('MountainCar-v0', render_mode='human')\n",
    "\n",
    "# Ejecutamos 3 episodios completos\n",
    "for i_episode in range(3):\n",
    "    print(\"Intento\", i_episode)\n",
    "    \n",
    "    # Reseteamos el entorno al inicio de cada episodio\n",
    "    # En Gymnasium, reset() devuelve (observación, info)\n",
    "    observation, info = env.reset()\n",
    "    \n",
    "    # Ejecutamos hasta 100 pasos por episodio\n",
    "    for t in range(100):\n",
    "        # Cada 10 pasos, imprimimos información de debug\n",
    "        if t % 10 == 0:\n",
    "            print(\"Accion\", t)\n",
    "            print(\"Observación\", observation)\n",
    "        \n",
    "        # Renderizamos el frame actual\n",
    "        # (en render_mode='human' esto se hace automáticamente)\n",
    "        env.render()\n",
    "        \n",
    "        # Elegimos una acción ALEATORIA del espacio de acciones\n",
    "        # 0 = acelerar izquierda, 1 = no acelerar, 2 = acelerar derecha\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Ejecutamos la acción en el entorno\n",
    "        # step() devuelve 5 valores en Gymnasium:\n",
    "        # - observation: nuevo estado (array con [posición, velocidad])\n",
    "        # - reward: recompensa obtenida (normalmente -1 por cada paso)\n",
    "        # - terminated: True si alcanzamos el objetivo (llegar a la bandera)\n",
    "        # - truncated: True si se acabó el tiempo máximo\n",
    "        # - info: diccionario con información adicional\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Si el episodio terminó (éxito o truncado), salimos del bucle\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "\n",
    "# Cerramos el entorno al finalizar\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
