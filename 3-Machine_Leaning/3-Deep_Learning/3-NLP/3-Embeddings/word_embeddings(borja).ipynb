{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IZBRUaiBBEpa"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "cellView": "form",
    "id": "YS3NA-i6nAFC"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN5USFEIIK3"
   },
   "source": "# Incrustaciones de Palabras (Word Embeddings)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Aojnnc7sXrab"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6mJg1g3apaz"
   },
   "source": "Este tutorial contiene una introducción a las incrustaciones de palabras (word embeddings). Entrenarás tus propias incrustaciones de palabras usando un modelo simple de Keras para una tarea de clasificación de sentimientos, y luego las visualizarás en el [Embedding Projector](http://projector.tensorflow.org).\n\n## Representando texto como números\n\nLos modelos de aprendizaje automático toman vectores (arrays de números) como entrada. Cuando trabajas con texto, lo primero que debes hacer es idear una estrategia para convertir cadenas de texto en números (o \"vectorizar\" el texto) antes de alimentarlo al modelo. En esta sección, verás tres estrategias para hacerlo.\n\n### Codificaciones One-Hot\n\nComo primera idea, podrías codificar en \"one-hot\" cada palabra en tu vocabulario. Considera la oración \"The cat sat on the mat\". El vocabulario (o palabras únicas) en esta oración es (cat, mat, on, sat, the). Para representar cada palabra, crearías un vector de ceros con longitud igual al vocabulario, y luego colocarías un uno en el índice que corresponde a la palabra.\n\nPara crear un vector que contenga la codificación de la oración, podrías concatenar los vectores one-hot de cada palabra.\n\n**Punto clave: Este enfoque es ineficiente.** Un vector codificado en one-hot es disperso (sparse, es decir, la mayoría de los índices son cero). Imagina que tienes 10,000 palabras en el vocabulario. Para codificar en one-hot cada palabra, crearías un vector donde el 99.99% de los elementos son cero.\n\n### Codificar cada palabra con un número único\n\nUn segundo enfoque que podrías probar es codificar cada palabra usando un **número único**. Continuando con el ejemplo anterior, podrías asignar 1 a \"cat\", 2 a \"mat\", y así sucesivamente. Luego podrías codificar la oración \"The cat sat on the mat\" como un vector denso como [5, 1, 4, 3, 5, 2]. Este enfoque es eficiente. En lugar de un vector disperso, ahora tienes uno denso (donde todos los elementos están llenos).\n\nSin embargo, hay dos desventajas en este enfoque:\n\n* La codificación entera es arbitraria (no captura ninguna relación entre palabras).\n\n* Una codificación entera puede ser desafiante para que un modelo la interprete. Un clasificador lineal, por ejemplo, aprende un solo peso para cada característica. Debido a que no hay relación entre la similitud de dos palabras y la similitud de sus codificaciones, esta combinación característica-peso no es significativa.\n\n### Incrustaciones de palabras (Word embeddings)\n\nLas incrustaciones de palabras nos dan una manera de usar una representación eficiente y densa en la que palabras similares tienen una codificación similar. Es importante destacar que no tienes que especificar esta codificación manualmente. Una incrustación es un vector denso de valores de punto flotante (la longitud del vector es un parámetro que especificas). En lugar de especificar los valores para la incrustación manualmente, son parámetros entrenables (pesos aprendidos por el modelo durante el entrenamiento, de la misma manera que un modelo aprende pesos para una capa densa). Es común ver incrustaciones de palabras de 8 dimensiones (para conjuntos de datos pequeños), hasta 1024 dimensiones cuando se trabaja con conjuntos de datos grandes. Una incrustación de dimensión más alta puede capturar relaciones más sutiles entre palabras, pero requiere más datos para aprender."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SZUQErGewZxE"
   },
   "source": "## Configuración Inicial"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": "# ===== IMPORTACIÓN DE LIBRERÍAS NECESARIAS =====\n# Estas son todas las bibliotecas que usaremos en este tutorial\n\nimport io  # Para operaciones de entrada/salida de archivos\nimport os  # Para operaciones del sistema operativo (rutas, directorios, etc.)\nimport re  # Para expresiones regulares (limpieza de texto)\nimport shutil  # Para operaciones de archivos de alto nivel\nimport string  # Para constantes de cadenas (puntuación, letras, etc.)\nimport tensorflow as tf  # Framework principal de deep learning\n\n# Importamos clases específicas de Keras (la API de alto nivel de TensorFlow)\nfrom tensorflow.keras import Sequential  # Para crear modelos secuenciales capa por capa\nfrom tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D  # Capas del modelo\n# Dense: capa totalmente conectada\n# Embedding: capa para incrustaciones de palabras\n# GlobalAveragePooling1D: capa para promediar secuencias\n\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization  # Para vectorizar texto"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": "### Descargar el Dataset IMDb\nUsarás el [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) a lo largo del tutorial. Entrenarás un modelo clasificador de sentimientos sobre este conjunto de datos y en el proceso aprenderás embeddings desde cero. Para leer más sobre cómo cargar un conjunto de datos desde cero, consulta el [Tutorial de Carga de Texto](../load_data/text.ipynb)."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6yROZNKvbd"
   },
   "source": "Echa un vistazo al directorio `train/`. Tiene carpetas `pos` y `neg` con reseñas de películas etiquetadas como positivas y negativas respectivamente. Usarás reseñas de las carpetas `pos` y `neg` para entrenar un modelo de clasificación binaria."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-iOHJGN6SDu"
   },
   "outputs": [],
   "source": "# ===== DEFINIR LA RUTA DEL DATASET =====\n# Aquí establecemos la ruta donde se encuentra nuestro conjunto de datos de reseñas de IMDb\n# os.getcwd() obtiene el directorio de trabajo actual\n# Añadimos \"\\\\data\\\\\" para acceder a la carpeta de datos\ndataset_dir = os.getcwd()+ \"\\\\data\\\\\""
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oFoJjiEyJz9u"
   },
   "source": "A continuación, crea un `tf.data.Dataset` usando `tf.keras.preprocessing.text_dataset_from_directory`. Puedes leer más sobre el uso de esta utilidad en este [tutorial de clasificación de texto](https://www.tensorflow.org/tutorials/keras/text_classification). \n\nUsa el directorio `train` para crear tanto el conjunto de datos de entrenamiento como el de validación con una división del 20% para validación."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItYD3TLkCOP1"
   },
   "outputs": [],
   "source": "# ===== CREAR LOS CONJUNTOS DE DATOS DE ENTRENAMIENTO Y VALIDACIÓN =====\n\n# Tamaño del lote: número de ejemplos que se procesan juntos en cada iteración\nbatch_size = 1024\n\n# Semilla para reproducibilidad: esto asegura que la división de datos sea la misma cada vez\nseed = 123\n\n# Crear el conjunto de datos de ENTRENAMIENTO (80% de los datos)\n# text_dataset_from_directory lee automáticamente las carpetas y etiqueta los datos\n# Las carpetas \"pos\" y \"neg\" se usan como etiquetas (1 y 0)\ntrain_ds = tf.keras.preprocessing.text_dataset_from_directory(\n    dataset_dir,  # Directorio donde están los datos\n    batch_size = batch_size,  # Cuántos ejemplos procesar a la vez\n    validation_split = 0.2,  # Reservar 20% para validación\n    subset = 'training',  # Este es el conjunto de entrenamiento\n    seed = seed  # Semilla para reproducibilidad\n)\n\n# Crear el conjunto de datos de VALIDACIÓN (20% de los datos)\n# La validación se usa para evaluar el modelo durante el entrenamiento\n# NO se usa para entrenar, solo para monitorear el rendimiento\nval_ds = tf.keras.preprocessing.text_dataset_from_directory(\n    dataset_dir,\n    batch_size = batch_size,\n    validation_split = 0.2,  # Mismo 20% que antes\n    subset = 'validation',  # Este es el conjunto de validación\n    seed = seed  # Misma semilla para consistencia\n)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eHa6cq0-Ym0g"
   },
   "source": "Echa un vistazo a algunas reseñas de películas y sus etiquetas `(1: positivo, 0: negativo)` del conjunto de datos de entrenamiento."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTCbSkvkYmTT"
   },
   "outputs": [],
   "source": "# ===== EXPLORAR ALGUNOS EJEMPLOS DEL CONJUNTO DE DATOS =====\n\n# Iteramos sobre el conjunto de entrenamiento\n# take(1) significa que solo tomamos 1 lote (batch) de datos\nfor text_batch, label_batch in train_ds.take(1):\n  # Dentro de este lote, mostramos solo los primeros 5 ejemplos\n  for i in range(5): \n    # Imprimimos la etiqueta (0=negativo, 1=positivo) y el texto de la reseña\n    # .numpy() convierte el tensor de TensorFlow a un array de NumPy para mostrarlo\n    print(label_batch[i].numpy(), text_batch.numpy()[i])"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBazMiVQkj1"
   },
   "source": "## Usando la Capa Embedding\n\nKeras facilita el uso de incrustaciones de palabras. Echa un vistazo a la capa [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding).\n\nLa capa Embedding puede entenderse como una tabla de búsqueda que mapea desde índices enteros (que representan palabras específicas) a vectores densos (sus incrustaciones). La dimensionalidad (o anchura) de la incrustación es un parámetro con el que puedes experimentar para ver qué funciona bien para tu problema, de la misma manera que experimentarías con el número de neuronas en una capa Dense."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== CÁLCULO DE LA DIMENSIÓN DE EMBEDDING =====\n# Esta es una heurística común: la raíz cuarta del tamaño del vocabulario\n# Para un vocabulario de 1000 palabras: 1000^(1/4) ≈ 5.6\n# Esto nos da una guía de cuántas dimensiones usar para nuestros embeddings\n1000**(1/4)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OjxLVrMvWUE"
   },
   "outputs": [],
   "source": "# ===== CREAR UNA CAPA DE EMBEDDING DE EJEMPLO =====\n# Creamos una capa de embedding con:\n# - 1000 palabras posibles en el vocabulario (índices del 0 al 999)\n# - 5 dimensiones para cada vector de embedding\n# Esto crea una matriz de pesos de tamaño (1000, 5)\n# Cada palabra se representará como un vector de 5 números\nembedding_layer = tf.keras.layers.Embedding(1000, 5)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2dKKV1L2Rk7e"
   },
   "source": "Cuando creas una capa Embedding, los pesos para la incrustación se inicializan aleatoriamente (al igual que cualquier otra capa). Durante el entrenamiento, se ajustan gradualmente mediante retropropagación. Una vez entrenados, los embeddings de palabras aprendidos codificarán aproximadamente similitudes entre palabras (tal como fueron aprendidas para el problema específico en el que tu modelo está entrenado).\n\nSi pasas un entero a una capa de embedding, el resultado reemplaza cada entero con el vector de la tabla de embeddings:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YUjPgP7w0PO"
   },
   "outputs": [],
   "source": "# ===== EJEMPLO DE USO DE LA CAPA EMBEDDING =====\n# Pasamos índices de palabras [0, 1, 2, 3, 4, 999] a la capa\n# La capa busca cada índice y devuelve su vector de embedding correspondiente\n# Por ejemplo, el índice 0 se convierte en un vector de 5 números\n# El resultado es una matriz de forma (6, 5): 6 palabras, cada una con 5 dimensiones\nresult = embedding_layer(tf.constant([0,1,2,3,4,999]))\nresult.numpy()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== INSPECCIONAR LOS PESOS DE LA CAPA EMBEDDING =====\n# Mostramos la forma de la matriz de pesos (embeddings)\n# Debe ser (1000, 5) = 1000 palabras × 5 dimensiones\nprint(embedding_layer.embeddings.shape)\n\n# Mostramos la matriz completa de embeddings\n# Cada fila es el vector de embedding de una palabra\n# Estos valores son aleatorios al inicio, pero se entrenan con el modelo\nembedding_layer.embeddings"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O4PC4QzsxTGx"
   },
   "source": "Para problemas de texto o secuencias, la capa Embedding toma un tensor 2D de enteros, de forma `(muestras, longitud_secuencia)`, donde cada entrada es una secuencia de enteros. Puede incrustar secuencias de longitudes variables. Podrías alimentar a la capa de embedding anterior lotes con formas `(32, 10)` (lote de 32 secuencias de longitud 10) o `(64, 15)` (lote de 64 secuencias de longitud 15).\n\nEl tensor devuelto tiene un eje más que la entrada, los vectores de embedding se alinean a lo largo del nuevo último eje. Pásale una entrada de lote `(2, 3)` y la salida es `(2, 3, N)`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwSYepRjyRGy"
   },
   "outputs": [],
   "source": "# ===== EMBEDDING CON MÚLTIPLES SECUENCIAS =====\n# Creamos un tensor 2D con:\n# - Primera fila (secuencia): [1, 2, 999]\n# - Segunda fila (secuencia): [3, 4, 5]\n# Esto representa 2 secuencias de 3 palabras cada una\nresult = embedding_layer(tf.constant([[1, 2, 999],\n                                      [3, 4, 5]]))\n\n# La forma de salida es (2, 3, 5):\n# - 2 secuencias\n# - 3 palabras por secuencia  \n# - 5 dimensiones por embedding de palabra\nprint(result.shape)\n\n# Mostramos los vectores de embedding resultantes\nresult.numpy()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQp2N92yOyB"
   },
   "source": "Cuando se le da un lote de secuencias como entrada, una capa de embedding devuelve un tensor de punto flotante 3D, de forma `(muestras, longitud_secuencia, dimensionalidad_embedding)`. Para convertir de esta secuencia de longitud variable a una representación fija, hay una variedad de enfoques estándar. Podrías usar una capa RNN, Attention o pooling antes de pasarla a una capa Dense. Este tutorial usa pooling porque es lo más simple. El tutorial [Clasificación de Texto con RNN](text_classification_rnn.ipynb) es un buen próximo paso."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aGicgV5qT0wh"
   },
   "source": "## Preprocesamiento de Texto"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N6NZSqIIoU0Y"
   },
   "source": "A continuación, define los pasos de preprocesamiento del conjunto de datos requeridos para tu modelo de clasificación de sentimientos. Inicializa una capa TextVectorization con los parámetros deseados para vectorizar las reseñas de películas. Puedes aprender más sobre el uso de esta capa en el tutorial de [Clasificación de Texto](https://www.tensorflow.org/tutorials/keras/text_classification)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [],
   "source": "# ===== FUNCIÓN DE ESTANDARIZACIÓN PERSONALIZADA =====\n# Esta función limpia y normaliza el texto antes de procesarlo\n\ndef custom_standardization(input_data):\n  \"\"\"\n  Preprocesa el texto de entrada realizando tres pasos:\n  1. Convierte todo a minúsculas\n  2. Elimina etiquetas HTML '<br />'\n  3. Elimina signos de puntuación\n  \"\"\"\n  # Paso 1: Convertir todo el texto a minúsculas\n  # Esto asegura que \"Película\" y \"película\" se traten como la misma palabra\n  lowercase = tf.strings.lower(input_data)\n  \n  # Paso 2: Eliminar las etiquetas de salto de línea HTML '<br />'\n  # Las reseñas de IMDb a menudo contienen estas etiquetas\n  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n  \n  # Paso 3: Eliminar todos los signos de puntuación (!,.:;? etc.)\n  # re.escape convierte caracteres especiales en literales para regex\n  # string.punctuation contiene todos los signos de puntuación\n  return tf.strings.regex_replace(stripped_html,\n                                  '[%s]' % re.escape(string.punctuation), '')\n\n\n# ===== CONFIGURACIÓN DE PARÁMETROS DE VECTORIZACIÓN =====\n# Tamaño del vocabulario: solo las 10,000 palabras más frecuentes\nvocab_size = 10000\n\n# Longitud de secuencia: todas las reseñas se cortarán/rellenarán a 100 palabras\nsequence_length = 100\n\n# ===== CREAR LA CAPA DE VECTORIZACIÓN =====\n# Esta capa convierte texto en secuencias de números\nvectorize_layer = TextVectorization(\n    standardize = custom_standardization,  # Usar nuestra función de limpieza\n    max_tokens = vocab_size,  # Mantener solo las 10,000 palabras más comunes\n    output_mode = 'int',  # Salida como enteros (índices de palabras)\n    output_sequence_length = sequence_length  # Todas las secuencias tendrán longitud 100\n)\n\n# ===== ADAPTAR EL VECTORIZADOR AL CONJUNTO DE DATOS =====\n# Crear un dataset solo de texto (sin etiquetas) para construir el vocabulario\n# map(lambda x, y: x) extrae solo el texto, descartando las etiquetas (y)\ntext_ds = train_ds.map(lambda x, y: x)\n\n# adapt() analiza todos los textos y construye el vocabulario\n# Cuenta la frecuencia de palabras y crea un diccionario palabra->índice\nvectorize_layer.adapt(text_ds)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_wLIiWO8Z"
   },
   "source": "## Crear un Modelo de Clasificación\n\nUsa la [API Sequential de Keras](https://www.tensorflow.org/guide/keras/sequential_model) para definir el modelo de clasificación de sentimientos. En este caso es un modelo de estilo \"Bolsa Continua de Palabras\" (Continuous bag of words).\n\n* La capa [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) transforma cadenas de texto en índices de vocabulario. Ya has inicializado `vectorize_layer` como una capa TextVectorization y construido su vocabulario llamando `adapt` en `text_ds`. Ahora vectorize_layer puede usarse como la primera capa de tu modelo de clasificación de extremo a extremo, alimentando cadenas transformadas a la capa Embedding.\n\n* La capa [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) toma el vocabulario codificado en enteros y busca el vector de embedding para cada índice de palabra. Estos vectores se aprenden a medida que el modelo se entrena. Los vectores añaden una dimensión al array de salida. Las dimensiones resultantes son: `(lote, secuencia, embedding)`.\n\n* La capa [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) devuelve un vector de salida de longitud fija para cada ejemplo promediando sobre la dimensión de la secuencia. Esto permite que el modelo maneje entradas de longitud variable, de la manera más simple posible.\n\n* El vector de salida de longitud fija se canaliza a través de una capa totalmente conectada ([`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) con 16 unidades ocultas.\n\n* La última capa está densamente conectada con un solo nodo de salida.\n\nPrecaución: Este modelo no usa enmascaramiento (masking), por lo que el relleno de ceros se usa como parte de la entrada y, por lo tanto, la longitud del relleno puede afectar la salida. Para solucionar esto, consulta la [guía de enmascaramiento y relleno](https://www.tensorflow.org/guide/keras/masking_and_padding)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== CALCULAR DIMENSIÓN DE EMBEDDING ÓPTIMA =====\n# Usando la heurística: raíz cuarta del tamaño del vocabulario\n# Para 10,000 palabras: 10000^(1/4) = 10\n# Esto nos sugiere usar embeddings de aproximadamente 10-16 dimensiones\n10000**(1/4)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [],
   "source": "# ===== DEFINIR LA ARQUITECTURA DEL MODELO =====\n\n# Dimensión de los vectores de embedding (cuántos números representan cada palabra)\nembedding_dim = 16\n\n# Tamaño del vocabulario (debe coincidir con el vectorize_layer)\nvocab_size = 10000\n\n# Crear el modelo secuencial (las capas se apilan una tras otra)\nmodel = Sequential([\n    # CAPA 1: Vectorización de texto\n    # Convierte texto crudo (\"me gusta esta película\") en números [45, 234, 12, 567]\n    vectorize_layer,\n    \n    # CAPA 2: Embedding\n    # Convierte cada número en un vector de 16 dimensiones\n    # Entrada: (batch_size, 100) números enteros\n    # Salida: (batch_size, 100, 16) vectores de embeddings\n    Embedding(vocab_size, embedding_dim, name='embedding'),\n    \n    # CAPA 3: Global Average Pooling\n    # Promedia todos los embeddings de palabras en una sola representación\n    # Entrada: (batch_size, 100, 16)\n    # Salida: (batch_size, 16) - un solo vector por reseña\n    GlobalAveragePooling1D(),\n    \n    # CAPA 4: Capa densa oculta con activación ReLU\n    # 16 neuronas que aprenden patrones complejos\n    # Salida: (batch_size, 16)\n    Dense(16, activation='relu'),\n    \n    # CAPA 5: Capa de salida con activación sigmoide\n    # 1 neurona que produce una probabilidad entre 0 y 1\n    # 0 = sentimiento negativo, 1 = sentimiento positivo\n    # Salida: (batch_size, 1)\n    Dense(1, activation='sigmoid')\n])"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JjLNgKO7W2fe"
   },
   "source": "## Compilar y Entrenar el Modelo"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jpX9etB6IOQd"
   },
   "source": "Usarás [TensorBoard](https://www.tensorflow.org/tensorboard) para visualizar métricas incluyendo pérdida y precisión. Crea un `tf.keras.callbacks.TensorBoard`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4Hg3IHFt4Px"
   },
   "outputs": [],
   "source": "# ===== COMPILAR EL MODELO =====\n# Configuramos cómo el modelo aprenderá\n\nmodel.compile(\n    # Optimizador: Adam es un algoritmo de optimización adaptativo eficiente\n    # Ajusta automáticamente la tasa de aprendizaje durante el entrenamiento\n    optimizer='adam',\n    \n    # Función de pérdida: Binary Crossentropy para clasificación binaria\n    # Mide qué tan lejos están las predicciones de las etiquetas reales\n    # Valores más bajos = mejores predicciones\n    loss = 'binary_crossentropy',\n    \n    # Métricas: Qué medir durante el entrenamiento\n    # Accuracy = porcentaje de predicciones correctas\n    metrics = ['accuracy']\n)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrKAKAKIbuH"
   },
   "source": "Compila y entrena el modelo usando el optimizador `Adam` y la pérdida `BinaryCrossentropy`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCUgdP69Wzix"
   },
   "outputs": [],
   "source": "# ===== ENTRENAR EL MODELO =====\n# Aquí es donde el aprendizaje realmente ocurre\n\nmodel.fit(\n    # Datos de entrenamiento: el modelo aprende de estos ejemplos\n    train_ds,\n    \n    # Datos de validación: usados para evaluar el modelo después de cada época\n    # El modelo NO aprende de estos datos, solo los usa para medir rendimiento\n    validation_data = val_ds,\n    \n    # Épocas: número de veces que el modelo verá TODO el conjunto de entrenamiento\n    # En cada época, el modelo pasa por todos los lotes de datos\n    # Durante el entrenamiento, verás:\n    # - loss: pérdida en entrenamiento (queremos que baje)\n    # - accuracy: precisión en entrenamiento (queremos que suba)\n    # - val_loss: pérdida en validación (queremos que baje)\n    # - val_accuracy: precisión en validación (queremos que suba)\n    epochs = 10\n)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1wYnVedSPfmX"
   },
   "source": "Con este enfoque, el modelo alcanza una precisión de validación de alrededor del 80% (nota que el modelo está sobreajustando ya que la precisión de entrenamiento es mayor).\n\nNota: Tus resultados pueden ser un poco diferentes, dependiendo de cómo se inicializaron aleatoriamente los pesos antes de entrenar la capa de embedding.\n\nPuedes ver el resumen del modelo para aprender más sobre cada capa del modelo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDCgjWyq_0dc"
   },
   "outputs": [],
   "source": "# ===== MOSTRAR EL RESUMEN DEL MODELO =====\n# Esto muestra la arquitectura completa del modelo con:\n# - Nombre de cada capa\n# - Forma de salida de cada capa\n# - Número de parámetros (pesos) en cada capa\n\n# El total de parámetros nos dice cuántos pesos está aprendiendo el modelo\n# La mayoría están en la capa Embedding (10,000 palabras × 16 dimensiones = 160,000)\nmodel.summary()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KCoA6qwqP836"
   },
   "source": "## Recuperar los Embeddings de Palabras Entrenados y Guardarlos en Disco\n\nA continuación, recupera los embeddings de palabras aprendidos durante el entrenamiento. Los embeddings son pesos de la capa Embedding en el modelo. La matriz de pesos tiene forma `(vocab_size, embedding_dimension)`."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5rv01WG2YA"
   },
   "source": "Obtén los pesos del modelo usando `get_layer()` y `get_weights()`. La función `get_vocabulary()` proporciona el vocabulario para construir un archivo de metadatos con un token por línea."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": "# ===== EXTRAER LOS PESOS DE LA CAPA EMBEDDING =====\n# Esta celda solo muestra los pesos, no los guarda aún\n# Los pesos son los vectores de embedding que el modelo aprendió\n# Cada fila es un vector de 16 números que representa una palabra\nmodel.get_layer('embedding').get_weights()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Uamp1YH8RzU"
   },
   "outputs": [],
   "source": "# ===== EXTRAER PESOS Y VOCABULARIO =====\n\n# Obtener la matriz de pesos (embeddings) de la capa de embedding\n# weights[0] porque get_weights() devuelve una lista, y queremos el primer elemento\n# Forma: (10000, 16) - 10,000 palabras, cada una con 16 dimensiones\nweights = model.get_layer('embedding').get_weights()[0]\n\n# Obtener la lista de palabras en el vocabulario\n# Esto nos da las palabras en orden de su índice\n# Por ejemplo: índice 0 = '', índice 1 = '[UNK]', índice 2 = 'the', etc.\nvocab = vectorize_layer.get_vocabulary()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== INSPECCIONAR EL VOCABULARIO =====\n\n# Mostrar el tamaño del vocabulario (debería ser 10,000)\nprint(len(vocab))\n\n# Mostrar las primeras 10 palabras del vocabulario\n# '' = padding (relleno), '[UNK]' = unknown (desconocido)\n# Las demás son las palabras más frecuentes\nprint(vocab[:10])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== INSPECCIONAR LOS PESOS (EMBEDDINGS) =====\n\n# Mostrar la forma de la matriz de pesos\n# Debería ser (10000, 16) = 10,000 palabras × 16 dimensiones\nprint(weights.shape)\n\n# Mostrar los primeros 2 vectores de embedding\n# Estos son los embeddings para el padding ('') y unknown ('[UNK]')\nprint(weights[:2])"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "J8MiCA77X8B8"
   },
   "source": "Escribe los pesos en disco. Para usar el [Embedding Projector](http://projector.tensorflow.org), subirás dos archivos en formato separado por tabulaciones: un archivo de vectores (que contiene los embeddings), y un archivo de metadatos (que contiene las palabras)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLIahl9s53XT"
   },
   "outputs": [],
   "source": "# ===== GUARDAR EMBEDDINGS EN ARCHIVOS TSV =====\n# Necesitamos 2 archivos para visualizar los embeddings:\n# 1. vectors.tsv: contiene los valores numéricos de los embeddings\n# 2. metadata.tsv: contiene las palabras correspondientes\n\n# Abrir archivo para escribir los vectores (encoding UTF-8 para caracteres especiales)\nout_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n\n# Abrir archivo para escribir los metadatos (las palabras)\nout_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n\n# Iterar sobre cada palabra en el vocabulario\nfor index, word in enumerate(vocab):\n  # Saltar el índice 0 porque es solo padding (relleno), no una palabra real\n  if index == 0:\n    continue\n  \n  # Obtener el vector de embedding para esta palabra\n  vec = weights[index]\n  \n  # Escribir el vector en el archivo de vectores\n  # Convertir cada número a string y unirlos con tabulaciones\n  # Agregar salto de línea al final\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n  \n  # Escribir la palabra en el archivo de metadatos\n  out_m.write(word + \"\\n\")\n\n# Cerrar ambos archivos para asegurar que se guarden correctamente\nout_v.close()\nout_m.close()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JQyMZWyxYjMr"
   },
   "source": "Si estás ejecutando este tutorial en [Colaboratory](https://colab.research.google.com), puedes usar el siguiente fragmento de código para descargar estos archivos a tu máquina local (o usar el navegador de archivos, *View -> Table of contents -> File browser*)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUsjQOKMIV2z"
   },
   "outputs": [],
   "source": "# ===== DESCARGAR ARCHIVOS EN GOOGLE COLAB (OPCIONAL) =====\n# Este código está comentado porque solo funciona en Google Colab\n# Si ejecutas este notebook en Colab, descomenta estas líneas para descargar los archivos\n\n# try:\n#   from google.colab import files\n#   files.download('vectors.tsv')\n#   files.download('metadata.tsv')\n# except Exception:\n#   pass"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXLfFA54Yz-o"
   },
   "source": "## Visualizar los Embeddings\n\nPara visualizar los embeddings, súbelos al Embedding Projector.\n\nAbre el [Embedding Projector](http://projector.tensorflow.org/) (esto también puede ejecutarse en una instancia local de TensorBoard).\n\n* Haz clic en \"Load data\" (Cargar datos).\n\n* Sube los dos archivos que creaste arriba: `vectors.tsv` y `metadata.tsv`.\n\nLos embeddings que has entrenado ahora se mostrarán. Puedes buscar palabras para encontrar sus vecinos más cercanos. Por ejemplo, intenta buscar \"beautiful\" (hermoso). Podrías ver vecinos como \"wonderful\" (maravilloso).\n\nNota: Experimentalmente, es posible que puedas producir embeddings más interpretables usando un modelo más simple. Intenta eliminar la capa `Dense(16)`, volver a entrenar el modelo y visualizar los embeddings nuevamente.\n\nNota: Típicamente, se necesita un conjunto de datos mucho más grande para entrenar embeddings de palabras más interpretables. Este tutorial usa un pequeño conjunto de datos de IMDb con fines de demostración."
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wvKiEHjramNh"
   },
   "source": "## Próximos Pasos"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BSgAZpwF5xF_"
   },
   "source": "Este tutorial te ha mostrado cómo entrenar y visualizar embeddings de palabras desde cero en un conjunto de datos pequeño.\n\n* Para entrenar embeddings de palabras usando el algoritmo Word2Vec, prueba el tutorial de [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec). \n\n* Para aprender más sobre procesamiento avanzado de texto, lee el [Modelo Transformer para comprensión del lenguaje](https://www.tensorflow.org/tutorials/text/transformer)."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}