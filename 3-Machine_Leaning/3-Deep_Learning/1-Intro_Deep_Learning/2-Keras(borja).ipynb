{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "Librería para programar redes neuronales de una manera más sencilla que con TensorFlow. Keras se encuentra en una capa de abstracción por encima de TensorFlow.\n",
    "\n",
    "[Documentación](https://keras.io/guides/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezamos importando librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n",
    "# ============================================\n",
    "\n",
    "# import tensorflow as tf  # TensorFlow es el framework base\n",
    "from tensorflow import keras  # Keras es la API de alto nivel de TensorFlow\n",
    "from tensorflow.keras import layers  # Módulo de capas para construir redes neuronales\n",
    "\n",
    "import pandas as pd  # Para manipulación y análisis de datos\n",
    "import numpy as np  # Para operaciones numéricas y manejo de arrays"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos de mnist. No vamos a tratar imagenes con redes convolucionales (perdemos la estructura espacial 2D). Todos los pixeles se convertirán en un vector de 28x28 features independientes, que serán las entradas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGA DEL DATASET MNIST\n",
    "# ============================================\n",
    "# MNIST es un dataset clásico de dígitos escritos a mano (0-9)\n",
    "# Cada imagen es de 28x28 píxeles en escala de grises\n",
    "\n",
    "# Keras incluye datasets populares que podemos cargar directamente\n",
    "# load_data() retorna 4 arrays: X_train, y_train, X_test, y_test\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos dimensiones del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXPLORACIÓN DE LAS DIMENSIONES DEL DATASET\n",
    "# ============================================\n",
    "'''\n",
    "Resultado esperado:\n",
    "- X_train: 60.000 imágenes de 28x28 píxeles (conjunto de entrenamiento)\n",
    "- y_train: 60.000 etiquetas (los números del 0 al 9)\n",
    "- X_test: 10.000 imágenes de prueba\n",
    "- y_test: 10.000 etiquetas de prueba\n",
    "'''\n",
    "\n",
    "# Shape de los datos de entrenamiento\n",
    "print(X_train.shape)  # (60000, 28, 28)\n",
    "print(y_train.shape)  # (60000,)\n",
    "\n",
    "# Shape de los datos de prueba\n",
    "print(X_test.shape)   # (10000, 28, 28)\n",
    "print(y_test.shape)   # (10000,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60.000 imágenes de 28x28 pixeles. Vamos a representar una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\borja\\AppData\\Local\\Temp\\ipykernel_27664\\1360854288.py:8: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE UNA IMAGEN DEL DATASET\n",
    "# ============================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mostramos la primera imagen del conjunto de entrenamiento\n",
    "# cmap='Greys' muestra la imagen en escala de grises\n",
    "plt.imshow(X_train[0], cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada imagen se compone de 28x28 pixeles, y cada pixel \n",
    "\n",
    "0 representa negro absoluto.\n",
    "\n",
    "255 representa blanco absoluto.\n",
    "\n",
    "¿Se te ocurre alguna manera de normalizar los datos?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# NORMALIZACIÓN DE LOS DATOS (FEATURE SCALING)\n",
    "# ============================================\n",
    "# Los valores de los píxeles van de 0 a 255\n",
    "# Dividimos entre 255 para normalizar los valores entre 0 y 1\n",
    "# Esto ayuda a que la red neuronal aprenda más rápido y mejor\n",
    "\n",
    "# Convertimos a float32 para mayor precisión y dividimos por 255\n",
    "X_train = X_train.astype(\"float32\") / 255  # Normalización del conjunto de entrenamiento\n",
    "X_test = X_test.astype(\"float32\") / 255    # Normalización del conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "        0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "        0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.11764706, 0.14117648,\n",
       "        0.36862746, 0.6039216 , 0.6666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.88235295, 0.6745098 ,\n",
       "        0.99215686, 0.9490196 , 0.7647059 , 0.2509804 , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.19215687, 0.93333334, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.9843137 , 0.3647059 , 0.32156864,\n",
       "        0.32156864, 0.21960784, 0.15294118, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.07058824, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7764706 ,\n",
       "        0.7137255 , 0.96862745, 0.94509804, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3137255 , 0.6117647 ,\n",
       "        0.41960785, 0.99215686, 0.99215686, 0.8039216 , 0.04313726,\n",
       "        0.        , 0.16862746, 0.6039216 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "        0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.54509807, 0.99215686, 0.74509805, 0.00784314,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04313726, 0.74509805, 0.99215686, 0.27450982,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.13725491, 0.94509804, 0.88235295,\n",
       "        0.627451  , 0.42352942, 0.00392157, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31764707, 0.9411765 ,\n",
       "        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "        0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0627451 , 0.3647059 , 0.9882353 , 0.99215686, 0.73333335,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.9764706 , 0.99215686, 0.9764706 ,\n",
       "        0.2509804 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.18039216,\n",
       "        0.50980395, 0.7176471 , 0.99215686, 0.99215686, 0.8117647 ,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.15294118, 0.5803922 , 0.8980392 ,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.7137255 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.09019608, 0.25882354,\n",
       "        0.8352941 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.7764706 , 0.31764707, 0.00784314, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.07058824, 0.67058825, 0.85882354, 0.99215686,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.7647059 , 0.3137255 ,\n",
       "        0.03529412, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.21568628,\n",
       "        0.6745098 , 0.8862745 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99215686, 0.95686275, 0.52156866, 0.04313726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.53333336,\n",
       "        0.99215686, 0.99215686, 0.99215686, 0.83137256, 0.5294118 ,\n",
       "        0.5176471 , 0.0627451 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# VERIFICACIÓN DE LA NORMALIZACIÓN\n",
    "# ============================================\n",
    "# Comprobamos que ahora los valores están entre 0 y 1\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONVERSIÓN DE ETIQUETAS A FLOAT32\n",
    "# ============================================\n",
    "# Convertimos las etiquetas (y) a float32 por consistencia\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos datos para validación. Estos datos se usarán durante el entrenamiento. Otra opción es decirle a keras en la etapa de entrenamiento que reserve un X % de los datos para validar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CREACIÓN DEL CONJUNTO DE VALIDACIÓN\n",
    "# ============================================\n",
    "# Separamos 10.000 muestras del conjunto de entrenamiento para validación\n",
    "# La validación se usa durante el entrenamiento para evaluar el modelo\n",
    "# en cada epoch y detectar overfitting\n",
    "\n",
    "# Últimas 10.000 muestras para validación\n",
    "X_val = X_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "\n",
    "# Las primeras 50.000 muestras quedan para entrenamiento\n",
    "X_train = X_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# VERIFICACIÓN DE LAS DIMENSIONES FINALES\n",
    "# ============================================\n",
    "print(X_train.shape)  # (50000, 28, 28) - Entrenamiento\n",
    "print(X_val.shape)    # (10000, 28, 28) - Validación\n",
    "print(X_test.shape)   # (10000, 28, 28) - Prueba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos la arquitectura de la red neuronal. Se va a componer de:\n",
    "* **Sequential**: API para iniciar la red neuronal. No cuenta como capa.\n",
    "* **Flatten**: capa de entrada. Necesita un vector unidimensional. Como tenemos imágenes, esta capa aplana las imagenes (2D) en 1D.\n",
    "* **Dense**: es una hidden layer. Se compondrá de `n` neuronas y de una función de activación que se aplicará a todas las neuronas de la capa.\n",
    "\n",
    "Recuerda que es un problema de clasificación multiclase (10 clases) y que por tanto la última capa se compondrá de tantas neuronas como clases tengas.\n",
    "\n",
    "En cuanto a las funciones de activación es recomendable usar relu en las hidden layer, que tarda menos en entrenar, mientras que la ultima (output) suele ser una softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\borja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CONSTRUCCIÓN DE LA ARQUITECTURA DE LA RED NEURONAL\n",
    "# ============================================\n",
    "# Vamos a crear una red neuronal secuencial (capa por capa)\n",
    "\n",
    "# Inicializamos el modelo secuencial\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# CAPA DE ENTRADA (Flatten)\n",
    "# Convierte la matriz 2D (28x28) en un vector 1D de 784 elementos\n",
    "# Esto es necesario porque las capas Dense requieren entrada 1D\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# PRIMERA CAPA OCULTA (Hidden Layer 1)\n",
    "# 300 neuronas con función de activación ReLU\n",
    "# ReLU (Rectified Linear Unit): f(x) = max(0, x)\n",
    "# Es rápida de calcular y evita el problema de gradientes que desaparecen\n",
    "model.add(keras.layers.Dense(units=300,\n",
    "                              activation='relu'))\n",
    "\n",
    "# SEGUNDA CAPA OCULTA (Hidden Layer 2)\n",
    "# 100 neuronas con función de activación ReLU\n",
    "# Al tener menos neuronas, va reduciendo la dimensionalidad\n",
    "model.add(keras.layers.Dense(units=100,\n",
    "                              activation='relu'))\n",
    "\n",
    "# CAPA DE SALIDA (Output Layer)\n",
    "# 10 neuronas (una por cada dígito del 0 al 9)\n",
    "# Softmax: convierte los valores en probabilidades que suman 1\n",
    "# Ideal para problemas de clasificación multiclase\n",
    "model.add(keras.layers.Dense(units=10,\n",
    "                              activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FORMA ALTERNATIVA DE DECLARAR LA RED NEURONAL\n",
    "# ============================================\n",
    "# Esta forma es más compacta y legible cuando tenemos muchas capas\n",
    "\n",
    "# Definimos todas las capas en una lista\n",
    "capas = [\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # Capa de entrada\n",
    "    keras.layers.Dense(units=300, activation='relu'),  # Hidden layer 1\n",
    "    keras.layers.Dense(units=100, activation='relu'),  # Hidden layer 2\n",
    "    keras.layers.Dense(units=10, activation='softmax')  # Capa de salida\n",
    "]\n",
    "\n",
    "# Creamos el modelo pasando la lista de capas directamente\n",
    "model = keras.models.Sequential(capas)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver las capas, y acceder a sus elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Flatten name=flatten_1, built=True>\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# INSPECCIÓN DE LAS CAPAS DEL MODELO\n",
    "# ============================================\n",
    "# Podemos acceder a cada capa individualmente usando índices\n",
    "print(model.layers[0])  # Primera capa (Flatten)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los pesos de las capas sin entrenar, porque los inicializa aleatoriamente. Los bias los inicializa a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE PESOS Y SESGOS INICIALES\n",
    "# ============================================\n",
    "# Keras inicializa los pesos aleatoriamente y los sesgos (bias) en 0\n",
    "# Esto ocurre antes del entrenamiento\n",
    "\n",
    "# Accedemos a la primera capa oculta (índice 1)\n",
    "hidden1 = model.layers[1]\n",
    "\n",
    "# get_weights() retorna una tupla: (weights, biases)\n",
    "weights, biases = hidden1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01131446,  0.03214083,  0.02625445, ...,  0.01113001,\n",
       "         0.05868475, -0.07246196],\n",
       "       [-0.06270335, -0.0494878 , -0.04371113, ...,  0.06257014,\n",
       "        -0.04753117, -0.06525592],\n",
       "       [-0.00640301,  0.0284531 , -0.05991141, ..., -0.00096421,\n",
       "        -0.0482851 , -0.07241929],\n",
       "       ...,\n",
       "       [ 0.05809818, -0.03942509, -0.01202272, ..., -0.05949966,\n",
       "        -0.07271319, -0.06703442],\n",
       "       [ 0.05766064, -0.04801146,  0.04844613, ...,  0.03699864,\n",
       "        -0.04538583,  0.0589208 ],\n",
       "       [-0.01884619,  0.03224362,  0.01797148, ...,  0.01384272,\n",
       "         0.06716406, -0.06738105]], shape=(784, 300), dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizamos la matriz de pesos (valores aleatorios iniciales)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número total de pesos en esta capa\n",
    "# Debería ser: 784 (entradas) × 300 (neuronas) = 235,200\n",
    "weights.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos la configuración de ejecución... el compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPILACIÓN DEL MODELO (FORMA DETALLADA)\n",
    "# ============================================\n",
    "# La compilación configura el proceso de aprendizaje del modelo\n",
    "\n",
    "model.compile(\n",
    "    # OPTIMIZADOR: SGD (Stochastic Gradient Descent)\n",
    "    # Actualiza los pesos usando el gradiente descendente estocástico\n",
    "    optimizer=keras.optimizers.SGD(),\n",
    "    \n",
    "    # FUNCIÓN DE PÉRDIDA: Sparse Categorical Crossentropy\n",
    "    # Mide el error entre las predicciones y las etiquetas reales\n",
    "    # \"Sparse\" porque las etiquetas son enteros (0-9), no one-hot encoded\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    \n",
    "    # MÉTRICA: Accuracy (precisión)\n",
    "    # Porcentaje de predicciones correctas durante el entrenamiento\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMPILACIÓN DEL MODELO (FORMA SIMPLIFICADA)\n",
    "# ============================================\n",
    "# Equivalente al anterior pero usando nombres de string\n",
    "# Esta forma es más corta y comúnmente usada\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"sgd\",  # Optimizador SGD\n",
    "    loss=\"sparse_categorical_crossentropy\",  # Función de pérdida\n",
    "    metrics=[\"accuracy\"]  # Métrica de evaluación\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# RESUMEN DEL MODELO\n",
    "# ============================================\n",
    "# Muestra la arquitectura completa: capas, shapes y parámetros\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo. Usamos los datos de entrenamiento. El batch_size es la cantidad de muestras que utiliza el SGD, y las epochs son las iteraciones que realiza en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificamos el shape de los datos de entrenamiento\n",
    "X_train.shape  # (50000, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6787 - loss: 1.2898 - val_accuracy: 0.8579 - val_loss: 0.6085\n",
      "Epoch 2/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8680 - loss: 0.5193 - val_accuracy: 0.8974 - val_loss: 0.3935\n",
      "Epoch 3/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8916 - loss: 0.3967 - val_accuracy: 0.9085 - val_loss: 0.3318\n",
      "Epoch 4/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9027 - loss: 0.3465 - val_accuracy: 0.9167 - val_loss: 0.3021\n",
      "Epoch 5/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9106 - loss: 0.3161 - val_accuracy: 0.9238 - val_loss: 0.2781\n",
      "Epoch 6/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9168 - loss: 0.2939 - val_accuracy: 0.9275 - val_loss: 0.2630\n",
      "Epoch 7/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9222 - loss: 0.2763 - val_accuracy: 0.9294 - val_loss: 0.2496\n",
      "Epoch 8/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9256 - loss: 0.2616 - val_accuracy: 0.9330 - val_loss: 0.2387\n",
      "Epoch 9/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9293 - loss: 0.2488 - val_accuracy: 0.9376 - val_loss: 0.2283\n",
      "Epoch 10/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9329 - loss: 0.2374 - val_accuracy: 0.9404 - val_loss: 0.2187\n",
      "Epoch 11/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9360 - loss: 0.2274 - val_accuracy: 0.9428 - val_loss: 0.2103\n",
      "Epoch 12/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9381 - loss: 0.2181 - val_accuracy: 0.9447 - val_loss: 0.2039\n",
      "Epoch 13/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9401 - loss: 0.2098 - val_accuracy: 0.9476 - val_loss: 0.1962\n",
      "Epoch 14/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9427 - loss: 0.2019 - val_accuracy: 0.9490 - val_loss: 0.1903\n",
      "Epoch 15/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9450 - loss: 0.1945 - val_accuracy: 0.9501 - val_loss: 0.1842\n",
      "Epoch 16/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9466 - loss: 0.1878 - val_accuracy: 0.9504 - val_loss: 0.1801\n",
      "Epoch 17/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9481 - loss: 0.1816 - val_accuracy: 0.9515 - val_loss: 0.1753\n",
      "Epoch 18/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9504 - loss: 0.1757 - val_accuracy: 0.9528 - val_loss: 0.1700\n",
      "Epoch 19/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9517 - loss: 0.1703 - val_accuracy: 0.9543 - val_loss: 0.1658\n",
      "Epoch 20/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9531 - loss: 0.1648 - val_accuracy: 0.9553 - val_loss: 0.1618\n",
      "Epoch 21/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9546 - loss: 0.1599 - val_accuracy: 0.9566 - val_loss: 0.1577\n",
      "Epoch 22/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9559 - loss: 0.1553 - val_accuracy: 0.9586 - val_loss: 0.1545\n",
      "Epoch 23/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9571 - loss: 0.1509 - val_accuracy: 0.9584 - val_loss: 0.1526\n",
      "Epoch 24/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1467 - val_accuracy: 0.9599 - val_loss: 0.1473\n",
      "Epoch 25/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9597 - loss: 0.1426 - val_accuracy: 0.9605 - val_loss: 0.1452\n",
      "Epoch 26/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9606 - loss: 0.1389 - val_accuracy: 0.9621 - val_loss: 0.1419\n",
      "Epoch 27/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9616 - loss: 0.1352 - val_accuracy: 0.9630 - val_loss: 0.1397\n",
      "Epoch 28/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9624 - loss: 0.1316 - val_accuracy: 0.9635 - val_loss: 0.1372\n",
      "Epoch 29/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9636 - loss: 0.1284 - val_accuracy: 0.9640 - val_loss: 0.1349\n",
      "Epoch 30/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9644 - loss: 0.1250 - val_accuracy: 0.9642 - val_loss: 0.1342\n",
      "Epoch 31/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9654 - loss: 0.1222 - val_accuracy: 0.9653 - val_loss: 0.1303\n",
      "Epoch 32/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9665 - loss: 0.1192 - val_accuracy: 0.9659 - val_loss: 0.1285\n",
      "Epoch 33/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9675 - loss: 0.1164 - val_accuracy: 0.9659 - val_loss: 0.1267\n",
      "Epoch 34/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9680 - loss: 0.1136 - val_accuracy: 0.9670 - val_loss: 0.1249\n",
      "Epoch 35/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9694 - loss: 0.1110 - val_accuracy: 0.9676 - val_loss: 0.1241\n",
      "Epoch 36/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.1085 - val_accuracy: 0.9670 - val_loss: 0.1214\n",
      "Epoch 37/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9707 - loss: 0.1060 - val_accuracy: 0.9674 - val_loss: 0.1206\n",
      "Epoch 38/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9711 - loss: 0.1037 - val_accuracy: 0.9679 - val_loss: 0.1201\n",
      "Epoch 39/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9722 - loss: 0.1015 - val_accuracy: 0.9679 - val_loss: 0.1173\n",
      "Epoch 40/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9722 - loss: 0.0994 - val_accuracy: 0.9688 - val_loss: 0.1162\n",
      "Epoch 41/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9733 - loss: 0.0971 - val_accuracy: 0.9689 - val_loss: 0.1149\n",
      "Epoch 42/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9737 - loss: 0.0952 - val_accuracy: 0.9698 - val_loss: 0.1127\n",
      "Epoch 43/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9743 - loss: 0.0934 - val_accuracy: 0.9699 - val_loss: 0.1120\n",
      "Epoch 44/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.0913 - val_accuracy: 0.9696 - val_loss: 0.1100\n",
      "Epoch 45/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9755 - loss: 0.0894 - val_accuracy: 0.9696 - val_loss: 0.1091\n",
      "Epoch 46/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9759 - loss: 0.0878 - val_accuracy: 0.9712 - val_loss: 0.1084\n",
      "Epoch 47/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9766 - loss: 0.0860 - val_accuracy: 0.9714 - val_loss: 0.1079\n",
      "Epoch 48/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.0844 - val_accuracy: 0.9708 - val_loss: 0.1067\n",
      "Epoch 49/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9775 - loss: 0.0827 - val_accuracy: 0.9709 - val_loss: 0.1048\n",
      "Epoch 50/50\n",
      "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9779 - loss: 0.0810 - val_accuracy: 0.9719 - val_loss: 0.1045\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ENTRENAMIENTO DEL MODELO\n",
    "# ============================================\n",
    "# Aquí es donde la red neuronal \"aprende\" ajustando sus pesos\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,  # Datos de entrada (imágenes)\n",
    "    y_train,  # Etiquetas (targets)\n",
    "    \n",
    "    # BATCH_SIZE: número de muestras procesadas antes de actualizar pesos\n",
    "    # Un batch más pequeño = más actualizaciones pero más lento\n",
    "    batch_size=128,\n",
    "    \n",
    "    # EPOCHS: número de veces que el modelo ve todo el dataset\n",
    "    # Más epochs = más aprendizaje, pero riesgo de overfitting\n",
    "    epochs=50,\n",
    "    \n",
    "    # VALIDATION_DATA: datos para evaluar el modelo en cada epoch\n",
    "    # Permite monitorear si hay overfitting (loss de validación aumenta)\n",
    "    validation_data=(X_val, y_val)  # Alternativa: validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Batch size\n",
    "\n",
    "En entrenamiento de modelos (machine learning / deep learning), el **batch size** es:\n",
    "\n",
    "> **La cantidad de muestras (datos) que se procesan juntas en una sola pasada antes de actualizar los pesos del modelo.**\n",
    "\n",
    "Ejemplos:\n",
    "\n",
    "- **batch size = 1** → el modelo procesa **1 muestra a la vez**\n",
    "- **batch size = 32** → procesa **32 muestras a la vez**\n",
    "- **batch size = 128** → procesa **128 muestras a la vez**\n",
    "\n",
    "**Impacto del batch size:**\n",
    "\n",
    "- **Velocidad:** batches grandes suelen aprovechar mejor la GPU → entrenamiento más rápido.\n",
    "- **Ruido en el gradiente:**  \n",
    "  - batch pequeño → más ruido, más variabilidad en las actualizaciones.  \n",
    "  - batch grande → actualizaciones más estables.\n",
    "- **Memoria:** batch más grande → mayor consumo de RAM/VRAM.\n",
    "\n",
    "---\n",
    "\n",
    "## Época (epoch) y su relación con el batch size\n",
    "\n",
    "### ¿Qué es un epoch?\n",
    "\n",
    "Un **epoch** es:\n",
    "\n",
    "> **Una pasada completa por todo el dataset de entrenamiento.**\n",
    "\n",
    "Es decir, el modelo ve **todas** las muestras del conjunto de datos **una vez**.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Cómo se relacionan epoch y batch size?\n",
    "\n",
    "El **batch size** define cuántas muestras se procesan a la vez dentro de un epoch, por lo que:\n",
    "\n",
    "> Un **epoch se divide en varios batches**.\n",
    "\n",
    "Si:\n",
    "\n",
    "- **N** = número total de muestras del dataset  \n",
    "- **B** = batch size  \n",
    "\n",
    "Entonces:\n",
    "\n",
    "\\[\n",
    "\\text{Número de batches por epoch} = \\frac{N}{B}\n",
    "\\]\n",
    "\n",
    "(normalmente se redondea hacia arriba si N no es múltiplo de B).\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo práctico\n",
    "\n",
    "Supón que tienes:\n",
    "\n",
    "- Dataset con **N = 10 000** imágenes  \n",
    "- **batch size = 100**\n",
    "\n",
    "Entonces:\n",
    "\n",
    "\\[\n",
    "\\text{batches por epoch} = \\frac{10 000}{100} = 100\n",
    "\\]\n",
    "\n",
    "Eso significa que:\n",
    "\n",
    "- En **cada epoch**, el modelo hará **100 actualizaciones de pesos** (una por batch).\n",
    "\n",
    "---\n",
    "\n",
    "### Efecto de cambiar el batch size\n",
    "\n",
    "| Batch size | Batches por epoch (con N = 10 000) | Comentario                                  |\n",
    "|-----------:|------------------------------------:|---------------------------------------------|\n",
    "| 50         | 200                                | Más actualizaciones, más detalle, más lento |\n",
    "| 100        | 100                                | Equilibrado                                 |\n",
    "| 500        | 20                                 | Menos actualizaciones, más rápido           |\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen rápido\n",
    "\n",
    "- **Escala de grises:**\n",
    "  - 0 = negro, 255 = blanco, intermedios = grises.\n",
    "- **Batch size:**\n",
    "  - Cantidad de muestras que pasan **de una vez** por el modelo antes de actualizar los pesos.\n",
    "- **Epoch:**\n",
    "  - Una pasada completa por todo el dataset.\n",
    "- **Relación:**\n",
    "  - Un epoch se divide en varios batches.\n",
    "  - \\(\\text{batches por epoch} = \\frac{N}{B}\\).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos reentrenar el modelo. No empieza de nuevo, sino que retoma el entrenamiento anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0569 - val_accuracy: 0.9741 - val_loss: 0.0922\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9858 - loss: 0.0549 - val_accuracy: 0.9736 - val_loss: 0.0942\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9864 - loss: 0.0532 - val_accuracy: 0.9751 - val_loss: 0.0884\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9869 - loss: 0.0514 - val_accuracy: 0.9710 - val_loss: 0.0988\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9875 - loss: 0.0499 - val_accuracy: 0.9762 - val_loss: 0.0873\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9879 - loss: 0.0483 - val_accuracy: 0.9765 - val_loss: 0.0862\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9881 - loss: 0.0468 - val_accuracy: 0.9757 - val_loss: 0.0865\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9887 - loss: 0.0453 - val_accuracy: 0.9769 - val_loss: 0.0847\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.9887 - loss: 0.0440 - val_accuracy: 0.9777 - val_loss: 0.0839\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9896 - loss: 0.0425 - val_accuracy: 0.9767 - val_loss: 0.0847\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# REENTRENAMIENTO DEL MODELO\n",
    "# ============================================\n",
    "# Podemos seguir entrenando un modelo ya entrenado\n",
    "# NO empieza desde cero, continúa desde donde se quedó\n",
    "# Útil para hacer ajustes finos (fine-tuning)\n",
    "\n",
    "rentre = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=64,  # Probamos con un batch diferente\n",
    "    epochs=10,  # Solo 10 epochs adicionales\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el histórico del entrenamiento, para poder representarlo posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': 'auto', 'epochs': 50, 'steps': 391}\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "{'accuracy': [0.678659975528717, 0.8679800033569336, 0.8916400074958801, 0.9027199745178223, 0.9105799794197083, 0.9167799949645996, 0.9222000241279602, 0.9255599975585938, 0.9293400049209595, 0.9328799843788147, 0.9359999895095825, 0.9381200075149536, 0.9400799870491028, 0.9427400231361389, 0.9449800252914429, 0.9465799927711487, 0.9480800032615662, 0.9503800272941589, 0.9517199993133545, 0.9531400203704834, 0.9545800089836121, 0.9559000134468079, 0.9570599794387817, 0.9580199718475342, 0.9596800208091736, 0.960640013217926, 0.9616199731826782, 0.9623799920082092, 0.9636200070381165, 0.9643800258636475, 0.9654399752616882, 0.9665399789810181, 0.9675400257110596, 0.9680200219154358, 0.9694399833679199, 0.9698399901390076, 0.970740020275116, 0.9711199998855591, 0.9721599817276001, 0.9721999764442444, 0.9732800126075745, 0.9736800193786621, 0.97434002161026, 0.9750000238418579, 0.9755399823188782, 0.9758800268173218, 0.9765999913215637, 0.9772800207138062, 0.9775000214576721, 0.9779000282287598], 'loss': [1.2898353338241577, 0.5193169713020325, 0.3966987431049347, 0.3464789092540741, 0.3160645663738251, 0.2939448058605194, 0.2763013243675232, 0.2616494596004486, 0.24884329736232758, 0.2374485731124878, 0.22738152742385864, 0.2181149423122406, 0.20975564420223236, 0.2018868327140808, 0.19446568191051483, 0.18784429132938385, 0.1815742403268814, 0.1757011115550995, 0.17029665410518646, 0.16477027535438538, 0.15994836390018463, 0.15532106161117554, 0.1509193629026413, 0.14668472111225128, 0.142578125, 0.13891325891017914, 0.13518927991390228, 0.13155852258205414, 0.12835688889026642, 0.1250493824481964, 0.12222325056791306, 0.11918097734451294, 0.1163877546787262, 0.11357998102903366, 0.11098388582468033, 0.10852993279695511, 0.10601018369197845, 0.10374918580055237, 0.10152776539325714, 0.09938088059425354, 0.0971401184797287, 0.09521646797657013, 0.09338772296905518, 0.0912870243191719, 0.08938214182853699, 0.08783898502588272, 0.08597204834222794, 0.0843658596277237, 0.08268391340970993, 0.08097235858440399], 'val_accuracy': [0.8579000234603882, 0.8974000215530396, 0.9085000157356262, 0.916700005531311, 0.923799991607666, 0.9275000095367432, 0.9294000267982483, 0.9330000281333923, 0.9376000165939331, 0.9404000043869019, 0.942799985408783, 0.9447000026702881, 0.9476000070571899, 0.9490000009536743, 0.9501000046730042, 0.9503999948501587, 0.9514999985694885, 0.9527999758720398, 0.9542999863624573, 0.955299973487854, 0.95660001039505, 0.9585999846458435, 0.9584000110626221, 0.9599000215530396, 0.9605000019073486, 0.9621000289916992, 0.9629999995231628, 0.9635000228881836, 0.9639999866485596, 0.9642000198364258, 0.9653000235557556, 0.9659000039100647, 0.9659000039100647, 0.9670000076293945, 0.9675999879837036, 0.9670000076293945, 0.9674000144004822, 0.9678999781608582, 0.9678999781608582, 0.9688000082969666, 0.9689000248908997, 0.9697999954223633, 0.9699000120162964, 0.9696000218391418, 0.9696000218391418, 0.9711999893188477, 0.9714000225067139, 0.97079998254776, 0.9708999991416931, 0.9718999862670898], 'val_loss': [0.6085324287414551, 0.3934567868709564, 0.3317676782608032, 0.3020879626274109, 0.27806779742240906, 0.262980580329895, 0.24963213503360748, 0.238683819770813, 0.2282586246728897, 0.21872961521148682, 0.21029578149318695, 0.20388546586036682, 0.196157306432724, 0.19032378494739532, 0.18422167003154755, 0.18008196353912354, 0.17531417310237885, 0.17000900208950043, 0.1658388376235962, 0.16177642345428467, 0.15773434937000275, 0.1544729322195053, 0.15263447165489197, 0.1472797840833664, 0.14519037306308746, 0.14188097417354584, 0.13967475295066833, 0.1371784806251526, 0.13485349714756012, 0.13417157530784607, 0.1303287297487259, 0.1285460740327835, 0.12670090794563293, 0.12490657716989517, 0.1241207867860794, 0.1214442029595375, 0.12056294083595276, 0.12005333602428436, 0.11725135147571564, 0.11615485697984695, 0.1148548349738121, 0.1126805916428566, 0.11204192787408829, 0.10996600240468979, 0.10913783311843872, 0.10839119553565979, 0.10794283449649811, 0.10673633962869644, 0.10479282587766647, 0.10450130701065063]}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EXPLORACIÓN DEL HISTÓRICO DE ENTRENAMIENTO\n",
    "# ============================================\n",
    "# El objeto history contiene métricas de cada epoch\n",
    "print(history.params)  # Parámetros del entrenamiento\n",
    "print(history.epoch)   # Número de epochs ejecutadas\n",
    "print(history.history)  # Diccionario con loss, accuracy, val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.678659975528717,\n",
       "  0.8679800033569336,\n",
       "  0.8916400074958801,\n",
       "  0.9027199745178223,\n",
       "  0.9105799794197083,\n",
       "  0.9167799949645996,\n",
       "  0.9222000241279602,\n",
       "  0.9255599975585938,\n",
       "  0.9293400049209595,\n",
       "  0.9328799843788147,\n",
       "  0.9359999895095825,\n",
       "  0.9381200075149536,\n",
       "  0.9400799870491028,\n",
       "  0.9427400231361389,\n",
       "  0.9449800252914429,\n",
       "  0.9465799927711487,\n",
       "  0.9480800032615662,\n",
       "  0.9503800272941589,\n",
       "  0.9517199993133545,\n",
       "  0.9531400203704834,\n",
       "  0.9545800089836121,\n",
       "  0.9559000134468079,\n",
       "  0.9570599794387817,\n",
       "  0.9580199718475342,\n",
       "  0.9596800208091736,\n",
       "  0.960640013217926,\n",
       "  0.9616199731826782,\n",
       "  0.9623799920082092,\n",
       "  0.9636200070381165,\n",
       "  0.9643800258636475,\n",
       "  0.9654399752616882,\n",
       "  0.9665399789810181,\n",
       "  0.9675400257110596,\n",
       "  0.9680200219154358,\n",
       "  0.9694399833679199,\n",
       "  0.9698399901390076,\n",
       "  0.970740020275116,\n",
       "  0.9711199998855591,\n",
       "  0.9721599817276001,\n",
       "  0.9721999764442444,\n",
       "  0.9732800126075745,\n",
       "  0.9736800193786621,\n",
       "  0.97434002161026,\n",
       "  0.9750000238418579,\n",
       "  0.9755399823188782,\n",
       "  0.9758800268173218,\n",
       "  0.9765999913215637,\n",
       "  0.9772800207138062,\n",
       "  0.9775000214576721,\n",
       "  0.9779000282287598],\n",
       " 'loss': [1.2898353338241577,\n",
       "  0.5193169713020325,\n",
       "  0.3966987431049347,\n",
       "  0.3464789092540741,\n",
       "  0.3160645663738251,\n",
       "  0.2939448058605194,\n",
       "  0.2763013243675232,\n",
       "  0.2616494596004486,\n",
       "  0.24884329736232758,\n",
       "  0.2374485731124878,\n",
       "  0.22738152742385864,\n",
       "  0.2181149423122406,\n",
       "  0.20975564420223236,\n",
       "  0.2018868327140808,\n",
       "  0.19446568191051483,\n",
       "  0.18784429132938385,\n",
       "  0.1815742403268814,\n",
       "  0.1757011115550995,\n",
       "  0.17029665410518646,\n",
       "  0.16477027535438538,\n",
       "  0.15994836390018463,\n",
       "  0.15532106161117554,\n",
       "  0.1509193629026413,\n",
       "  0.14668472111225128,\n",
       "  0.142578125,\n",
       "  0.13891325891017914,\n",
       "  0.13518927991390228,\n",
       "  0.13155852258205414,\n",
       "  0.12835688889026642,\n",
       "  0.1250493824481964,\n",
       "  0.12222325056791306,\n",
       "  0.11918097734451294,\n",
       "  0.1163877546787262,\n",
       "  0.11357998102903366,\n",
       "  0.11098388582468033,\n",
       "  0.10852993279695511,\n",
       "  0.10601018369197845,\n",
       "  0.10374918580055237,\n",
       "  0.10152776539325714,\n",
       "  0.09938088059425354,\n",
       "  0.0971401184797287,\n",
       "  0.09521646797657013,\n",
       "  0.09338772296905518,\n",
       "  0.0912870243191719,\n",
       "  0.08938214182853699,\n",
       "  0.08783898502588272,\n",
       "  0.08597204834222794,\n",
       "  0.0843658596277237,\n",
       "  0.08268391340970993,\n",
       "  0.08097235858440399],\n",
       " 'val_accuracy': [0.8579000234603882,\n",
       "  0.8974000215530396,\n",
       "  0.9085000157356262,\n",
       "  0.916700005531311,\n",
       "  0.923799991607666,\n",
       "  0.9275000095367432,\n",
       "  0.9294000267982483,\n",
       "  0.9330000281333923,\n",
       "  0.9376000165939331,\n",
       "  0.9404000043869019,\n",
       "  0.942799985408783,\n",
       "  0.9447000026702881,\n",
       "  0.9476000070571899,\n",
       "  0.9490000009536743,\n",
       "  0.9501000046730042,\n",
       "  0.9503999948501587,\n",
       "  0.9514999985694885,\n",
       "  0.9527999758720398,\n",
       "  0.9542999863624573,\n",
       "  0.955299973487854,\n",
       "  0.95660001039505,\n",
       "  0.9585999846458435,\n",
       "  0.9584000110626221,\n",
       "  0.9599000215530396,\n",
       "  0.9605000019073486,\n",
       "  0.9621000289916992,\n",
       "  0.9629999995231628,\n",
       "  0.9635000228881836,\n",
       "  0.9639999866485596,\n",
       "  0.9642000198364258,\n",
       "  0.9653000235557556,\n",
       "  0.9659000039100647,\n",
       "  0.9659000039100647,\n",
       "  0.9670000076293945,\n",
       "  0.9675999879837036,\n",
       "  0.9670000076293945,\n",
       "  0.9674000144004822,\n",
       "  0.9678999781608582,\n",
       "  0.9678999781608582,\n",
       "  0.9688000082969666,\n",
       "  0.9689000248908997,\n",
       "  0.9697999954223633,\n",
       "  0.9699000120162964,\n",
       "  0.9696000218391418,\n",
       "  0.9696000218391418,\n",
       "  0.9711999893188477,\n",
       "  0.9714000225067139,\n",
       "  0.97079998254776,\n",
       "  0.9708999991416931,\n",
       "  0.9718999862670898],\n",
       " 'val_loss': [0.6085324287414551,\n",
       "  0.3934567868709564,\n",
       "  0.3317676782608032,\n",
       "  0.3020879626274109,\n",
       "  0.27806779742240906,\n",
       "  0.262980580329895,\n",
       "  0.24963213503360748,\n",
       "  0.238683819770813,\n",
       "  0.2282586246728897,\n",
       "  0.21872961521148682,\n",
       "  0.21029578149318695,\n",
       "  0.20388546586036682,\n",
       "  0.196157306432724,\n",
       "  0.19032378494739532,\n",
       "  0.18422167003154755,\n",
       "  0.18008196353912354,\n",
       "  0.17531417310237885,\n",
       "  0.17000900208950043,\n",
       "  0.1658388376235962,\n",
       "  0.16177642345428467,\n",
       "  0.15773434937000275,\n",
       "  0.1544729322195053,\n",
       "  0.15263447165489197,\n",
       "  0.1472797840833664,\n",
       "  0.14519037306308746,\n",
       "  0.14188097417354584,\n",
       "  0.13967475295066833,\n",
       "  0.1371784806251526,\n",
       "  0.13485349714756012,\n",
       "  0.13417157530784607,\n",
       "  0.1303287297487259,\n",
       "  0.1285460740327835,\n",
       "  0.12670090794563293,\n",
       "  0.12490657716989517,\n",
       "  0.1241207867860794,\n",
       "  0.1214442029595375,\n",
       "  0.12056294083595276,\n",
       "  0.12005333602428436,\n",
       "  0.11725135147571564,\n",
       "  0.11615485697984695,\n",
       "  0.1148548349738121,\n",
       "  0.1126805916428566,\n",
       "  0.11204192787408829,\n",
       "  0.10996600240468979,\n",
       "  0.10913783311843872,\n",
       "  0.10839119553565979,\n",
       "  0.10794283449649811,\n",
       "  0.10673633962869644,\n",
       "  0.10479282587766647,\n",
       "  0.10450130701065063]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizamos el diccionario completo del histórico\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos las métricas disponibles en el histórico\n",
    "history.history.keys()  # dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.67866</td>\n",
       "      <td>1.289835</td>\n",
       "      <td>0.8579</td>\n",
       "      <td>0.608532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86798</td>\n",
       "      <td>0.519317</td>\n",
       "      <td>0.8974</td>\n",
       "      <td>0.393457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.89164</td>\n",
       "      <td>0.396699</td>\n",
       "      <td>0.9085</td>\n",
       "      <td>0.331768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.90272</td>\n",
       "      <td>0.346479</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.302088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.91058</td>\n",
       "      <td>0.316065</td>\n",
       "      <td>0.9238</td>\n",
       "      <td>0.278068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.91678</td>\n",
       "      <td>0.293945</td>\n",
       "      <td>0.9275</td>\n",
       "      <td>0.262981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.92220</td>\n",
       "      <td>0.276301</td>\n",
       "      <td>0.9294</td>\n",
       "      <td>0.249632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.92556</td>\n",
       "      <td>0.261649</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>0.238684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.92934</td>\n",
       "      <td>0.248843</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>0.228259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.93288</td>\n",
       "      <td>0.237449</td>\n",
       "      <td>0.9404</td>\n",
       "      <td>0.218730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.93600</td>\n",
       "      <td>0.227382</td>\n",
       "      <td>0.9428</td>\n",
       "      <td>0.210296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.93812</td>\n",
       "      <td>0.218115</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>0.203885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.94008</td>\n",
       "      <td>0.209756</td>\n",
       "      <td>0.9476</td>\n",
       "      <td>0.196157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.94274</td>\n",
       "      <td>0.201887</td>\n",
       "      <td>0.9490</td>\n",
       "      <td>0.190324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.94498</td>\n",
       "      <td>0.194466</td>\n",
       "      <td>0.9501</td>\n",
       "      <td>0.184222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.94658</td>\n",
       "      <td>0.187844</td>\n",
       "      <td>0.9504</td>\n",
       "      <td>0.180082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.94808</td>\n",
       "      <td>0.181574</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>0.175314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.95038</td>\n",
       "      <td>0.175701</td>\n",
       "      <td>0.9528</td>\n",
       "      <td>0.170009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.95172</td>\n",
       "      <td>0.170297</td>\n",
       "      <td>0.9543</td>\n",
       "      <td>0.165839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.95314</td>\n",
       "      <td>0.164770</td>\n",
       "      <td>0.9553</td>\n",
       "      <td>0.161776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.95458</td>\n",
       "      <td>0.159948</td>\n",
       "      <td>0.9566</td>\n",
       "      <td>0.157734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.95590</td>\n",
       "      <td>0.155321</td>\n",
       "      <td>0.9586</td>\n",
       "      <td>0.154473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.95706</td>\n",
       "      <td>0.150919</td>\n",
       "      <td>0.9584</td>\n",
       "      <td>0.152634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.95802</td>\n",
       "      <td>0.146685</td>\n",
       "      <td>0.9599</td>\n",
       "      <td>0.147280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.95968</td>\n",
       "      <td>0.142578</td>\n",
       "      <td>0.9605</td>\n",
       "      <td>0.145190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.96064</td>\n",
       "      <td>0.138913</td>\n",
       "      <td>0.9621</td>\n",
       "      <td>0.141881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.96162</td>\n",
       "      <td>0.135189</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>0.139675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.96238</td>\n",
       "      <td>0.131559</td>\n",
       "      <td>0.9635</td>\n",
       "      <td>0.137178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.96362</td>\n",
       "      <td>0.128357</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.134853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.96438</td>\n",
       "      <td>0.125049</td>\n",
       "      <td>0.9642</td>\n",
       "      <td>0.134172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.96544</td>\n",
       "      <td>0.122223</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>0.130329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.96654</td>\n",
       "      <td>0.119181</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>0.128546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.96754</td>\n",
       "      <td>0.116388</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>0.126701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.96802</td>\n",
       "      <td>0.113580</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.124907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.96944</td>\n",
       "      <td>0.110984</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>0.124121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.96984</td>\n",
       "      <td>0.108530</td>\n",
       "      <td>0.9670</td>\n",
       "      <td>0.121444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.97074</td>\n",
       "      <td>0.106010</td>\n",
       "      <td>0.9674</td>\n",
       "      <td>0.120563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.97112</td>\n",
       "      <td>0.103749</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>0.120053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.97216</td>\n",
       "      <td>0.101528</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>0.117251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.97220</td>\n",
       "      <td>0.099381</td>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.116155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.97328</td>\n",
       "      <td>0.097140</td>\n",
       "      <td>0.9689</td>\n",
       "      <td>0.114855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.97368</td>\n",
       "      <td>0.095216</td>\n",
       "      <td>0.9698</td>\n",
       "      <td>0.112681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.97434</td>\n",
       "      <td>0.093388</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>0.112042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.97500</td>\n",
       "      <td>0.091287</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.109966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.97554</td>\n",
       "      <td>0.089382</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.109138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.97588</td>\n",
       "      <td>0.087839</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>0.108391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.97660</td>\n",
       "      <td>0.085972</td>\n",
       "      <td>0.9714</td>\n",
       "      <td>0.107943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.97728</td>\n",
       "      <td>0.084366</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>0.106736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.97750</td>\n",
       "      <td>0.082684</td>\n",
       "      <td>0.9709</td>\n",
       "      <td>0.104793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.97790</td>\n",
       "      <td>0.080972</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>0.104501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy      loss  val_accuracy  val_loss\n",
       "0    0.67866  1.289835        0.8579  0.608532\n",
       "1    0.86798  0.519317        0.8974  0.393457\n",
       "2    0.89164  0.396699        0.9085  0.331768\n",
       "3    0.90272  0.346479        0.9167  0.302088\n",
       "4    0.91058  0.316065        0.9238  0.278068\n",
       "5    0.91678  0.293945        0.9275  0.262981\n",
       "6    0.92220  0.276301        0.9294  0.249632\n",
       "7    0.92556  0.261649        0.9330  0.238684\n",
       "8    0.92934  0.248843        0.9376  0.228259\n",
       "9    0.93288  0.237449        0.9404  0.218730\n",
       "10   0.93600  0.227382        0.9428  0.210296\n",
       "11   0.93812  0.218115        0.9447  0.203885\n",
       "12   0.94008  0.209756        0.9476  0.196157\n",
       "13   0.94274  0.201887        0.9490  0.190324\n",
       "14   0.94498  0.194466        0.9501  0.184222\n",
       "15   0.94658  0.187844        0.9504  0.180082\n",
       "16   0.94808  0.181574        0.9515  0.175314\n",
       "17   0.95038  0.175701        0.9528  0.170009\n",
       "18   0.95172  0.170297        0.9543  0.165839\n",
       "19   0.95314  0.164770        0.9553  0.161776\n",
       "20   0.95458  0.159948        0.9566  0.157734\n",
       "21   0.95590  0.155321        0.9586  0.154473\n",
       "22   0.95706  0.150919        0.9584  0.152634\n",
       "23   0.95802  0.146685        0.9599  0.147280\n",
       "24   0.95968  0.142578        0.9605  0.145190\n",
       "25   0.96064  0.138913        0.9621  0.141881\n",
       "26   0.96162  0.135189        0.9630  0.139675\n",
       "27   0.96238  0.131559        0.9635  0.137178\n",
       "28   0.96362  0.128357        0.9640  0.134853\n",
       "29   0.96438  0.125049        0.9642  0.134172\n",
       "30   0.96544  0.122223        0.9653  0.130329\n",
       "31   0.96654  0.119181        0.9659  0.128546\n",
       "32   0.96754  0.116388        0.9659  0.126701\n",
       "33   0.96802  0.113580        0.9670  0.124907\n",
       "34   0.96944  0.110984        0.9676  0.124121\n",
       "35   0.96984  0.108530        0.9670  0.121444\n",
       "36   0.97074  0.106010        0.9674  0.120563\n",
       "37   0.97112  0.103749        0.9679  0.120053\n",
       "38   0.97216  0.101528        0.9679  0.117251\n",
       "39   0.97220  0.099381        0.9688  0.116155\n",
       "40   0.97328  0.097140        0.9689  0.114855\n",
       "41   0.97368  0.095216        0.9698  0.112681\n",
       "42   0.97434  0.093388        0.9699  0.112042\n",
       "43   0.97500  0.091287        0.9696  0.109966\n",
       "44   0.97554  0.089382        0.9696  0.109138\n",
       "45   0.97588  0.087839        0.9712  0.108391\n",
       "46   0.97660  0.085972        0.9714  0.107943\n",
       "47   0.97728  0.084366        0.9708  0.106736\n",
       "48   0.97750  0.082684        0.9709  0.104793\n",
       "49   0.97790  0.080972        0.9719  0.104501"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertimos el histórico a DataFrame para visualizarlo mejor\n",
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DEL PROCESO DE ENTRENAMIENTO\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graficamos todas las métricas (loss, accuracy, val_loss, val_accuracy)\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "\n",
    "# Configuramos la gráfica\n",
    "plt.grid(True)  # Añadimos cuadrícula\n",
    "plt.gca().set_ylim(0, 1)  # Rango vertical de 0 a 1\n",
    "\n",
    "# Interpretación:\n",
    "# - Si val_loss aumenta mientras loss disminuye = OVERFITTING\n",
    "# - Si ambas disminuyen = el modelo está aprendiendo bien\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si el modelo no ha ido bien, prueba a cambiar el learning rate, cambia de optimizador y después prueba a cambiar capas, neuronas y funciones de activación.\n",
    "\n",
    "Ya tenemos el modelo entrenado. Probémoslo con test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9753 - loss: 0.0803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08032266050577164, 0.9753000140190125]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# EVALUACIÓN DEL MODELO CON DATOS DE PRUEBA\n",
    "# ============================================\n",
    "# Evaluamos el modelo con datos que NUNCA ha visto durante el entrenamiento\n",
    "# Esto nos da una idea del rendimiento real del modelo\n",
    "\n",
    "# evaluate() retorna [loss, accuracy]\n",
    "results = model.evaluate(X_test, y_test)\n",
    "results  # [pérdida en test, accuracy en test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\borja\\AppData\\Local\\Temp\\ipykernel_27664\\3012439489.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  plt.imshow(X_test[0].reshape(28, 28), cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DE UN EJEMPLO DE PRUEBA\n",
    "# ============================================\n",
    "# Mostramos la primera imagen del conjunto de prueba\n",
    "plt.imshow(X_test[0].reshape(28, 28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.32941177, 0.7254902 , 0.62352943, 0.5921569 ,\n",
       "         0.23529412, 0.14117648, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.87058824, 0.99607843, 0.99607843, 0.99607843,\n",
       "         0.99607843, 0.94509804, 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 , 0.7764706 ,\n",
       "         0.6666667 , 0.20392157, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.2627451 , 0.44705883, 0.28235295, 0.44705883,\n",
       "         0.6392157 , 0.8901961 , 0.99607843, 0.88235295, 0.99607843,\n",
       "         0.99607843, 0.99607843, 0.98039216, 0.8980392 , 0.99607843,\n",
       "         0.99607843, 0.54901963, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.06666667, 0.25882354, 0.05490196, 0.2627451 ,\n",
       "         0.2627451 , 0.2627451 , 0.23137255, 0.08235294, 0.9254902 ,\n",
       "         0.99607843, 0.41568628, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.3254902 , 0.99215686,\n",
       "         0.81960785, 0.07058824, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.08627451, 0.9137255 , 1.        ,\n",
       "         0.3254902 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.5058824 , 0.99607843, 0.93333334,\n",
       "         0.17254902, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.23137255, 0.9764706 , 0.99607843, 0.24313726,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.73333335, 0.01960784,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.03529412, 0.8039216 , 0.972549  , 0.22745098, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.49411765, 0.99607843, 0.7137255 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "         0.9843137 , 0.9411765 , 0.22352941, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.07450981, 0.8666667 ,\n",
       "         0.99607843, 0.6509804 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.01176471, 0.79607844, 0.99607843,\n",
       "         0.85882354, 0.13725491, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.14901961, 0.99607843, 0.99607843,\n",
       "         0.3019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.12156863, 0.8784314 , 0.99607843, 0.4509804 ,\n",
       "         0.00392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52156866, 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.23921569, 0.9490196 , 0.99607843, 0.99607843, 0.20392157,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.99607843, 0.85882354, 0.15686275,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4745098 , 0.99607843, 0.8117647 , 0.07058824, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vemos los datos crudos de la primera imagen\n",
    "X_test[:1]  # Array con valores normalizados entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "(1, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 0.   , 0.001, 0.   , 0.   , 0.   , 0.999, 0.   ,\n",
       "        0.   ]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# PREDICCIONES DEL MODELO\n",
    "# ============================================\n",
    "# Hacemos una predicción con la primera imagen del test\n",
    "\n",
    "predictions = model.predict(X_test[:1])\n",
    "\n",
    "# Shape de las predicciones: (1, 10)\n",
    "# 1 muestra, 10 probabilidades (una por cada dígito 0-9)\n",
    "print(predictions.shape)\n",
    "\n",
    "# Redondeamos a 3 decimales para ver las probabilidades\n",
    "# La suma de todas las probabilidades debe ser 1 (gracias a softmax)\n",
    "np.round(predictions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(7)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# OBTENER LA CLASE PREDICHA\n",
    "# ============================================\n",
    "# argmax() retorna el índice del valor más alto\n",
    "# Es decir, el dígito con mayor probabilidad\n",
    "predictions.argmax()  # Dígito predicho (0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], shape=(10000,))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# PREDICCIONES PARA TODO EL CONJUNTO DE PRUEBA\n",
    "# ============================================\n",
    "# Obtenemos las predicciones para todas las imágenes del test\n",
    "# axis=1 indica que argmax se aplica a cada fila (cada predicción)\n",
    "model.predict(X_test).argmax(axis=1)  # Array con todos los dígitos predichos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\borja\\AppData\\Local\\Temp\\ipykernel_27664\\3701829453.py:2: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  plt.imshow(X_test[1].reshape(28, 28), cmap=plt.cm.get_cmap('Greys'));\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizamos la segunda imagen del test\n",
    "plt.imshow(X_test[1].reshape(28, 28), cmap=plt.cm.get_cmap('Greys'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 966,    0,    1,    1,    1,    3,    3,    1,    3,    1],\n",
       "       [   0, 1119,    3,    0,    0,    2,    3,    0,    8,    0],\n",
       "       [   3,    1, 1010,    4,    2,    0,    2,    5,    5,    0],\n",
       "       [   0,    0,    3,  991,    0,    5,    0,    3,    5,    3],\n",
       "       [   1,    0,    5,    1,  950,    1,    3,    3,    2,   16],\n",
       "       [   6,    0,    0,    7,    0,  866,    7,    1,    4,    1],\n",
       "       [   4,    2,    0,    0,    3,    6,  938,    0,    5,    0],\n",
       "       [   1,    7,   10,    3,    0,    0,    0,  995,    1,   11],\n",
       "       [   5,    0,    3,    5,    3,    5,    5,    4,  941,    3],\n",
       "       [   3,    4,    1,    8,    8,    1,    1,    3,    3,  977]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# MATRIZ DE CONFUSIÓN\n",
    "# ============================================\n",
    "# Muestra los errores del modelo: qué dígitos confunde con cuáles\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Filas = etiquetas reales, Columnas = predicciones\n",
    "# Diagonal = predicciones correctas\n",
    "confusion_matrix(y_test, model.predict(X_test).argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98       980\n",
      "         1.0       0.99      0.99      0.99      1135\n",
      "         2.0       0.97      0.98      0.98      1032\n",
      "         3.0       0.97      0.98      0.98      1010\n",
      "         4.0       0.98      0.97      0.97       982\n",
      "         5.0       0.97      0.97      0.97       892\n",
      "         6.0       0.98      0.98      0.98       958\n",
      "         7.0       0.98      0.97      0.97      1028\n",
      "         8.0       0.96      0.97      0.96       974\n",
      "         9.0       0.97      0.97      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# REPORTE DE CLASIFICACIÓN\n",
    "# ============================================\n",
    "# Muestra precision, recall, f1-score para cada clase (dígito)\n",
    "# También incluye accuracy global y promedios macro/weighted\n",
    "print(classification_report(y_test, model.predict(X_test).argmax(axis=1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problema de regresión\n",
    "Veamos un ejemplo de cómo aplicar una red neuronal de TensorFlow a un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# CARGA DEL DATASET CALIFORNIA HOUSING\n",
    "# ============================================\n",
    "# Dataset de regresión: predecir precios de casas en California\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Cargamos el dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Convertimos a DataFrame para visualizar mejor\n",
    "df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df['target'] = housing['target']  # Añadimos la columna objetivo (precio)\n",
    "\n",
    "df.head()  # Primeras 5 filas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divimos en train, test y validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIVISIÓN Y NORMALIZACIÓN DE DATOS\n",
    "# ============================================\n",
    "\n",
    "# PRIMERA DIVISIÓN: Train completo (75%) y Test (25%)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data,\n",
    "    housing.target\n",
    ")\n",
    "\n",
    "# SEGUNDA DIVISIÓN: Train (75% de 75%) y Validation (25% de 75%)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full\n",
    ")\n",
    "\n",
    "# ESTANDARIZACIÓN (StandardScaler)\n",
    "# Transforma los datos para tener media=0 y desviación estándar=1\n",
    "# IMPORTANTE: fit_transform solo en train, transform en valid/test\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Aprende y transforma\n",
    "X_valid = scaler.transform(X_valid)      # Solo transforma\n",
    "X_test = scaler.transform(X_test)        # Solo transforma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11610, 8)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificamos el shape de los datos de entrenamiento\n",
    "# Debería mostrar (num_muestras, 8) porque hay 8 características\n",
    "X_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el modelo. Simplemente se compondrá de una hidden layer, a la que le configuramos una capa previa de entrada de 8 neuronas (las features).\n",
    "\n",
    "Se trata de un modelo de regresión, por lo que la capa de salida es una única neurona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m  1/363\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 142ms/step - loss: 7.3833"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\borja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8396 - val_loss: 2.1843\n",
      "Epoch 2/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.6055 - val_loss: 0.4388\n",
      "Epoch 3/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4615 - val_loss: 0.4188\n",
      "Epoch 4/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.4421 - val_loss: 0.3930\n",
      "Epoch 5/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4299 - val_loss: 0.3879\n",
      "Epoch 6/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4208 - val_loss: 0.3852\n",
      "Epoch 7/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4173 - val_loss: 0.3743\n",
      "Epoch 8/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4076 - val_loss: 0.3700\n",
      "Epoch 9/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4030 - val_loss: 0.3706\n",
      "Epoch 10/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4001 - val_loss: 0.3656\n",
      "Epoch 11/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3998 - val_loss: 0.3612\n",
      "Epoch 12/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3937 - val_loss: 0.3572\n",
      "Epoch 13/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.3904 - val_loss: 0.3558\n",
      "Epoch 14/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3901 - val_loss: 0.3673\n",
      "Epoch 15/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3890 - val_loss: 0.3534\n",
      "Epoch 16/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3844 - val_loss: 0.3491\n",
      "Epoch 17/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3821 - val_loss: 0.3520\n",
      "Epoch 18/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.4038 - val_loss: 0.3520\n",
      "Epoch 19/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3791 - val_loss: 0.3464\n",
      "Epoch 20/20\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3785 - val_loss: 0.3517\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# MODELO DE REGRESIÓN CON KERAS\n",
    "# ============================================\n",
    "\n",
    "# Creamos una red neuronal simple para regresión\n",
    "model = keras.models.Sequential([\n",
    "    # CAPA OCULTA\n",
    "    # 30 neuronas con activación ReLU\n",
    "    # input_shape=[8] porque tenemos 8 características\n",
    "    # X_train.shape[1:] es otra forma de especificar las dimensiones\n",
    "    keras.layers.Dense(30, activation='relu',\n",
    "                       input_shape=[8]),\n",
    "    \n",
    "    # CAPA DE SALIDA\n",
    "    # 1 neurona SIN función de activación (regresión lineal)\n",
    "    # Predice un valor continuo (precio de la casa)\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# COMPILACIÓN\n",
    "# loss='mean_squared_error': función de pérdida para regresión\n",
    "# Mide la distancia cuadrática entre predicción y valor real\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=\"sgd\")\n",
    "\n",
    "# ENTRENAMIENTO\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">270</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)             │           \u001b[38;5;34m270\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m31\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">303</span> (1.19 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m303\u001b[0m (1.19 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> (1.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m301\u001b[0m (1.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================\n",
    "# RESUMEN DEL MODELO DE REGRESIÓN\n",
    "# ============================================\n",
    "model.summary()\n",
    "# Total params = (8 inputs × 30 neurons) + 30 bias + (30 × 1) + 1 bias = 271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 862us/step - loss: 0.3652\n",
      "0.3651953935623169\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# EVALUACIÓN DEL MODELO DE REGRESIÓN\n",
    "# ============================================\n",
    "# Calculamos el MSE (Mean Squared Error) en el conjunto de test\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "print(mse_test)  # Cuanto menor sea el MSE, mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.94108367],\n",
       "       [2.6742039 ],\n",
       "       [1.6785305 ],\n",
       "       [0.81581604],\n",
       "       [2.8915496 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# PREDICCIONES DEL MODELO DE REGRESIÓN\n",
    "# ============================================\n",
    "# Predecimos los precios de las primeras 5 casas del test\n",
    "y_pred = model.predict(X_test[:5])\n",
    "y_pred  # Array con los precios predichos (valores continuos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelo\n",
    "Para guardar el modelo, en el formato de Keras (HDF5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GUARDAR EL MODELO\n",
    "# ============================================\n",
    "# Guardamos el modelo completo (arquitectura + pesos + configuración)\n",
    "# Formato .keras es el recomendado (antes se usaba .h5)\n",
    "model.save(\"my_keras_model.keras\")\n",
    "model.save(\"my_keras_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGAR UN MODELO GUARDADO\n",
    "# ============================================\n",
    "# Cargamos el modelo completo desde el archivo\n",
    "# Podemos usar este modelo directamente para predicciones\n",
    "model = keras.models.load_model(\"my_keras_model.keras\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks\n",
    "Son funciones predefinidas de Keras a aplicar durante el entrenamiento\n",
    "Por ejemplo, `ModelCheckpoint` sirve para que el modelo se vaya guardando tras cada epoch. Así no perdemos el progreso en caso de que decidamos interrumpir el entrenamiento. El callback recibe como argumento el nombre del objeto donde queremos que se guarde el modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m332/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 768us/step - loss: 0.3927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 852us/step - loss: 0.3776\n",
      "Epoch 2/30\n",
      "\u001b[1m324/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 0.3688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - loss: 0.3740\n",
      "Epoch 3/30\n",
      "\u001b[1m306/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 826us/step - loss: 0.3813"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 0.3744\n",
      "Epoch 4/30\n",
      "\u001b[1m311/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 814us/step - loss: 0.3813"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 935us/step - loss: 0.3737\n",
      "Epoch 5/30\n",
      "\u001b[1m349/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3626"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3717\n",
      "Epoch 6/30\n",
      "\u001b[1m360/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 0.3847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 0.3719\n",
      "Epoch 7/30\n",
      "\u001b[1m358/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 847us/step - loss: 0.3681"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 0.3678\n",
      "Epoch 8/30\n",
      "\u001b[1m302/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 0.3744"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940us/step - loss: 0.3777\n",
      "Epoch 9/30\n",
      "\u001b[1m320/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 0.3800"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - loss: 0.3699\n",
      "Epoch 10/30\n",
      "\u001b[1m358/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 851us/step - loss: 0.3582"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 0.3639\n",
      "Epoch 11/30\n",
      "\u001b[1m321/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 0.3957"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 878us/step - loss: 0.3639\n",
      "Epoch 12/30\n",
      "\u001b[1m314/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 0.3822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step - loss: 0.3580\n",
      "Epoch 13/30\n",
      "\u001b[1m299/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 846us/step - loss: 0.3436"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 953us/step - loss: 0.3612\n",
      "Epoch 14/30\n",
      "\u001b[1m312/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 811us/step - loss: 0.3596"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 927us/step - loss: 0.3570\n",
      "Epoch 15/30\n",
      "\u001b[1m315/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 0.3574"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 926us/step - loss: 0.3581\n",
      "Epoch 16/30\n",
      "\u001b[1m316/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 802us/step - loss: 0.3510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 895us/step - loss: 0.3535\n",
      "Epoch 17/30\n",
      "\u001b[1m306/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 0.3433"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 947us/step - loss: 0.3542\n",
      "Epoch 18/30\n",
      "\u001b[1m321/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 789us/step - loss: 0.3399"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911us/step - loss: 0.3485\n",
      "Epoch 19/30\n",
      "\u001b[1m323/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 0.3340"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 908us/step - loss: 0.3473\n",
      "Epoch 20/30\n",
      "\u001b[1m302/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 842us/step - loss: 0.3505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 0.3460\n",
      "Epoch 21/30\n",
      "\u001b[1m356/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 855us/step - loss: 0.3482"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 0.3443\n",
      "Epoch 22/30\n",
      "\u001b[1m342/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 893us/step - loss: 0.3834"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 0.3697\n",
      "Epoch 23/30\n",
      "\u001b[1m303/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 843us/step - loss: 0.3398"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 954us/step - loss: 0.3417\n",
      "Epoch 24/30\n",
      "\u001b[1m304/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 833us/step - loss: 0.3499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 922us/step - loss: 0.3409\n",
      "Epoch 25/30\n",
      "\u001b[1m336/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 907us/step - loss: 0.3342"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.3411  \n",
      "Epoch 26/30\n",
      "\u001b[1m353/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 872us/step - loss: 0.3381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 0.3386\n",
      "Epoch 27/30\n",
      "\u001b[1m362/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 839us/step - loss: 0.3205"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 0.3390\n",
      "Epoch 28/30\n",
      "\u001b[1m306/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 830us/step - loss: 0.3430"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 956us/step - loss: 0.3406\n",
      "Epoch 29/30\n",
      "\u001b[1m318/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 803us/step - loss: 0.3280"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 924us/step - loss: 0.3351\n",
      "Epoch 30/30\n",
      "\u001b[1m341/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 900us/step - loss: 0.3253"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 0.3385\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CALLBACK: MODEL CHECKPOINT\n",
    "# ============================================\n",
    "# Guarda el modelo automáticamente después de cada epoch\n",
    "# Útil para no perder progreso si se interrumpe el entrenamiento\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"callback_model.h5\")\n",
    "\n",
    "# El modelo se guardará en \"callback_model.h5\" tras cada epoch\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    callbacks=[checkpoint_cb])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Interrumpe el entrenamiento cuando no ve progreso en el set de validación. Para ello tiene en cuenta un numero de epochs llamado `patience`. Se puede combinar con el callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CALLBACK: EARLY STOPPING\n",
    "# ============================================\n",
    "# Detiene el entrenamiento automáticamente cuando no hay mejora\n",
    "# Previene overfitting y ahorra tiempo de entrenamiento\n",
    "\n",
    "# patience=3: espera 3 epochs sin mejora antes de detener\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=3)\n",
    "\n",
    "# Podemos combinar múltiples callbacks en una lista\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=50,  # Máximo 50 epochs\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    # Se detendrá antes si val_loss no mejora durante 3 epochs\n",
    "                    callbacks=[early_stopping_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
