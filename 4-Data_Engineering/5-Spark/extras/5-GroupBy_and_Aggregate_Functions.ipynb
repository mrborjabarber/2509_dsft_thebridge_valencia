{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# GroupBy y Funciones de Agregación\n\nAprenderemos cómo usar los métodos GroupBy y Aggregate en un DataFrame. GroupBy te permite agrupar filas basándote en el valor de alguna columna. Por ejemplo, podrías agrupar datos de ventas por el día en que ocurrió la venta, o agrupar datos de clientes repetidos basándote en el nombre del cliente. Una vez que has realizado la operación GroupBy, puedes usar una función de agregación sobre esos datos. Una función de agregación agrega múltiples filas de datos en una única salida, como sumar las entradas o contar el número de entradas.\n\n¡Veamos algunos ejemplos con un conjunto de datos de prueba!",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4a3587d8-6d21-43ea-bbb3-580b92d0e7c3",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Listamos los archivos disponibles en el sistema de archivos de Databricks\n# dbutils.fs.ls() nos permite explorar directorios en DBFS (Databricks File System)\ndisplay(dbutils.fs.ls(\"dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/data-001\"))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1ad5a788-32ca-43d6-a27f-d310e97da906",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Leemos un archivo CSV desde DBFS y lo cargamos en un DataFrame de Spark\n# header=True indica que la primera fila contiene los nombres de las columnas\n# inferSchema=True permite a Spark inferir automáticamente los tipos de datos de cada columna\ndf = spark.read.csv(\"dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/data-001\", header = True, inferSchema=True)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3c7e4586-f1e5-4c7e-9965-1a226e5bb0f7",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Mostramos el contenido del DataFrame en formato tabular\n# display() es específico de Databricks y proporciona una visualización mejorada\ndisplay(df)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "65976198-89d8-4dcd-90b9-12f7e4f44362",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cargamos un archivo CSV específico (market_data.csv) del directorio de mercados de agricultores\n# Esta vez especificamos la ruta completa al archivo CSV\ndf = spark.read.csv(\"dbfs:/databricks-datasets/data.gov/farmers_markets_geographic_data/data-001/market_data.csv\", header = True, inferSchema=True)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e5c362d1-701c-492c-a477-3a81f2c506fc",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Visualizamos el DataFrame de datos de mercado\ndisplay(df)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "d3ba38c6-913a-4535-8426-e79c85b031d6",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Exploramos otro dataset: datos de compartición de bicicletas\n# Listamos el contenido del directorio bikeSharing\ndisplay(dbutils.fs.ls(\"dbfs:/databricks-datasets/bikeSharing/data-001\"))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "61c17cdd-2b62-4a68-ac5b-33d065741b68",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cargamos el dataset de compartición de bicicletas por día (day.csv)\n# Este dataset contiene información agregada por día sobre el uso de bicicletas compartidas\ndf = spark.read.csv(\"dbfs:/databricks-datasets/bikeSharing/data-001/day.csv\", header = True, inferSchema = True)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "45414582-1201-467d-bc85-29d89d37bb4d",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Mostramos el DataFrame de bicicletas compartidas\ndisplay(df)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3e69ceb5-a695-48f7-a5a5-9a0decfbc98c",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Imprimimos el esquema del DataFrame para ver la estructura de los datos\n# printSchema() muestra el nombre de cada columna, su tipo de dato y si acepta valores nulos\ndf.printSchema()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1bdad812-5530-4370-baf6-89f1837a61c2",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Leemos los datos de ventas de clientes",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1eb0a2d4-2afe-457a-a3b0-bba67ff2d2c4",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "¡Agrupemos los datos por día laborable (workingday)!",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b4c2d862-2604-4842-8f3a-9cf6335fa3fa",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Agrupamos el DataFrame por la columna \"workingday\"\n# Esto crea un objeto GroupedData que nos permite aplicar funciones de agregación\ndf.groupBy(\"workingday\")",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "485fd047-0674-4104-aa86-759ae72d6614",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Esto devuelve un objeto GroupedData, sobre el cual puedes llamar varios métodos de agregación",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8a458995-b66b-4f17-97b2-6d0753314fb2",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Calculamos el promedio de humedad (hum) agrupado por día laborable\n# avg() calcula el valor promedio de la columna especificada para cada grupo\n# show() muestra los resultados en consola\ndf.groupBy(\"workingday\").avg(\"hum\").show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8d01c862-b4eb-4405-a70c-748cc10ca9a0",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Contamos cuántos registros hay para cada día de la semana (Weekday)\n# count() devuelve el número de filas en cada grupo\ndf.groupBy(\"Weekday\").count().show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2fb2a3e0-9bd8-4023-bbfd-abafb2035cb4",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Calculamos el valor máximo de todas las columnas numéricas agrupadas por mes (mnth)\n# max() encuentra el valor máximo en cada columna para cada grupo\n# take(4) solo toma las primeras 4 filas del resultado\ndisplay(df.groupBy(\"mnth\").max().take(4))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "825ff8d5-74a9-4542-b5c8-838f5103a293",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Calculamos el valor mínimo de todas las columnas numéricas agrupadas por mes\n# min() encuentra el valor mínimo en cada columna para cada grupo\ndisplay(df.groupBy(\"mnth\").min().take(4))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3d2a1eb9-cd9f-43a1-8f1e-4311c003cc52",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Calculamos la suma de todas las columnas numéricas agrupadas por día festivo (holiday)\n# sum() suma todos los valores en cada columna para cada grupo\n# holiday: 0 = día no festivo, 1 = día festivo\ndisplay(df.groupBy(\"holiday\").sum().take(2))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "95d92b89-4557-4a29-bc17-a1335e3e6969",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2aca35bd-7219-48ba-8318-54b26edafa48",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": "Consulta este enlace para más información sobre otros métodos:\nhttp://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark-sql-module\n\nNo todos los métodos necesitan una llamada a groupby, en su lugar puedes simplemente llamar al método generalizado .agg(), que llamará al agregado en todas las filas del DataFrame para la columna especificada. Puede tomar argumentos como una sola columna, o crear múltiples llamadas de agregación a la vez usando notación de diccionario.\n\nPor ejemplo:",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "da07af4e-7d9c-4d59-8ba0-b5e7fb121326",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Calculamos el valor máximo de humedad en todo el DataFrame (sin agrupar)\n# agg() permite aplicar funciones de agregación sin necesidad de groupBy\n# {\"hum\":\"max\"} es notación de diccionario: columna -> función de agregación\ndf.agg({\"hum\":\"max\"}).show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "55a8bad6-2503-4016-8719-ee461bc33cac",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# También podríamos haber hecho esto en el objeto groupBy:",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "659ac75f-2fd5-4f2e-8a34-68e11521e275",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Creamos un objeto agrupado por día laborable que reutilizaremos\ngrouped = df.groupBy(\"workingday\")",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1c3a13f7-6a88-4d1c-89ae-dd4fac0a9788",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Aplicamos la función de agregación max sobre la columna \"hum\" en el objeto agrupado\n# Esto nos da el valor máximo de humedad para cada tipo de día (laborable o no)\ngrouped.agg({\"hum\":\"max\"}).show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "47cd8eb8-a0fa-4c61-93ef-858376e66bb1",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Importamos funciones de agregación desde pyspark.sql.functions\n# Esto nos permite usar funciones más específicas como F.min() y F.max()\nfrom pyspark.sql import functions as F\n\n# Aplicamos múltiples funciones de agregación a la vez\n# Calculamos tanto el mínimo como el máximo de humedad para cada grupo\ngrouped.agg(F.min(\"hum\"), F.max(\"hum\")).show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4b33f437-038d-4d8d-9d09-cc654ce1a212",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Convertimos el DataFrame de Spark a un DataFrame de Pandas\n# Pandas es útil para análisis y visualización en memoria con datasets pequeños\n# IMPORTANTE: Solo usa esto con datos que caben en memoria\npandas_df = df.toPandas()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "22792d13-5d83-495f-a54a-c9ca4ce1e816",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Mostramos las primeras 5 filas del DataFrame de Pandas\n# head() es un método estándar de Pandas\npandas_df.head()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "cf35212b-43a7-44d2-b3d3-304832b60ca0",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Funciones\nHay una variedad de funciones que puedes importar desde pyspark.sql.functions. Consulta la documentación para ver la lista completa disponible:\nhttp://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "c983bf47-7aba-42e0-86fb-c7f86f925f02",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Importamos funciones especializadas para cálculos estadísticos\n# countDistinct: cuenta valores únicos\n# avg: calcula el promedio\n# stddev: calcula la desviación estándar\n# Estas funciones son óptimas para Big Data porque están diseñadas para trabajar con particiones distribuidas\nfrom pyspark.sql.functions import countDistinct, avg, stddev",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5ee65807-54cc-418d-b404-a2dbf0789b3d",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Contamos cuántos valores distintos (únicos) hay en la columna \"registered\"\n# countDistinct() elimina duplicados y cuenta solo valores únicos\ndf.select(countDistinct(\"registered\")).show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "997c2b7b-2e29-4951-9c43-c6af127cc80e",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "A menudo querrás cambiar el nombre de la columna resultante, usa el método .alias() para esto:",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6b82422b-365a-4a4d-aaea-062143146806",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Renombramos la columna del resultado usando alias()\n# Esto hace que la salida sea más legible y descriptiva\ndf.select(countDistinct(\"registered\").alias(\"registros_unicos\")).show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2154a2ac-04f9-4370-afb5-0b9d62d59bf8",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Calculamos el promedio (media) de la columna \"hum\" (humedad)\n# avg() suma todos los valores y los divide por el número de filas\ndf.select(avg(\"hum\")).show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "6cf4c522-f085-4716-ac6a-431247067225",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Calculamos la desviación estándar de la columna \"hum\"\n# stddev() mide la dispersión de los datos respecto a la media\n# Es útil para entender la variabilidad de los datos\ndf.select(stddev(\"hum\")).show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a4768040-ffcf-47f6-b36e-f07c789ffc74",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "¡Eso es mucha precisión en los dígitos! Usemos format_number para arreglar eso!",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "3b979119-1086-4dd0-943c-12e69d4a2bed",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Importamos format_number para formatear números con decimales específicos\nfrom pyspark.sql.functions import format_number",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a8ba57ec-c06d-4df9-927a-42c5e4b0d0fb",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Calculamos la desviación estándar y la guardamos en un DataFrame\n# Le damos el alias \"std\" para simplificar el nombre de la columna\nhum_std = df.select(stddev(\"hum\").alias(\"std\"))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "7875846c-108d-47e5-91ac-53d5df627132",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Verificamos el tipo de objeto que hemos creado\n# Sigue siendo un DataFrame de PySpark, no un valor numérico simple\ntype(hum_std)",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2730e9f7-dc7c-4d6f-8787-ca5733e5f7a0",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Mostramos el DataFrame con la desviación estándar\nhum_std.show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1a162193-0d32-4613-8276-df4bb6f6c661",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Formateamos el número para mostrar solo 2 decimales\n# format_number(columna, número_de_decimales)\n# Esto hace que los resultados sean más legibles\nhum_std.select(format_number(\"std\", 2)).show()",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e4eb476a-c5e5-4e56-8756-4663f71a43f2",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Ordenar (Order By)\n\nPuedes ordenar fácilmente con el método orderBy:",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "2f9010f2-20e3-44aa-8be3-abab2e1ca845",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": "# Ordenamos el DataFrame por la columna \"registered\" en orden ascendente (por defecto)\n# Los valores más pequeños aparecen primero\ndisplay(df.orderBy(\"registered\"))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "5e066d4b-c41d-480b-8c81-5d640639bbf6",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Ordenamos el DataFrame en orden descendente usando .desc()\n# Los valores más grandes aparecen primero\ndisplay(df.orderBy(df[\"registered\"].desc()))",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1edbd90f-6526-40e8-bc2a-1c51a5cca4c0",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "La mayoría de las funciones básicas que esperarías están disponibles, ¡así que asegúrate de consultar la documentación!",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "86d748ad-b30d-422c-953d-7623b9c33e06",
     "inputWidgets": {},
     "title": ""
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "5-GroupBy_and_Aggregate_Functions",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 834199842432380
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}