{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Completa de LangChain: De Fundamentos a Sistemas Multi-Agente\n",
    "\n",
    "**Instructor:** Borja Barber  \n",
    "**Fecha:** 2025  \n",
    "**Duracion:** 3-4 horas\n",
    "\n",
    "---\n",
    "\n",
    "## Tabla de Contenidos\n",
    "\n",
    "1. [Introduccion a LangChain](#1)\n",
    "2. [Instalacion y Configuracion](#2)\n",
    "3. [Fundamentos: Modelos y Prompts](#3)\n",
    "4. [Cadenas (Chains) y LCEL](#4)\n",
    "5. [Herramientas Personalizadas](#5)\n",
    "6. [Sistemas Multi-Agente con LangGraph](#6)\n",
    "7. [Proyecto Final: Equipo de Data Science Multi-Agente](#7)\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "\n",
    "Al finalizar esta clase, seras capaz de:\n",
    "\n",
    "- Comprender la arquitectura modular de LangChain\n",
    "- Crear prompts dinamicos y cadenas de procesamiento\n",
    "- Implementar herramientas personalizadas\n",
    "- Construir sistemas multi-agente con LangGraph\n",
    "- Aplicar LangChain a casos de uso de Data Science\n",
    "\n",
    "---\n",
    "\n",
    "## Referencias\n",
    "\n",
    "- [Documentacion oficial de LangChain](https://python.langchain.com/)\n",
    "- [LangGraph Multi-Agent Workflows](https://blog.langchain.com/langgraph-multi-agent-workflows/)\n",
    "- [Multi-Agent Tutorial LangGraph](https://blog.futuresmart.ai/multi-agent-system-with-langgraph)\n",
    "- [LangChain for Data Science](https://towardsdatascience.com/langchain-for-eda-build-a-csv-sanity-check-agent-in-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. Introduccion a LangChain <a id='1'></a>\n",
    "\n",
    "## Que es LangChain?\n",
    "\n",
    "**LangChain** es un framework de codigo abierto disenado para construir aplicaciones potenciadas por **Modelos de Lenguaje Grande (LLMs)**.\n",
    "\n",
    "### Conceptos Clave\n",
    "\n",
    "1. **Integracion**: Conecta LLMs con fuentes de datos externas (APIs, bases de datos, archivos)\n",
    "2. **Orquestacion**: Encadena multiples llamadas a LLMs de forma estructurada\n",
    "3. **Agencia**: Permite que los LLMs tomen decisiones y usen herramientas\n",
    "\n",
    "### Arquitectura Modular (2025)\n",
    "\n",
    "LangChain se ha dividido en paquetes especializados:\n",
    "\n",
    "```\n",
    "langchain-core       → Abstracciones base\n",
    "langchain-openai     → Integracion con OpenAI\n",
    "langchain-community  → Integraciones comunitarias\n",
    "langchain            → Cadenas y agentes de alto nivel\n",
    "langgraph            → Sistemas multi-agente con grafos\n",
    "```\n",
    "\n",
    "### Por que usar LangChain?\n",
    "\n",
    "- **Abstraccion**: Simplifica la complejidad de trabajar con LLMs\n",
    "- **Componibilidad**: Construye aplicaciones complejas con bloques simples\n",
    "- **Comunidad**: Ecosistema activo con miles de integraciones\n",
    "- **Flexibilidad**: Compatible con multiples proveedores de LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Instalacion y Configuracion <a id='2'></a>\n",
    "\n",
    "## Instalacion de Dependencias\n",
    "\n",
    "Vamos a instalar todos los paquetes necesarios para esta clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las dependencias instaladas correctamente\n"
     ]
    }
   ],
   "source": [
    "# Instalacion de paquetes principales de LangChain\n",
    "# Ejecuta esta celda primero antes de continuar con el resto del notebook\n",
    "\n",
    "# Paquetes core de LangChain\n",
    "# %pip install -q langchain\n",
    "# %pip install -q langchain-core\n",
    "# %pip install -q langchain-openai\n",
    "# %pip install -q langchain-community\n",
    "\n",
    "# Paquetes para multi-agente\n",
    "# %pip install -q langgraph\n",
    "\n",
    "# Utilidades\n",
    "# %pip install -q python-dotenv\n",
    "# %pip install -q openai\n",
    "\n",
    "# Para analisis de datos\n",
    "# %pip install -q pandas\n",
    "# %pip install -q matplotlib\n",
    "# %pip install -q seaborn\n",
    "# %pip install -q tabulate\n",
    "\n",
    "print(\"Todas las dependencias instaladas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion de API Keys\n",
    "\n",
    "Para usar LangChain necesitaras una **API Key de OpenAI**. \n",
    "\n",
    "### Opciones para configurar tu API Key:\n",
    "\n",
    "1. **Archivo `.env`** (Recomendado para desarrollo):\n",
    "   ```\n",
    "   OPENAI_API_KEY=sk-tu-clave-aqui\n",
    "   ```\n",
    "\n",
    "2. **Variable de entorno del sistema**:\n",
    "   ```bash\n",
    "   export OPENAI_API_KEY='sk-tu-clave-aqui'\n",
    "   ```\n",
    "\n",
    "3. **Directamente en el codigo** (Solo para pruebas):\n",
    "   ```python\n",
    "   openai_api_key = 'sk-tu-clave-aqui'\n",
    "   ```\n",
    "\n",
    "### Como obtener tu API Key:\n",
    "\n",
    "1. Visita [platform.openai.com](https://platform.openai.com/)\n",
    "2. Crea una cuenta o inicia sesion\n",
    "3. Ve a **API Keys** en tu perfil\n",
    "4. Genera una nueva clave secreta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key configurada correctamente (termina en: ...ycIA)\n"
     ]
    }
   ],
   "source": [
    "# Configuracion de variables de entorno y API Keys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar variables de entorno desde archivo .env (si existe)\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la API Key de OpenAI\n",
    "# Si no esta en .env, reemplaza 'YourAPIKey' con tu clave real\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'YourAPIKey')\n",
    "\n",
    "# Validar que la API Key esta configurada\n",
    "if OPENAI_API_KEY == 'YourAPIKey' or not OPENAI_API_KEY:\n",
    "    print(\"ADVERTENCIA: OpenAI API Key no configurada\")\n",
    "    print(\"Por favor, configura tu API Key en el archivo .env\")\n",
    "else:\n",
    "    # Mostrar solo los ultimos 4 caracteres por seguridad\n",
    "    print(f\"API Key configurada correctamente (termina en: ...{OPENAI_API_KEY[-4:]})\")\n",
    "    \n",
    "# Configurar la clave en el entorno para que LangChain la use automaticamente\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 3. Fundamentos: Modelos y Prompts <a id='3'></a>\n",
    "\n",
    "## 3.1 Modelos de Lenguaje (LLMs)\n",
    "\n",
    "Los **LLMs** son el corazon de LangChain. Hay dos tipos principales:\n",
    "\n",
    "### LLMs vs Chat Models\n",
    "\n",
    "| Caracteristica | LLM | Chat Model |\n",
    "|---------------|-----|------------|\n",
    "| **Entrada** | String | Lista de mensajes |\n",
    "| **Salida** | String | Mensaje (AIMessage) |\n",
    "| **Uso** | Texto simple | Conversaciones |\n",
    "| **Ejemplo** | gpt-3.5-turbo-instruct | gpt-4o-mini |\n",
    "\n",
    "### Modelos Recomendados (2025)\n",
    "\n",
    "- **gpt-4o-mini**: Rapido, economico, ideal para la mayoria de tareas\n",
    "- **gpt-4o**: Mas potente, mejor razonamiento\n",
    "- **gpt-3.5-turbo-instruct**: LLM tradicional (texto a texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: Que es Machine Learning?\n",
      "Respuesta: \n",
      "\n",
      "El Machine Learning es una rama de la inteligencia artificial que permite a las máquinas aprender y mejorar a partir de datos y experiencias anteriores sin ser explícitamente programadas.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 1: LLM Tradicional (Texto a Texto)\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Crear instancia del modelo LLM\n",
    "# temperature: controla la creatividad (0 = determinista, 1 = creativo)\n",
    "llm = OpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Invocar el modelo con un prompt simple\n",
    "respuesta = llm.invoke(\"Explica que es Machine Learning en una frase.\")\n",
    "\n",
    "print(\"Pregunta: Que es Machine Learning?\")\n",
    "print(f\"Respuesta: {respuesta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta: Diferencia entre regresion y clasificacion?\n",
      "Respuesta: La regresión y la clasificación son dos tipos de problemas en el ámbito del aprendizaje supervisado en Machine Learning, y se diferencian principalmente en la naturaleza de la variable objetivo que intentan predecir.\n",
      "\n",
      "1. **Regresión**:\n",
      "   - **Objetivo**: Predecir un valor numérico continuo.\n",
      "   - **Ejemplo**: Predecir el precio de una casa basado en sus características (tamaño, ubicación, número de habitaciones, etc.).\n",
      "   - **Algoritmos comunes**: Regresión lineal, regresión polinómica, regresión de soporte vectorial (SVR), entre otros.\n",
      "\n",
      "2. **Clasificación**:\n",
      "   - **Objetivo**: Predecir una etiqueta o categoría discreta.\n",
      "   - **Ejemplo**: Clasificar correos electrónicos como \"spam\" o \"no spam\".\n",
      "   - **Algoritmos comunes**: Regresión logística, árboles de decisión, máquinas de soporte vectorial (SVM), redes neuronales, entre otros.\n",
      "\n",
      "En resumen, la regresión se utiliza para problemas donde la salida es un valor continuo, mientras que la clasificación se utiliza cuando la salida es una categoría o clase.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 2: Chat Model (Mensajes a Mensaje)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# Crear instancia del modelo de chat\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Crear una conversacion con contexto\n",
    "# SystemMessage: Define el rol y comportamiento del asistente\n",
    "# HumanMessage: Representa la pregunta o entrada del usuario\n",
    "mensajes = [\n",
    "    SystemMessage(content=\"Eres un experto en Data Science que explica conceptos de forma clara y concisa.\"),\n",
    "    HumanMessage(content=\"Cual es la diferencia entre regresion y clasificacion?\")\n",
    "]\n",
    "\n",
    "# Invocar el modelo\n",
    "respuesta = chat.invoke(mensajes)\n",
    "\n",
    "print(\"Pregunta: Diferencia entre regresion y clasificacion?\")\n",
    "print(f\"Respuesta: {respuesta.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Prompt Templates\n",
    "\n",
    "Los **Prompt Templates** permiten crear prompts dinamicos y reutilizables.\n",
    "\n",
    "### Ventajas:\n",
    "\n",
    "- Reutilizacion de prompts\n",
    "- Variables dinamicas\n",
    "- Mejor organizacion del codigo\n",
    "- Facilita el testing y debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Generado:\n",
      "============================================================\n",
      "\n",
      "Eres un asistente de Data Science experto.\n",
      "\n",
      "Tarea: Realizar analisis exploratorio de datos (EDA)\n",
      "Dataset: ventas_mensuales.csv con 10,000 registros\n",
      "Contexto adicional: El cliente quiere identificar patrones estacionales\n",
      "\n",
      "Por favor, proporciona una respuesta clara y tecnica.\n",
      "\n",
      "============================================================\n",
      "\n",
      "Respuesta del LLM:\n",
      "\n",
      "Respuesta:\n",
      "\n",
      "El análisis exploratorio de datos (EDA) es una técnica utilizada en el campo de Data Science para comprender y explorar un conjunto de datos de manera detallada, con el fin de identificar patrones, tendencias y relaciones entre variables. En este caso, se utilizará el conjunto de datos \"ventas_mensuales.csv\" que contiene 10,000 registros de ventas mensuales.\n",
      "\n",
      "El primer paso en el EDA es revisar la estructura de los datos, es decir, el número de variables y observaciones. En este cas\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 3: Prompt Templates Simples\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Definir un template con variables entre llaves {}\n",
    "# Las variables seran reemplazadas con valores reales al formatear\n",
    "template = \"\"\"\n",
    "Eres un asistente de Data Science experto.\n",
    "\n",
    "Tarea: {tarea}\n",
    "Dataset: {dataset}\n",
    "Contexto adicional: {contexto}\n",
    "\n",
    "Por favor, proporciona una respuesta clara y tecnica.\n",
    "\"\"\"\n",
    "\n",
    "# Crear el PromptTemplate especificando las variables de entrada\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"tarea\", \"dataset\", \"contexto\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Formatear el prompt reemplazando las variables con valores especificos\n",
    "prompt_formateado = prompt.format(\n",
    "    tarea=\"Realizar analisis exploratorio de datos (EDA)\",\n",
    "    dataset=\"ventas_mensuales.csv con 10,000 registros\",\n",
    "    contexto=\"El cliente quiere identificar patrones estacionales\"\n",
    ")\n",
    "\n",
    "print(\"Prompt Generado:\")\n",
    "print(\"=\"*60)\n",
    "print(prompt_formateado)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Usar el prompt con el LLM\n",
    "respuesta = llm.invoke(prompt_formateado)\n",
    "print(\"\\nRespuesta del LLM:\")\n",
    "print(respuesta[:500])  # Mostrar solo los primeros 500 caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mensajes Generados:\n",
      "\n",
      "1. SystemMessage:\n",
      "   Eres un Data Scientist especializado en analisis de series temporales.\n",
      "\n",
      "2. HumanMessage:\n",
      "   Necesito ayuda con: predecir ventas futuras\n",
      "\n",
      "3. HumanMessage:\n",
      "   Contexto adicional: tengo datos historicos de 3 anios con estacionalidad\n",
      "\n",
      "Respuesta:\n",
      "Para predecir ventas futuras utilizando datos históricos con estacionalidad, puedes seguir estos pasos:\n",
      "\n",
      "### 1. **Exploración de Datos**\n",
      "   - **Visualización**: Grafica tus datos históricos para identificar patrones, tendencias y estacionalidades. Puedes utilizar gráficos de líneas y descomposición de series temporales.\n",
      "   - **Estadísticas Descriptivas**: Revisa medidas como la media, mediana, desviación estándar, etc.\n",
      "\n",
      "### 2. **Descomposición de la Serie Temporal**\n",
      "   - Utiliza métodos como la \n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 4: Chat Prompt Templates\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Crear un template para chat con multiples mensajes\n",
    "# Cada tupla representa (tipo_mensaje, contenido_con_variables)\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un {rol} especializado en {especialidad}.\"),\n",
    "    (\"human\", \"Necesito ayuda con: {problema}\"),\n",
    "    (\"human\", \"Contexto adicional: {contexto}\")\n",
    "])\n",
    "\n",
    "# Formatear el prompt con valores especificos\n",
    "mensajes = chat_template.format_messages(\n",
    "    rol=\"Data Scientist\",\n",
    "    especialidad=\"analisis de series temporales\",\n",
    "    problema=\"predecir ventas futuras\",\n",
    "    contexto=\"tengo datos historicos de 3 anios con estacionalidad\"\n",
    ")\n",
    "\n",
    "print(\"Mensajes Generados:\")\n",
    "for i, msg in enumerate(mensajes, 1):\n",
    "    print(f\"\\n{i}. {msg.__class__.__name__}:\")\n",
    "    print(f\"   {msg.content}\")\n",
    "\n",
    "# Invocar el chat model con los mensajes generados\n",
    "respuesta = chat.invoke(mensajes)\n",
    "print(\"\\nRespuesta:\")\n",
    "print(respuesta.content[:500])  # Mostrar solo los primeros 500 caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 4. Cadenas (Chains) y LCEL <a id='4'></a>\n",
    "\n",
    "## Que son las Cadenas?\n",
    "\n",
    "Las **Chains** (cadenas) permiten combinar multiples componentes (prompts, LLMs, procesadores) en un flujo secuencial.\n",
    "\n",
    "## LCEL: LangChain Expression Language\n",
    "\n",
    "**LCEL** es el metodo moderno (2025) para crear cadenas usando el operador `|` (pipe).\n",
    "\n",
    "### Ventajas de LCEL:\n",
    "\n",
    "- **Sintaxis clara**: Similar a pipes en Unix/Linux\n",
    "- **Composicion**: Facil de combinar componentes\n",
    "- **Streaming**: Soporte nativo para respuestas en tiempo real\n",
    "- **Paralelizacion**: Ejecuta componentes en paralelo automaticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado de la Cadena:\n",
      "En el ámbito de Data Science en Python, las tres bibliotecas más importantes son:\n",
      "\n",
      "1. **Pandas**: Es la biblioteca fundamental para la manipulación y análisis de datos. Proporciona estructuras de datos como DataFrames y Series, que facilitan la limpieza, transformación y análisis de datos. Permite manejar datos tabulares de manera eficiente y realizar operaciones como filtrado, agrupamiento y fusión de conjuntos de datos.\n",
      "\n",
      "2. **NumPy**: Es la biblioteca básica para la computación numérica en Python. Proporciona soporte para arreglos multidimensionales (ndarrays) y funciones matemáticas de alto rendimiento que permiten realizar cálculos complejos de manera eficiente. Es fundamental para la manipulación de datos numéricos y sirve como base para muchas otras bibliotecas de Data Science.\n",
      "\n",
      "3. **Matplotlib**: Es una biblioteca de visualización de datos que permite crear gráficos estáticos, animados e interactivos en Python. Es muy versátil y se utiliza para representar datos de diferentes maneras, desde gráficos de líneas y dispersión hasta histogramas y gráficos de barras. Junto con otras bibliotecas como Seaborn, que se basa en Matplotlib, permite realizar visualizaciones más sofisticadas.\n",
      "\n",
      "Estas bibliotecas son esenciales para cualquier proyecto de Data Science en Python, ya que abarcan las áreas de manipulación de datos, computación numérica y visualización.\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 5: Cadena Simple con LCEL\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Paso 1: Crear el prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un experto en {tema}.\"),\n",
    "    (\"human\", \"{pregunta}\")\n",
    "])\n",
    "\n",
    "# Paso 2: Crear el modelo\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "\n",
    "# Paso 3: Crear el parser de salida\n",
    "# StrOutputParser convierte AIMessage a string simple\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Paso 4: Construir la cadena usando el operador | (pipe)\n",
    "# El flujo es: prompt -> model -> output_parser\n",
    "# Los datos fluyen de izquierda a derecha\n",
    "cadena = prompt | model | output_parser\n",
    "\n",
    "# Paso 5: Invocar la cadena con parametros\n",
    "resultado = cadena.invoke({\n",
    "    \"tema\": \"Python para Data Science\",\n",
    "    \"pregunta\": \"Cuales son las 3 bibliotecas mas importantes?\"\n",
    "})\n",
    "\n",
    "print(\"Resultado de la Cadena:\")\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paso 1: Generando pregunta analitica...\n",
      "Pregunta: Claro, aquí tienes una pregunta analítica que podrías considerar para explorar el dataset de ventas por región y producto:\n",
      "\n",
      "**¿Cuál es la relación entre las ventas de diferentes productos en cada regi...\n",
      "\n",
      "Paso 2: Sugiriendo metodo de analisis...\n",
      "Metodo: Para abordar la pregunta analítica sobre la relación entre las ventas de diferentes productos en cada región y las tendencias a lo largo del tiempo, puedes seguir un enfoque estructurado que incluya v...\n",
      "\n",
      "Paso 3: Generando codigo Python...\n",
      "Codigo:\n",
      "```python\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from statsmodels.tsa.seasonal import seasonal_decompose\n",
      "from sklearn.linear_model import LinearRegression\n",
      "import numpy as np\n",
      "\n",
      "# Cargar los datos\n",
      "data = pd.read_csv('ventas.csv')\n",
      "\n",
      "# 1. Análisis Descriptivo\n",
      "summary = data.groupby(['Producto', 'Region']).agg({'Ventas': ['mean', 'median', 'std']}).reset_index()\n",
      "print(summary)\n",
      "\n",
      "# Distribución de Ventas\n",
      "plt.figure(figsize=(12, 6))\n",
      "sns.boxplot(data=data, x='Producto', y...\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 6: Cadena Secuencial (Multi-Step)\n",
    "\n",
    "# Paso 1: Generar una pregunta de analisis\n",
    "paso1_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Genera 1 pregunta analitica sobre el dataset: {dataset}\"\n",
    ")\n",
    "paso1_chain = paso1_prompt | model | StrOutputParser()\n",
    "\n",
    "# Paso 2: Sugerir metodo de analisis\n",
    "paso2_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Para responder esta pregunta: {pregunta}\\n\\n\"\n",
    "    \"Sugiere un metodo de analisis de datos apropiado.\"\n",
    ")\n",
    "paso2_chain = paso2_prompt | model | StrOutputParser()\n",
    "\n",
    "# Paso 3: Generar codigo Python\n",
    "paso3_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Genera codigo Python usando pandas para: {metodo}\\n\\n\"\n",
    "    \"Solo el codigo, sin explicaciones.\"\n",
    ")\n",
    "paso3_chain = paso3_prompt | model | StrOutputParser()\n",
    "\n",
    "# Funcion para encadenar los pasos manualmente\n",
    "def analisis_completo(dataset: str):\n",
    "    print(\"Paso 1: Generando pregunta analitica...\")\n",
    "    pregunta = paso1_chain.invoke({\"dataset\": dataset})\n",
    "    print(f\"Pregunta: {pregunta[:200]}...\\n\")\n",
    "    \n",
    "    print(\"Paso 2: Sugiriendo metodo de analisis...\")\n",
    "    metodo = paso2_chain.invoke({\"pregunta\": pregunta})\n",
    "    print(f\"Metodo: {metodo[:200]}...\\n\")\n",
    "    \n",
    "    print(\"Paso 3: Generando codigo Python...\")\n",
    "    codigo = paso3_chain.invoke({\"metodo\": metodo})\n",
    "    print(f\"Codigo:\\n{codigo[:500]}...\")\n",
    "    \n",
    "    return {\"pregunta\": pregunta, \"metodo\": metodo, \"codigo\": codigo}\n",
    "\n",
    "# Ejecutar el analisis completo\n",
    "resultado = analisis_completo(\"ventas por region y producto (CSV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Output Parsers: Structured Output\n",
    "\n",
    "Los **Output Parsers** convierten las respuestas de texto del LLM en estructuras de datos (JSON, objetos Python).\n",
    "\n",
    "### Metodo Moderno: `with_structured_output()`\n",
    "\n",
    "OpenAI soporta nativamente salidas estructuradas usando Pydantic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analisis Estructurado del Dataset:\n",
      "============================================================\n",
      "Nombre: Ventas Mensuales de Productos\n",
      "Tipo: numerico, categorico, temporal\n",
      "Dificultad: medio\n",
      "Tiempo estimado: 2-4 semanas\n",
      "\n",
      "Analisis Recomendados:\n",
      "  1. Análisis de tendencias de ventas a lo largo del tiempo\n",
      "  2. Comparación de ventas por región\n",
      "  3. Análisis de correlación entre precio y cantidad vendida\n",
      "  4. Segmentación de productos por temporada\n",
      "  5. Identificación de productos más y menos vendidos\n",
      "\n",
      "Herramientas Python:\n",
      "  1. pandas\n",
      "  2. numpy\n",
      "  3. matplotlib\n",
      "  4. seaborn\n",
      "  5. statsmodels\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 7: Salida Estructurada con Pydantic\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Definir el esquema de datos que queremos extraer usando Pydantic\n",
    "# Pydantic valida automaticamente los tipos de datos\n",
    "class AnalisisDataset(BaseModel):\n",
    "    \"\"\"Analisis estructurado de un dataset.\"\"\"\n",
    "    \n",
    "    nombre_dataset: str = Field(description=\"Nombre del dataset\")\n",
    "    tipo_datos: str = Field(description=\"Tipo de datos (numerico, categorico, temporal, etc.)\")\n",
    "    analisis_recomendados: List[str] = Field(description=\"Lista de analisis recomendados\")\n",
    "    herramientas_python: List[str] = Field(description=\"Librerias Python recomendadas\")\n",
    "    dificultad: str = Field(description=\"Nivel de dificultad (bajo, medio, alto)\")\n",
    "    tiempo_estimado: Optional[str] = Field(default=None, description=\"Tiempo estimado de analisis\")\n",
    "\n",
    "# Crear modelo con salida estructurada\n",
    "# with_structured_output() fuerza al LLM a devolver datos en el formato especificado\n",
    "model_estructurado = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).with_structured_output(AnalisisDataset)\n",
    "\n",
    "# Invocar el modelo con una descripcion del dataset\n",
    "resultado = model_estructurado.invoke(\n",
    "    \"Analiza este dataset: Ventas mensuales de 50 productos en 10 regiones durante 3 anios, \"\n",
    "    \"incluyendo precio, cantidad vendida, y temporada del anio.\"\n",
    ")\n",
    "\n",
    "# El resultado es un objeto Pydantic con atributos tipados\n",
    "print(\"Analisis Estructurado del Dataset:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Nombre: {resultado.nombre_dataset}\")\n",
    "print(f\"Tipo: {resultado.tipo_datos}\")\n",
    "print(f\"Dificultad: {resultado.dificultad}\")\n",
    "print(f\"Tiempo estimado: {resultado.tiempo_estimado}\")\n",
    "print(f\"\\nAnalisis Recomendados:\")\n",
    "for i, analisis in enumerate(resultado.analisis_recomendados, 1):\n",
    "    print(f\"  {i}. {analisis}\")\n",
    "print(f\"\\nHerramientas Python:\")\n",
    "for i, herramienta in enumerate(resultado.herramientas_python, 1):\n",
    "    print(f\"  {i}. {herramienta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Herramientas Personalizadas <a id='5'></a>\n",
    "\n",
    "## Que es una Herramienta?\n",
    "\n",
    "Una **Tool** (herramienta) es una funcion que puede ser ejecutada por un LLM o un agente.\n",
    "\n",
    "### Componentes de una Herramienta:\n",
    "\n",
    "- **Nombre**: Identificador de la herramienta\n",
    "- **Descripcion**: Explica que hace la herramienta (el LLM usa esto para decidir cuando usarla)\n",
    "- **Funcion**: El codigo que se ejecuta\n",
    "- **Parametros**: Argumentos que la funcion acepta\n",
    "\n",
    "### Decorador @tool\n",
    "\n",
    "El decorador `@tool` convierte una funcion Python normal en una herramienta que LangChain puede usar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas creadas:\n",
      "  - obtener_estadisticas_dataset\n",
      "    Descripcion: Obtiene estadisticas basicas de un archivo CSV.\n",
      "\n",
      "    Args:\n",
      "        ruta_csv: Ruta al archivo CSV\n",
      "\n",
      "  ...\n",
      "  - calcular_metricas_ventas\n",
      "    Descripcion: Calcula metricas de ventas desde un CSV.\n",
      "\n",
      "    Args:\n",
      "        ruta_csv: Ruta al archivo CSV con column...\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 8: Crear Herramientas Personalizadas\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import pandas as pd\n",
    "\n",
    "# Decorador @tool convierte la funcion en una herramienta de LangChain\n",
    "# El docstring se usa como descripcion de la herramienta\n",
    "@tool\n",
    "def obtener_estadisticas_dataset(ruta_csv: str) -> str:\n",
    "    \"\"\"Obtiene estadisticas basicas de un archivo CSV.\n",
    "    \n",
    "    Args:\n",
    "        ruta_csv: Ruta al archivo CSV\n",
    "        \n",
    "    Returns:\n",
    "        String con estadisticas del dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leer el archivo CSV usando pandas\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        \n",
    "        # Calcular estadisticas basicas\n",
    "        stats = f\"\"\"\n",
    "Estadisticas del Dataset:\n",
    "- Filas: {len(df)}\n",
    "- Columnas: {len(df.columns)}\n",
    "- Nombres de columnas: {', '.join(df.columns)}\n",
    "- Tipos de datos: {df.dtypes.to_dict()}\n",
    "- Valores nulos: {df.isnull().sum().to_dict()}\n",
    "- Primeras filas:\n",
    "{df.head(3).to_string()}\n",
    "\"\"\"\n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        return f\"Error al leer el archivo: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def calcular_metricas_ventas(ruta_csv: str) -> str:\n",
    "    \"\"\"Calcula metricas de ventas desde un CSV.\n",
    "    \n",
    "    Args:\n",
    "        ruta_csv: Ruta al archivo CSV con columnas: precio, cantidad\n",
    "        \n",
    "    Returns:\n",
    "        String con metricas calculadas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leer el archivo CSV\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        \n",
    "        # Calcular ingresos totales (precio * cantidad)\n",
    "        df['ingresos'] = df['precio'] * df['cantidad']\n",
    "        \n",
    "        # Calcular metricas de negocio\n",
    "        metricas = f\"\"\"\n",
    "Metricas de Ventas:\n",
    "- Ingresos totales: ${df['ingresos'].sum():,.2f}\n",
    "- Ticket promedio: ${df['ingresos'].mean():,.2f}\n",
    "- Producto mas vendido: {df.groupby('producto')['cantidad'].sum().idxmax()}\n",
    "- Region con mas ventas: {df.groupby('region')['ingresos'].sum().idxmax()}\n",
    "\"\"\"\n",
    "        return metricas\n",
    "    except Exception as e:\n",
    "        return f\"Error al calcular metricas: {str(e)}\"\n",
    "\n",
    "# Lista de herramientas disponibles\n",
    "tools = [obtener_estadisticas_dataset, calcular_metricas_ventas]\n",
    "\n",
    "print(\"Herramientas creadas:\")\n",
    "for tool_item in tools:\n",
    "    print(f\"  - {tool_item.name}\")\n",
    "    print(f\"    Descripcion: {tool_item.description[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando herramienta: obtener_estadisticas_dataset\n",
      "============================================================\n",
      "\n",
      "Estadisticas del Dataset:\n",
      "- Filas: 50\n",
      "- Columnas: 8\n",
      "- Nombres de columnas: fecha, producto, categoria, precio, cantidad, vendedor, region, cliente_id\n",
      "- Tipos de datos: {'fecha': dtype('O'), 'producto': dtype('O'), 'categoria': dtype('O'), 'precio': dtype('float64'), 'cantidad': dtype('int64'), 'vendedor': dtype('O'), 'region': dtype('O'), 'cliente_id': dtype('O')}\n",
      "- Valores nulos: {'fecha': 0, 'producto': 0, 'categoria': 0, 'precio': 0, 'cantidad': 0, 'vendedor': 0, 'region': 0, 'cliente_id': 0}\n",
      "- Primeras filas:\n",
      "        fecha          producto    categoria  precio  cantidad      vendedor region cliente_id\n",
      "0  2024-01-15         Laptop HP  Electronica  899.99         2    Juan Perez  Norte       C001\n",
      "1  2024-01-16    Mouse Logitech   Accesorios   25.50         5  Maria Garcia    Sur       C002\n",
      "2  2024-01-16  Teclado Mecanico   Accesorios   89.99         3    Juan Perez  Norte       C003\n",
      "\n",
      "\n",
      "============================================================\n",
      "Ejecutando herramienta: calcular_metricas_ventas\n",
      "============================================================\n",
      "\n",
      "Metricas de Ventas:\n",
      "- Ingresos totales: $29,633.15\n",
      "- Ticket promedio: $592.66\n",
      "- Producto mas vendido: Mouse Logitech\n",
      "- Region con mas ventas: Sur\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 9: Usar Herramientas Directamente (sin agente)\n",
    "\n",
    "# Las herramientas pueden ser invocadas directamente como funciones normales\n",
    "print(\"Ejecutando herramienta: obtener_estadisticas_dataset\")\n",
    "print(\"=\"*60)\n",
    "resultado = obtener_estadisticas_dataset.invoke({\"ruta_csv\": \"ventas_ejemplo.csv\"})\n",
    "print(resultado)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ejecutando herramienta: calcular_metricas_ventas\")\n",
    "print(\"=\"*60)\n",
    "resultado = calcular_metricas_ventas.invoke({\"ruta_csv\": \"ventas_ejemplo.csv\"})\n",
    "print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. Sistemas Multi-Agente con LangGraph <a id='6'></a>\n",
    "\n",
    "## Que es LangGraph?\n",
    "\n",
    "**LangGraph** es una biblioteca para construir **sistemas multi-agente** usando grafos.\n",
    "\n",
    "### Conceptos Clave:\n",
    "\n",
    "- **Nodos**: Funciones que realizan tareas especificas\n",
    "- **Edges**: Conexiones entre nodos (flujo de control)\n",
    "- **State**: Estado compartido entre todos los nodos\n",
    "- **Graph**: Estructura que define el flujo de ejecucion\n",
    "\n",
    "### Patrones de Arquitectura:\n",
    "\n",
    "1. **Sequential**: Nodos ejecutan en secuencia\n",
    "2. **Parallel**: Nodos ejecutan en paralelo\n",
    "3. **Conditional**: Flujo depende de condiciones\n",
    "4. **Loop**: Nodos pueden ejecutarse multiples veces\n",
    "\n",
    "## Referencias:\n",
    "\n",
    "- [LangGraph Multi-Agent Workflows](https://blog.langchain.com/langgraph-multi-agent-workflows/)\n",
    "- [Multi-Agent Tutorial](https://blog.futuresmart.ai/multi-agent-system-with-langgraph)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo LangGraph creado:\n",
      "Flujo: analizar -> procesar -> END\n",
      "\n",
      "Ejecutando grafo...\n",
      "\n",
      "Nodo: Analizando entrada...\n",
      "Nodo: Procesando analisis...\n",
      "\n",
      "Resultado Final:\n",
      "  Analisis completado de: Dataset de ventas | Procesamiento finalizado\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo 10: Introduccion a LangGraph - Grafo Simple\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# Paso 1: Definir el State (estado compartido entre nodos)\n",
    "# TypedDict permite definir la estructura del estado con tipos\n",
    "class State(TypedDict):\n",
    "    \"\"\"Estado del grafo.\"\"\"\n",
    "    # Lista de mensajes con merge automatico usando add_messages\n",
    "    messages: Annotated[list, add_messages]\n",
    "    # Resultado del procesamiento\n",
    "    resultado: str\n",
    "\n",
    "# Paso 2: Definir nodos (funciones que procesan el estado)\n",
    "# Cada nodo recibe el estado, lo modifica y lo retorna\n",
    "def nodo_analizar(state: State) -> State:\n",
    "    \"\"\"Nodo que analiza la entrada.\"\"\"\n",
    "    print(\"Nodo: Analizando entrada...\")\n",
    "    # Obtener el ultimo mensaje si existe\n",
    "    mensaje_entrada = state[\"messages\"][-1].content if state.get(\"messages\") else \"Sin entrada\"\n",
    "    # Actualizar el estado con el resultado del analisis\n",
    "    state[\"resultado\"] = f\"Analisis completado de: {mensaje_entrada}\"\n",
    "    return state\n",
    "\n",
    "def nodo_procesar(state: State) -> State:\n",
    "    \"\"\"Nodo que procesa el analisis.\"\"\"\n",
    "    print(\"Nodo: Procesando analisis...\")\n",
    "    # Agregar informacion al resultado existente\n",
    "    state[\"resultado\"] += \" | Procesamiento finalizado\"\n",
    "    return state\n",
    "\n",
    "# Paso 3: Construir el grafo\n",
    "# StateGraph recibe la definicion del estado como parametro\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Agregar nodos al grafo\n",
    "# Sintaxis: add_node(nombre, funcion)\n",
    "workflow.add_node(\"analizar\", nodo_analizar)\n",
    "workflow.add_node(\"procesar\", nodo_procesar)\n",
    "\n",
    "# Definir el flujo entre nodos usando edges\n",
    "workflow.set_entry_point(\"analizar\")  # Primer nodo a ejecutar\n",
    "workflow.add_edge(\"analizar\", \"procesar\")  # analizar -> procesar\n",
    "workflow.add_edge(\"procesar\", END)  # procesar -> fin\n",
    "\n",
    "# Compilar el grafo para poder ejecutarlo\n",
    "app = workflow.compile()\n",
    "\n",
    "print(\"Grafo LangGraph creado:\")\n",
    "print(\"Flujo: analizar -> procesar -> END\")\n",
    "\n",
    "# Paso 4: Ejecutar el grafo\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Definir el estado inicial\n",
    "estado_inicial = {\n",
    "    \"messages\": [HumanMessage(content=\"Dataset de ventas\")],\n",
    "    \"resultado\": \"\"\n",
    "}\n",
    "\n",
    "print(\"\\nEjecutando grafo...\\n\")\n",
    "# invoke() ejecuta todo el grafo hasta END\n",
    "resultado_final = app.invoke(estado_inicial)\n",
    "\n",
    "print(\"\\nResultado Final:\")\n",
    "print(f\"  {resultado_final['resultado']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 7. Proyecto Final: Equipo de Data Science Multi-Agente <a id='7'></a>\n",
    "\n",
    "## Objetivo del Proyecto\n",
    "\n",
    "Construir un **sistema multi-agente** que simule un equipo de Data Science trabajando en un analisis de datos real.\n",
    "\n",
    "## Equipo de Agentes:\n",
    "\n",
    "1. **Data Analyst Agent**: Explora y describe los datos\n",
    "2. **Statistics Agent**: Calcula estadisticas y metricas\n",
    "3. **Visualization Agent**: Genera visualizaciones\n",
    "4. **Report Agent**: Genera el reporte final\n",
    "5. **Supervisor Agent**: Coordina a todos los agentes\n",
    "\n",
    "## Dataset: Ventas de Productos\n",
    "\n",
    "Usaremos el archivo `ventas_ejemplo.csv` con las siguientes columnas:\n",
    "- **fecha**: Fecha de la venta\n",
    "- **producto**: Nombre del producto\n",
    "- **categoria**: Categoria del producto\n",
    "- **precio**: Precio unitario\n",
    "- **cantidad**: Cantidad vendida\n",
    "- **vendedor**: Nombre del vendedor\n",
    "- **region**: Region de venta\n",
    "- **cliente_id**: ID del cliente\n",
    "\n",
    "## Arquitectura del Sistema\n",
    "\n",
    "```\n",
    "                      SUPERVISOR\n",
    "                           |\n",
    "           +---------------+---------------+\n",
    "           |               |               |\n",
    "      ANALYST         STATISTICS     VISUALIZATION\n",
    "           |               |               |\n",
    "           +---------------+---------------+\n",
    "                           |\n",
    "                        REPORT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Preparacion: Crear Herramientas para los Agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas del Data Analyst Agent creadas\n"
     ]
    }
   ],
   "source": [
    "# Herramientas para el Data Analyst Agent\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def explorar_dataset(ruta_csv: str) -> str:\n",
    "    \"\"\"Explora un dataset CSV y retorna informacion basica.\n",
    "    \n",
    "    Args:\n",
    "        ruta_csv: Ruta al archivo CSV\n",
    "    \n",
    "    Returns:\n",
    "        String con informacion del dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leer el archivo CSV\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        \n",
    "        # Construir el reporte de exploracion\n",
    "        info = f\"\"\"\n",
    "EXPLORACION DEL DATASET\n",
    "{'='*60}\n",
    "\n",
    "Dimensiones:\n",
    "  - Filas: {len(df):,}\n",
    "  - Columnas: {len(df.columns)}\n",
    "\n",
    "Columnas y Tipos:\n",
    "\"\"\"\n",
    "        # Listar todas las columnas con sus tipos de datos\n",
    "        for col in df.columns:\n",
    "            info += f\"  - {col}: {df[col].dtype}\\n\"\n",
    "        \n",
    "        info += f\"\"\"\n",
    "Valores Nulos:\n",
    "\"\"\"\n",
    "        # Verificar valores nulos\n",
    "        nulos = df.isnull().sum()\n",
    "        if nulos.sum() == 0:\n",
    "            info += \"  No hay valores nulos\\n\"\n",
    "        else:\n",
    "            for col, count in nulos[nulos > 0].items():\n",
    "                info += f\"  - {col}: {count} ({count/len(df)*100:.1f}%)\\n\"\n",
    "        \n",
    "        # Mostrar primeras filas\n",
    "        info += f\"\"\"\n",
    "Primeras 5 Filas:\n",
    "{df.head().to_string()}\n",
    "\"\"\"\n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def analizar_categorias(ruta_csv: str, columna: str) -> str:\n",
    "    \"\"\"Analiza la distribucion de una columna categorica.\n",
    "    \n",
    "    Args:\n",
    "        ruta_csv: Ruta al archivo CSV\n",
    "        columna: Nombre de la columna a analizar\n",
    "    \n",
    "    Returns:\n",
    "        String con el analisis de categorias\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        \n",
    "        # Verificar que la columna existe\n",
    "        if columna not in df.columns:\n",
    "            return f\"La columna '{columna}' no existe en el dataset\"\n",
    "        \n",
    "        # Calcular frecuencias absolutas y relativas\n",
    "        conteo = df[columna].value_counts()\n",
    "        porcentaje = df[columna].value_counts(normalize=True) * 100\n",
    "        \n",
    "        # Construir el reporte\n",
    "        info = f\"\"\"\n",
    "ANALISIS DE CATEGORIAS: {columna}\n",
    "{'='*60}\n",
    "\n",
    "Distribucion:\n",
    "\"\"\"\n",
    "        for cat in conteo.index:\n",
    "            info += f\"  - {cat}: {conteo[cat]} ({porcentaje[cat]:.1f}%)\\n\"\n",
    "        \n",
    "        info += f\"\"\"\n",
    "Resumen:\n",
    "  - Valores unicos: {df[columna].nunique()}\n",
    "  - Categoria mas frecuente: {conteo.index[0]} ({conteo.iloc[0]} ocurrencias)\n",
    "\"\"\"\n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"Herramientas del Data Analyst Agent creadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas del Statistics Agent creadas\n"
     ]
    }
   ],
   "source": [
    "# Herramientas para el Statistics Agent\n",
    "\n",
    "@tool\n",
    "def calcular_estadisticas_descriptivas(ruta_csv: str) -> str:\n",
    "    \"\"\"Calcula estadisticas descriptivas para columnas numericas.\n",
    "    \n",
    "    Args:\n",
    "        ruta_csv: Ruta al archivo CSV\n",
    "    \n",
    "    Returns:\n",
    "        String con estadisticas descriptivas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        \n",
    "        # Convertir fecha a datetime si existe\n",
    "        if 'fecha' in df.columns:\n",
    "            df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "        \n",
    "        # Calcular ingresos si tenemos precio y cantidad\n",
    "        if 'precio' in df.columns and 'cantidad' in df.columns:\n",
    "            df['ingresos'] = df['precio'] * df['cantidad']\n",
    "        \n",
    "        # Obtener estadisticas descriptivas de columnas numericas\n",
    "        stats_df = df.describe()\n",
    "        \n",
    "        info = f\"\"\"\n",
    "ESTADISTICAS DESCRIPTIVAS\n",
    "{'='*60}\n",
    "\n",
    "{stats_df.to_string()}\n",
    "\n",
    "METRICAS DE NEGOCIO:\n",
    "\"\"\"\n",
    "        # Calcular metricas de negocio si tenemos ingresos\n",
    "        if 'ingresos' in df.columns:\n",
    "            info += f\"\"\"\n",
    "  - Ingresos Totales: ${df['ingresos'].sum():,.2f}\n",
    "  - Ingreso Promedio por Venta: ${df['ingresos'].mean():,.2f}\n",
    "  - Ingreso Mediano: ${df['ingresos'].median():,.2f}\n",
    "  - Desviacion Estandar: ${df['ingresos'].std():,.2f}\n",
    "  - Venta Minima: ${df['ingresos'].min():,.2f}\n",
    "  - Venta Maxima: ${df['ingresos'].max():,.2f}\n",
    "\"\"\"\n",
    "        \n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def analizar_por_categoria(ruta_csv: str, columna_agrupacion: str) -> str:\n",
    "    \"\"\"Analiza metricas agrupadas por una categoria.\n",
    "    \n",
    "    Args:\n",
    "        ruta_csv: Ruta al archivo CSV\n",
    "        columna_agrupacion: Columna para agrupar (ej: 'region', 'categoria')\n",
    "    \n",
    "    Returns:\n",
    "        String con analisis por categoria\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        \n",
    "        # Verificar que la columna existe\n",
    "        if columna_agrupacion not in df.columns:\n",
    "            return f\"La columna '{columna_agrupacion}' no existe\"\n",
    "        \n",
    "        # Calcular ingresos\n",
    "        df['ingresos'] = df['precio'] * df['cantidad']\n",
    "        \n",
    "        # Agrupar y calcular metricas agregadas\n",
    "        grupo = df.groupby(columna_agrupacion).agg({\n",
    "            'ingresos': ['sum', 'mean', 'count'],\n",
    "            'cantidad': 'sum',\n",
    "            'precio': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        # Ordenar por ingresos totales descendente\n",
    "        grupo = grupo.sort_values(('ingresos', 'sum'), ascending=False)\n",
    "        \n",
    "        info = f\"\"\"\n",
    "ANALISIS POR {columna_agrupacion.upper()}\n",
    "{'='*60}\n",
    "\n",
    "{grupo.to_string()}\n",
    "\n",
    "TOP 3:\n",
    "\"\"\"\n",
    "        # Mostrar el top 3\n",
    "        for i, idx in enumerate(grupo.index[:3], 1):\n",
    "            ingresos = grupo.loc[idx, ('ingresos', 'sum')]\n",
    "            ventas = grupo.loc[idx, ('ingresos', 'count')]\n",
    "            info += f\"  {i}. {idx}: ${ingresos:,.2f} ({int(ventas)} ventas)\\n\"\n",
    "        \n",
    "        return info\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"Herramientas del Statistics Agent creadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Herramientas del Visualization Agent creadas\n"
     ]
    }
   ],
   "source": [
    "# Herramientas para el Visualization Agent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurar estilo de visualizaciones\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "@tool\n",
    "def crear_grafico_barras(ruta_csv: str, columna: str, titulo: str = \"Grafico de Barras\") -> str:\n",
    "    \"\"\"Crea un grafico de barras para una columna categorica.\n",
    "    \n",
    "    Args:\n",
    "        ruta_csv: Ruta al archivo CSV\n",
    "        columna: Columna a graficar\n",
    "        titulo: Titulo del grafico\n",
    "    \n",
    "    Returns:\n",
    "        String con la ruta del archivo guardado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        # Calcular ingresos por categoria\n",
    "        df['ingresos'] = df['precio'] * df['cantidad']\n",
    "        \n",
    "        # Agrupar y ordenar por ingresos\n",
    "        datos = df.groupby(columna)['ingresos'].sum().sort_values(ascending=False)\n",
    "        \n",
    "        # Crear figura y grafico\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = datos.plot(kind='bar', color='steelblue', edgecolor='black')\n",
    "        plt.title(titulo, fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(columna.capitalize(), fontsize=12)\n",
    "        plt.ylabel('Ingresos ($)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        \n",
    "        # Agregar valores en las barras\n",
    "        for i, v in enumerate(datos):\n",
    "            ax.text(i, v, f'${v:,.0f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar el grafico con timestamp\n",
    "        filename = f\"grafico_barras_{columna}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return f\"Grafico guardado: {filename}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "@tool\n",
    "def crear_serie_temporal(ruta_csv: str, titulo: str = \"Ventas en el Tiempo\") -> str:\n",
    "    \"\"\"Crea un grafico de serie temporal de ventas.\n",
    "    \n",
    "    Args:\n",
    "        ruta_csv: Ruta al archivo CSV\n",
    "        titulo: Titulo del grafico\n",
    "    \n",
    "    Returns:\n",
    "        String con la ruta del archivo guardado\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(ruta_csv)\n",
    "        # Convertir fecha a datetime\n",
    "        df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "        df['ingresos'] = df['precio'] * df['cantidad']\n",
    "        \n",
    "        # Agrupar por fecha y sumar ingresos\n",
    "        serie = df.groupby('fecha')['ingresos'].sum()\n",
    "        \n",
    "        # Crear figura y grafico\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(serie.index, serie.values, marker='o', linewidth=2, \n",
    "                 markersize=6, color='steelblue')\n",
    "        plt.title(titulo, fontsize=16, fontweight='bold')\n",
    "        plt.xlabel('Fecha', fontsize=12)\n",
    "        plt.ylabel('Ingresos ($)', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Agregar linea de tendencia\n",
    "        z = np.polyfit(range(len(serie)), serie.values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(serie.index, p(range(len(serie))), \"r--\", \n",
    "                 alpha=0.8, linewidth=2, label='Tendencia')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Guardar el grafico\n",
    "        filename = f\"serie_temporal_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        return f\"Grafico guardado: {filename}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"Herramientas del Visualization Agent creadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Crear una Funcion Helper para Ejecutar Herramientas con LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funcion helper creada\n"
     ]
    }
   ],
   "source": [
    "# Funcion helper para ejecutar herramientas con decision del LLM\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "def ejecutar_con_herramientas(llm: ChatOpenAI, tools: list, pregunta: str, max_iterations: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Ejecuta un LLM con herramientas disponibles.\n",
    "    \n",
    "    Args:\n",
    "        llm: Modelo de chat de OpenAI\n",
    "        tools: Lista de herramientas disponibles\n",
    "        pregunta: Pregunta o tarea para el LLM\n",
    "        max_iterations: Maximo numero de iteraciones\n",
    "    \n",
    "    Returns:\n",
    "        Respuesta final del LLM\n",
    "    \"\"\"\n",
    "    # Bind tools permite que el LLM sepa que herramientas estan disponibles\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "    \n",
    "    # Inicializar lista de mensajes con la pregunta del usuario\n",
    "    messages = [HumanMessage(content=pregunta)]\n",
    "    \n",
    "    # Loop de razonamiento: el LLM puede llamar herramientas multiples veces\n",
    "    for iteration in range(max_iterations):\n",
    "        # Invocar el LLM con los mensajes actuales\n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        messages.append(response)\n",
    "        \n",
    "        # Si el LLM no quiere llamar mas herramientas, terminamos\n",
    "        if not response.tool_calls:\n",
    "            return response.content\n",
    "        \n",
    "        # Ejecutar cada herramienta que el LLM solicito\n",
    "        for tool_call in response.tool_calls:\n",
    "            # Buscar la herramienta por nombre\n",
    "            tool_to_use = None\n",
    "            for tool in tools:\n",
    "                if tool.name == tool_call[\"name\"]:\n",
    "                    tool_to_use = tool\n",
    "                    break\n",
    "            \n",
    "            if tool_to_use:\n",
    "                # Ejecutar la herramienta con los argumentos proporcionados\n",
    "                tool_result = tool_to_use.invoke(tool_call[\"args\"])\n",
    "                # Agregar el resultado como ToolMessage\n",
    "                messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=str(tool_result),\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    # Si llegamos al maximo de iteraciones, devolver el ultimo mensaje\n",
    "    return messages[-1].content if messages else \"No se pudo generar respuesta\"\n",
    "\n",
    "print(\"Funcion helper creada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Construir el Sistema Multi-Agente con LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones de nodos definidas\n"
     ]
    }
   ],
   "source": [
    "# Sistema Multi-Agente con Supervisor\n",
    "\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "import operator\n",
    "\n",
    "# Crear LLM para los agentes\n",
    "llm_agentes = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Paso 1: Definir el Estado del Sistema\n",
    "class DataScienceTeamState(TypedDict):\n",
    "    \"\"\"Estado compartido del equipo de Data Science.\"\"\"\n",
    "    # Historial de mensajes (se acumulan con operator.add)\n",
    "    messages: Annotated[list, operator.add]\n",
    "    # Ruta del dataset a analizar\n",
    "    ruta_csv: str\n",
    "    # Descripcion de la tarea solicitada\n",
    "    tarea: str\n",
    "    # Resultados de cada agente (diccionario)\n",
    "    resultados: dict\n",
    "    # Proximo agente a ejecutar\n",
    "    siguiente: str\n",
    "    # Reporte final consolidado\n",
    "    reporte_final: str\n",
    "\n",
    "# Paso 2: Funciones para cada nodo (agente)\n",
    "\n",
    "def nodo_supervisor(state: DataScienceTeamState) -> DataScienceTeamState:\n",
    "    \"\"\"\n",
    "    Supervisor que decide que agente debe actuar.\n",
    "    Implementa la logica de coordinacion del equipo.\n",
    "    \"\"\"\n",
    "    print(\"\\nSUPERVISOR: Analizando tarea...\")\n",
    "    \n",
    "    resultados = state.get(\"resultados\", {})\n",
    "    \n",
    "    # Decidir el siguiente paso basado en que ya se ha completado\n",
    "    # El supervisor ejecuta los agentes en secuencia:\n",
    "    # analyst -> statistics -> visualization -> report\n",
    "    if not resultados.get(\"analyst\"):\n",
    "        siguiente = \"analyst\"\n",
    "        print(\"   -> Delegando a: Data Analyst Agent (exploracion inicial)\")\n",
    "    elif not resultados.get(\"statistics\"):\n",
    "        siguiente = \"statistics\"\n",
    "        print(\"   -> Delegando a: Statistics Agent (calculo de metricas)\")\n",
    "    elif not resultados.get(\"visualization\"):\n",
    "        siguiente = \"visualization\"\n",
    "        print(\"   -> Delegando a: Visualization Agent (creacion de graficos)\")\n",
    "    else:\n",
    "        siguiente = \"report\"\n",
    "        print(\"   -> Delegando a: Report Agent (generacion de reporte final)\")\n",
    "    \n",
    "    state[\"siguiente\"] = siguiente\n",
    "    return state\n",
    "\n",
    "def nodo_analyst(state: DataScienceTeamState) -> DataScienceTeamState:\n",
    "    \"\"\"\n",
    "    Data Analyst: Explora y describe el dataset.\n",
    "    Usa herramientas de exploracion y analisis de categorias.\n",
    "    \"\"\"\n",
    "    print(\"\\nDATA ANALYST: Explorando dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Herramientas disponibles para este agente\n",
    "        tools_analyst = [explorar_dataset, analizar_categorias]\n",
    "        \n",
    "        # Explorar dataset usando la funcion helper\n",
    "        resultado_exploracion = ejecutar_con_herramientas(\n",
    "            llm_agentes,\n",
    "            tools_analyst,\n",
    "            f\"Explora el dataset en '{state['ruta_csv']}' y proporciona un resumen detallado de su estructura.\"\n",
    "        )\n",
    "        \n",
    "        # Analizar categorias principales\n",
    "        resultado_categorias = ejecutar_con_herramientas(\n",
    "            llm_agentes,\n",
    "            tools_analyst,\n",
    "            f\"Analiza la distribucion de 'categoria' y 'region' en '{state['ruta_csv']}'.\"\n",
    "        )\n",
    "        \n",
    "        # Guardar resultados en el estado\n",
    "        state[\"resultados\"][\"analyst\"] = {\n",
    "            \"exploracion\": resultado_exploracion,\n",
    "            \"categorias\": resultado_categorias\n",
    "        }\n",
    "        \n",
    "        print(\"   Exploracion completada\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        state[\"resultados\"][\"analyst\"] = {\"error\": str(e)}\n",
    "    \n",
    "    return state\n",
    "\n",
    "def nodo_statistics(state: DataScienceTeamState) -> DataScienceTeamState:\n",
    "    \"\"\"\n",
    "    Statistics Agent: Calcula metricas estadisticas.\n",
    "    Usa herramientas de estadisticas descriptivas y analisis por categoria.\n",
    "    \"\"\"\n",
    "    print(\"\\nSTATISTICS AGENT: Calculando metricas...\")\n",
    "    \n",
    "    try:\n",
    "        # Herramientas disponibles para este agente\n",
    "        tools_statistics = [calcular_estadisticas_descriptivas, analizar_por_categoria]\n",
    "        \n",
    "        # Calcular estadisticas descriptivas\n",
    "        resultado_stats = ejecutar_con_herramientas(\n",
    "            llm_agentes,\n",
    "            tools_statistics,\n",
    "            f\"Calcula estadisticas descriptivas completas del dataset '{state['ruta_csv']}'.\"\n",
    "        )\n",
    "        \n",
    "        # Analizar por region\n",
    "        resultado_region = ejecutar_con_herramientas(\n",
    "            llm_agentes,\n",
    "            tools_statistics,\n",
    "            f\"Analiza las metricas de ventas agrupadas por 'region' en '{state['ruta_csv']}'.\"\n",
    "        )\n",
    "        \n",
    "        # Analizar por categoria\n",
    "        resultado_categoria = ejecutar_con_herramientas(\n",
    "            llm_agentes,\n",
    "            tools_statistics,\n",
    "            f\"Analiza las metricas de ventas agrupadas por 'categoria' en '{state['ruta_csv']}'.\"\n",
    "        )\n",
    "        \n",
    "        # Guardar resultados\n",
    "        state[\"resultados\"][\"statistics\"] = {\n",
    "            \"descriptivas\": resultado_stats,\n",
    "            \"por_region\": resultado_region,\n",
    "            \"por_categoria\": resultado_categoria\n",
    "        }\n",
    "        \n",
    "        print(\"   Metricas calculadas\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        state[\"resultados\"][\"statistics\"] = {\"error\": str(e)}\n",
    "    \n",
    "    return state\n",
    "\n",
    "def nodo_visualization(state: DataScienceTeamState) -> DataScienceTeamState:\n",
    "    \"\"\"\n",
    "    Visualization Agent: Crea visualizaciones.\n",
    "    Usa herramientas para crear graficos de barras y series temporales.\n",
    "    \"\"\"\n",
    "    print(\"\\nVISUALIZATION AGENT: Creando graficos...\")\n",
    "    \n",
    "    try:\n",
    "        # Herramientas disponibles para este agente\n",
    "        tools_visualization = [crear_grafico_barras, crear_serie_temporal]\n",
    "        \n",
    "        # Crear grafico de ingresos por region\n",
    "        resultado_region = ejecutar_con_herramientas(\n",
    "            llm_agentes,\n",
    "            tools_visualization,\n",
    "            f\"Crea un grafico de barras de ingresos por region usando '{state['ruta_csv']}' con titulo 'Ingresos por Region'.\"\n",
    "        )\n",
    "        \n",
    "        # Crear grafico de ingresos por categoria\n",
    "        resultado_categoria = ejecutar_con_herramientas(\n",
    "            llm_agentes,\n",
    "            tools_visualization,\n",
    "            f\"Crea un grafico de barras de ingresos por categoria usando '{state['ruta_csv']}' con titulo 'Ingresos por Categoria'.\"\n",
    "        )\n",
    "        \n",
    "        # Crear serie temporal\n",
    "        resultado_temporal = ejecutar_con_herramientas(\n",
    "            llm_agentes,\n",
    "            tools_visualization,\n",
    "            f\"Crea un grafico de serie temporal de ventas usando '{state['ruta_csv']}' con titulo 'Evolucion de Ventas'.\"\n",
    "        )\n",
    "        \n",
    "        # Guardar resultados\n",
    "        state[\"resultados\"][\"visualization\"] = {\n",
    "            \"por_region\": resultado_region,\n",
    "            \"por_categoria\": resultado_categoria,\n",
    "            \"temporal\": resultado_temporal\n",
    "        }\n",
    "        \n",
    "        print(\"   Graficos creados\")\n",
    "    except Exception as e:\n",
    "        print(f\"   Error: {str(e)}\")\n",
    "        state[\"resultados\"][\"visualization\"] = {\"error\": str(e)}\n",
    "    \n",
    "    return state\n",
    "\n",
    "def nodo_report(state: DataScienceTeamState) -> DataScienceTeamState:\n",
    "    \"\"\"\n",
    "    Report Agent: Genera el reporte final consolidado.\n",
    "    Combina los resultados de todos los agentes en un reporte ejecutivo.\n",
    "    \"\"\"\n",
    "    print(\"\\nREPORT AGENT: Generando reporte final...\")\n",
    "    \n",
    "    resultados = state[\"resultados\"]\n",
    "    \n",
    "    # Consolidar todos los resultados en un reporte estructurado\n",
    "    reporte = f\"\"\"\n",
    "{'='*80}\n",
    "REPORTE DE ANALISIS DE DATOS - EQUIPO DE DATA SCIENCE\n",
    "{'='*80}\n",
    "\n",
    "Dataset: {state['ruta_csv']}\n",
    "Tarea: {state['tarea']}\n",
    "Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "{'='*80}\n",
    "1. EXPLORACION INICIAL (Data Analyst Agent)\n",
    "{'='*80}\n",
    "\n",
    "{resultados.get('analyst', {}).get('exploracion', 'No disponible')}\n",
    "\n",
    "{resultados.get('analyst', {}).get('categorias', '')}\n",
    "\n",
    "{'='*80}\n",
    "2. ANALISIS ESTADISTICO (Statistics Agent)\n",
    "{'='*80}\n",
    "\n",
    "{resultados.get('statistics', {}).get('descriptivas', 'No disponible')}\n",
    "\n",
    "{resultados.get('statistics', {}).get('por_region', '')}\n",
    "\n",
    "{resultados.get('statistics', {}).get('por_categoria', '')}\n",
    "\n",
    "{'='*80}\n",
    "3. VISUALIZACIONES (Visualization Agent)\n",
    "{'='*80}\n",
    "\n",
    "{resultados.get('visualization', {}).get('por_region', 'No disponible')}\n",
    "{resultados.get('visualization', {}).get('por_categoria', 'No disponible')}\n",
    "{resultados.get('visualization', {}).get('temporal', 'No disponible')}\n",
    "\n",
    "{'='*80}\n",
    "4. CONCLUSIONES Y RECOMENDACIONES\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "    \n",
    "    # Usar LLM para generar conclusiones inteligentes basadas en los resultados\n",
    "    prompt_conclusiones = f\"\"\"\n",
    "Basandote en los siguientes resultados de analisis de datos, genera un resumen ejecutivo con:\n",
    "1. 3-5 hallazgos clave\n",
    "2. 2-3 recomendaciones accionables\n",
    "\n",
    "Datos del analisis:\n",
    "{str(resultados)[:2000]}\n",
    "\n",
    "Se conciso y enfocate en insights de negocio.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Generar conclusiones usando el LLM\n",
    "        conclusiones = llm_agentes.invoke(prompt_conclusiones)\n",
    "        reporte += conclusiones.content\n",
    "    except:\n",
    "        # Si falla, usar conclusiones por defecto\n",
    "        reporte += \"\"\"\n",
    "Hallazgos Clave:\n",
    "  - Dataset analizado exitosamente\n",
    "  - Metricas calculadas y visualizadas\n",
    "  - Ver graficos generados para mas detalles\n",
    "\"\"\"\n",
    "    \n",
    "    reporte += f\"\"\"\n",
    "\n",
    "{'='*80}\n",
    "FIN DEL REPORTE\n",
    "{'='*80}\n",
    "\"\"\"\n",
    "    \n",
    "    state[\"reporte_final\"] = reporte\n",
    "    state[\"siguiente\"] = \"END\"\n",
    "    \n",
    "    print(\"   Reporte generado\")\n",
    "    return state\n",
    "\n",
    "# Paso 3: Funcion de routing (decide el siguiente nodo)\n",
    "def route_siguiente(state: DataScienceTeamState) -> Literal[\"analyst\", \"statistics\", \"visualization\", \"report\", \"END\"]:\n",
    "    \"\"\"\n",
    "    Determina el siguiente nodo basado en el estado.\n",
    "    Esta funcion controla el flujo condicional del grafo.\n",
    "    \"\"\"\n",
    "    siguiente = state.get(\"siguiente\", \"analyst\")\n",
    "    if siguiente == \"END\":\n",
    "        return END\n",
    "    return siguiente\n",
    "\n",
    "print(\"Funciones de nodos definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sistema Multi-Agente construido exitosamente\n",
      "\n",
      "Arquitectura:\n",
      "\n",
      "    SUPERVISOR\n",
      "         |\n",
      "    +----|----+----------+-------------+\n",
      "    |         |          |             |\n",
      " ANALYST  STATISTICS  VISUALIZATION  REPORT\n",
      "    |         |          |             |\n",
      "    +----+----|----------+-------------+\n",
      "         |\n",
      "    SUPERVISOR (loop hasta completar)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construir el Grafo del Sistema Multi-Agente\n",
    "\n",
    "# Crear el grafo con el tipo de estado definido\n",
    "workflow = StateGraph(DataScienceTeamState)\n",
    "\n",
    "# Agregar todos los nodos al grafo\n",
    "workflow.add_node(\"supervisor\", nodo_supervisor)\n",
    "workflow.add_node(\"analyst\", nodo_analyst)\n",
    "workflow.add_node(\"statistics\", nodo_statistics)\n",
    "workflow.add_node(\"visualization\", nodo_visualization)\n",
    "workflow.add_node(\"report\", nodo_report)\n",
    "\n",
    "# Definir el punto de entrada: siempre empieza en supervisor\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# Definir edges condicionales desde supervisor\n",
    "# El supervisor decide dinamicamente que agente ejecutar\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    route_siguiente,\n",
    "    {\n",
    "        \"analyst\": \"analyst\",\n",
    "        \"statistics\": \"statistics\",\n",
    "        \"visualization\": \"visualization\",\n",
    "        \"report\": \"report\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Cada agente vuelve al supervisor despues de ejecutar\n",
    "# Esto crea un loop: supervisor -> agente -> supervisor\n",
    "workflow.add_edge(\"analyst\", \"supervisor\")\n",
    "workflow.add_edge(\"statistics\", \"supervisor\")\n",
    "workflow.add_edge(\"visualization\", \"supervisor\")\n",
    "\n",
    "# El report es el nodo final: report -> END\n",
    "workflow.add_edge(\"report\", END)\n",
    "\n",
    "# Compilar el grafo para poder ejecutarlo\n",
    "app_multiagente = workflow.compile()\n",
    "\n",
    "print(\"Sistema Multi-Agente construido exitosamente\")\n",
    "print(\"\\nArquitectura:\")\n",
    "print(\"\"\"\n",
    "    SUPERVISOR\n",
    "         |\n",
    "    +----|----+----------+-------------+\n",
    "    |         |          |             |\n",
    " ANALYST  STATISTICS  VISUALIZATION  REPORT\n",
    "    |         |          |             |\n",
    "    +----+----|----------+-------------+\n",
    "         |\n",
    "    SUPERVISOR (loop hasta completar)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Ejecutar el Sistema Multi-Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INICIANDO SISTEMA MULTI-AGENTE DE DATA SCIENCE\n",
      "================================================================================\n",
      "\n",
      "Tarea: Analisis exploratorio completo (EDA) del dataset de ventas\n",
      "Dataset: ventas_ejemplo.csv\n",
      "\n",
      "================================================================================\n",
      "INICIANDO WORKFLOW...\n",
      "================================================================================\n",
      "\n",
      "SUPERVISOR: Analizando tarea...\n",
      "   -> Delegando a: Data Analyst Agent (exploracion inicial)\n",
      "\n",
      "DATA ANALYST: Explorando dataset...\n",
      "   Exploracion completada\n",
      "\n",
      "SUPERVISOR: Analizando tarea...\n",
      "   -> Delegando a: Statistics Agent (calculo de metricas)\n",
      "\n",
      "STATISTICS AGENT: Calculando metricas...\n",
      "   Metricas calculadas\n",
      "\n",
      "SUPERVISOR: Analizando tarea...\n",
      "   -> Delegando a: Visualization Agent (creacion de graficos)\n",
      "\n",
      "VISUALIZATION AGENT: Creando graficos...\n",
      "   Graficos creados\n",
      "\n",
      "SUPERVISOR: Analizando tarea...\n",
      "   -> Delegando a: Report Agent (generacion de reporte final)\n",
      "\n",
      "REPORT AGENT: Generando reporte final...\n",
      "   Reporte generado\n",
      "\n",
      "================================================================================\n",
      "WORKFLOW COMPLETADO\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "REPORTE DE ANALISIS DE DATOS - EQUIPO DE DATA SCIENCE\n",
      "================================================================================\n",
      "\n",
      "Dataset: ventas_ejemplo.csv\n",
      "Tarea: Analisis exploratorio completo (EDA) del dataset de ventas\n",
      "Fecha: 2025-11-24 21:58:03\n",
      "\n",
      "================================================================================\n",
      "1. EXPLORACION INICIAL (Data Analyst Agent)\n",
      "================================================================================\n",
      "\n",
      "El dataset 'ventas_ejemplo.csv' presenta la siguiente estructura:\n",
      "\n",
      "### Dimensiones\n",
      "- **Filas:** 50\n",
      "- **Columnas:** 8\n",
      "\n",
      "### Columnas y Tipos de Datos\n",
      "1. **fecha:** object (tipo de dato que generalmente representa fechas)\n",
      "2. **producto:** object (nombre del producto vendido)\n",
      "3. **categoria:** object (categoría a la que pertenece el producto)\n",
      "4. **precio:** float64 (precio del producto)\n",
      "5. **cantidad:** int64 (cantidad de productos vendidos)\n",
      "6. **vendedor:** object (nombre del vendedor)\n",
      "7. **region:** object (región donde se realizó la venta)\n",
      "8. **cliente_id:** object (identificación del cliente)\n",
      "\n",
      "### Valores Nulos\n",
      "- No hay valores nulos en el dataset.\n",
      "\n",
      "### Primeras 5 Filas\n",
      "|    | fecha       | producto          | categoria   | precio  | cantidad | vendedor      | region | cliente_id |\n",
      "|----|-------------|-------------------|-------------|---------|----------|----------------|--------|------------|\n",
      "| 0  | 2024-01-15  | Laptop HP         | Electronica | 899.99  | 2        | Juan Perez     | Norte  | C001       |\n",
      "| 1  | 2024-01-16  | Mouse Logitech     | Accesorios  | 25.50   | 5        | Maria Garcia   | Sur    | C002       |\n",
      "| 2  | 2024-01-16  | Teclado Mecanico  | Accesorios  | 89.99   | 3        | Juan Perez     | Norte  | C003       |\n",
      "| 3  | 2024-01-17  | Monitor Samsung    | Electronica | 299.99  | 1        | Carlos Lopez   | Este   | C004       |\n",
      "| 4  | 2024-01-18  | Laptop Dell       | Electronica | 1299.99 | 1        | Maria Garcia   | Sur    | C005       |\n",
      "\n",
      "Este resumen proporciona una visión general de la estructura y contenido del dataset, lo que puede ser útil para análisis posteriores.\n",
      "\n",
      "Aquí tienes el análisis de la distribución de las columnas 'categoria' y 'region' en el archivo 'ventas_ejemplo.csv':\n",
      "\n",
      "### Análisis de Categorías: categoria\n",
      "============================================================\n",
      "**Distribución:**\n",
      "- Accesorios: 28 (56.0%)\n",
      "- Electrónica: 22 (44.0%)\n",
      "\n",
      "**Resumen:**\n",
      "- Valores únicos: 2\n",
      "- Categoría más frecuente: Accesorios (28 ocurrencias)\n",
      "\n",
      "---\n",
      "\n",
      "### Análisis de Categorías: region\n",
      "============================================================\n",
      "**Distribución:**\n",
      "- Norte: 13 (26.0%)\n",
      "- Sur: 13 (26.0%)\n",
      "- Este: 12 (24.0%)\n",
      "- Oeste: 12 (24.0%)\n",
      "\n",
      "**Resumen:**\n",
      "- Valores únicos: 4\n",
      "- Región más frecuente: Norte (13 ocurrencias)\n",
      "\n",
      "Si necesitas más información o un análisis adicional, házmelo saber.\n",
      "\n",
      "================================================================================\n",
      "2. ANALISIS ESTADISTICO (Statistics Agent)\n",
      "================================================================================\n",
      "\n",
      "Aquí tienes las estadísticas descriptivas completas del dataset 'ventas_ejemplo.csv':\n",
      "\n",
      "### Estadísticas Descriptivas\n",
      "```\n",
      "                     fecha       precio   cantidad     ingresos\n",
      "count                   50    50.000000  50.000000    50.000000\n",
      "mean   2024-02-07 06:43:12   363.022000   2.660000   592.663000\n",
      "min    2024-01-15 00:00:00    25.500000   1.000000    71.980000\n",
      "25%    2024-01-25 06:00:00    47.625000   1.000000   179.985000\n",
      "50%    2024-02-07 12:00:00   164.990000   2.000000   299.990000\n",
      "75%    2024-02-19 18:00:00   699.990000   3.750000   799.987500\n",
      "max    2024-03-03 00:00:00  1299.990000   7.000000  2699.970000\n",
      "std                    NaN   403.975175   1.623928   607.041843\n",
      "```\n",
      "\n",
      "### Métricas de Negocio\n",
      "- **Ingresos Totales:** $29,633.15\n",
      "- **Ingreso Promedio por Venta:** $592.66\n",
      "- **Ingreso Mediano:** $299.99\n",
      "- **Desviación Estándar:** $607.04\n",
      "- **Venta Mínima:** $71.98\n",
      "- **Venta Máxima:** $2,699.97\n",
      "\n",
      "Si necesitas más información o análisis adicional, no dudes en decírmelo.\n",
      "\n",
      "Aquí tienes el análisis de las métricas de ventas agrupadas por región en el archivo 'ventas_ejemplo.csv':\n",
      "\n",
      "### Análisis por Región\n",
      "============================================================\n",
      "\n",
      "| Región | Ingresos Total | Ingresos Promedio | Cantidad Total | Cantidad Promedio | Precio Promedio |\n",
      "|--------|----------------|-------------------|----------------|-------------------|------------------|\n",
      "| Sur    | $10,649.78     | $819.21           | 13             | 42                | $529.38          |\n",
      "| Este   | $10,399.81     | $866.65           | 12             | 19                | $574.99          |\n",
      "| Norte  | $5,235.66      | $402.74           | 13             | 34                | $209.07          |\n",
      "| Oeste  | $3,347.90      | $278.99           | 12             | 38                | $137.62          |\n",
      "\n",
      "### Top 3 Regiones por Ingresos\n",
      "1. **Sur**: $10,649.78 (13 ventas)\n",
      "2. **Este**: $10,399.81 (12 ventas)\n",
      "3. **Norte**: $5,235.66 (13 ventas)\n",
      "\n",
      "Si necesitas más información o un análisis adicional, házmelo saber.\n",
      "\n",
      "Aquí tienes el análisis de las métricas de ventas agrupadas por categoría en el archivo 'ventas_ejemplo.csv':\n",
      "\n",
      "### Análisis por Categoría\n",
      "============================================================\n",
      "\n",
      "| Categoría     | Ingresos (sum) | Ingresos (mean) | Ventas (count) | Cantidad (sum) | Cantidad (mean) | Precio (mean) |\n",
      "|---------------|----------------|------------------|----------------|----------------|-----------------|----------------|\n",
      "| Electrónica   | $23,649.67     | $1,074.99        | 22             | 33             | 729.54          | -              |\n",
      "| Accesorios    | $5,983.48      | $213.70          | 28             | 100            | 75.05           | -              |\n",
      "\n",
      "### Top 3 Categorías:\n",
      "1. **Electrónica**: $23,649.67 (22 ventas)\n",
      "2. **Accesorios**: $5,983.48 (28 ventas)\n",
      "\n",
      "Si necesitas más información o un análisis adicional, házmelo saber.\n",
      "\n",
      "================================================================================\n",
      "3. VISUALIZACIONES (Visualization Agent)\n",
      "================================================================================\n",
      "\n",
      "He creado el gráfico de barras de ingresos por región con el título \"Ingresos por Región\". Puedes verlo [aquí](sandbox:/grafico_barras_region_20251124_215752.png).\n",
      "He creado el gráfico de barras de ingresos por categoría con el título \"Ingresos por Categoria\". Puedes verlo [aquí](sandbox:/grafico_barras_categoria_20251124_215756.png).\n",
      "El gráfico de serie temporal de ventas ha sido creado y guardado como `serie_temporal_20251124_215802.png`.\n",
      "\n",
      "================================================================================\n",
      "4. CONCLUSIONES Y RECOMENDACIONES\n",
      "================================================================================\n",
      "### Resumen Ejecutivo\n",
      "\n",
      "#### Hallazgos Clave\n",
      "1. **Distribución de Ventas por Categoría**: El 56% de las ventas corresponde a la categoría de Accesorios, mientras que la categoría de Electrónica representa el 44%. Esto indica una fuerte preferencia del cliente por productos accesorios, lo que podría ser un área de enfoque para futuras estrategias de marketing.\n",
      "\n",
      "2. **Variedad de Productos**: El dataset incluye una variedad de productos, con un total de 8 columnas que abarcan información relevante como precio, cantidad, vendedor y región. Esto permite un análisis detallado de las tendencias de ventas y el rendimiento de los vendedores.\n",
      "\n",
      "3. **Rendimiento de Vendedores**: Aunque el análisis no proporciona datos específicos sobre el rendimiento individual de los vendedores, la diversidad de productos y categorías sugiere que algunos vendedores podrían estar sobresaliendo en ciertas áreas, lo que podría ser aprovechado para optimizar la asignación de recursos.\n",
      "\n",
      "4. **Cobertura Regional**: Las ventas están distribuidas en diferentes regiones, lo que sugiere que hay oportunidades para identificar áreas geográficas con mayor potencial de crecimiento y ajustar las estrategias de ventas en consecuencia.\n",
      "\n",
      "5. **Sin Valores Nulos**: La ausencia de valores nulos en el dataset garantiza la integridad de los datos, lo que es fundamental para realizar análisis precisos y tomar decisiones informadas.\n",
      "\n",
      "#### Recomendaciones Accionables\n",
      "1. **Estrategia de Marketing Focalizada**: Dado que la categoría de Accesorios representa más de la mitad de las ventas, se recomienda desarrollar campañas de marketing específicas para esta categoría, incluyendo promociones y descuentos, para maximizar las ventas.\n",
      "\n",
      "2. **Análisis de Rendimiento de Vendedores**: Implementar un análisis más profundo del rendimiento de cada vendedor, identificando aquellos que tienen un mejor desempeño en categorías específicas. Esto permitirá optimizar la capacitación y asignación de vendedores a productos y regiones donde puedan ser más efectivos.\n",
      "\n",
      "3. **Expansión Regional**: Realizar un análisis geográfico más detallado para identificar regiones con bajo rendimiento en ventas. A partir de esto, se pueden diseñar estrategias de ventas y marketing adaptadas a las características y necesidades de cada región, potenciando así el crecimiento en áreas menos explotadas.\n",
      "\n",
      "================================================================================\n",
      "FIN DEL REPORTE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Reporte guardado en: reporte_multiagente_20251124_215817.txt\n"
     ]
    }
   ],
   "source": [
    "# EJECUTAR EL SISTEMA MULTI-AGENTE\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INICIANDO SISTEMA MULTI-AGENTE DE DATA SCIENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Definir el estado inicial del sistema\n",
    "estado_inicial = {\n",
    "    \"messages\": [HumanMessage(content=\"Realizar analisis completo de ventas\")],\n",
    "    \"ruta_csv\": \"ventas_ejemplo.csv\",\n",
    "    \"tarea\": \"Analisis exploratorio completo (EDA) del dataset de ventas\",\n",
    "    \"resultados\": {},\n",
    "    \"siguiente\": \"supervisor\",\n",
    "    \"reporte_final\": \"\"\n",
    "}\n",
    "\n",
    "print(f\"\\nTarea: {estado_inicial['tarea']}\")\n",
    "print(f\"Dataset: {estado_inicial['ruta_csv']}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INICIANDO WORKFLOW...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ejecutar el sistema multi-agente\n",
    "try:\n",
    "    # invoke() ejecuta todo el grafo hasta END\n",
    "    resultado_final = app_multiagente.invoke(estado_inicial)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WORKFLOW COMPLETADO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Mostrar el reporte final\n",
    "    print(\"\\n\" + resultado_final[\"reporte_final\"])\n",
    "    \n",
    "    # Guardar reporte en archivo de texto\n",
    "    nombre_archivo = f\"reporte_multiagente_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "    with open(nombre_archivo, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(resultado_final[\"reporte_final\"])\n",
    "    \n",
    "    print(f\"\\nReporte guardado en: {nombre_archivo}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError al ejecutar sistema: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion y Proximos Pasos\n",
    "\n",
    "## Felicitaciones!\n",
    "\n",
    "Has completado esta clase completa de LangChain. Ahora sabes:\n",
    "\n",
    "- Como usar modelos de lenguaje (LLMs y Chat Models)\n",
    "- Crear prompts dinamicos con templates\n",
    "- Construir cadenas con LCEL\n",
    "- Obtener salidas estructuradas\n",
    "- Crear herramientas personalizadas\n",
    "- Construir sistemas multi-agente con LangGraph\n",
    "- Aplicar LangChain a problemas reales de Data Science\n",
    "\n",
    "## Recursos Adicionales\n",
    "\n",
    "### Documentacion Oficial:\n",
    "- [LangChain Python Docs](https://python.langchain.com/)\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangSmith Hub](https://smith.langchain.com/hub)\n",
    "\n",
    "### Tutoriales Avanzados:\n",
    "- [LangGraph Multi-Agent Workflows](https://blog.langchain.com/langgraph-multi-agent-workflows/)\n",
    "- [Multi-Agent System Tutorial](https://blog.futuresmart.ai/multi-agent-system-with-langgraph)\n",
    "- [Building Coordinated AI Agents](https://codecut.ai/building-multi-agent-ai-langgraph-tutorial/)\n",
    "- [LangGraph Agents DataCamp](https://www.datacamp.com/tutorial/langgraph-agents)\n",
    "\n",
    "### Data Science con LangChain:\n",
    "- [LangChain for EDA](https://towardsdatascience.com/langchain-for-eda-build-a-csv-sanity-check-agent-in-python/)\n",
    "- [CSV Plot Agent](https://towardsai.net/p/machine-learning/csv-plot-agent-with-langchain-streamlit-your-introduction-to-data-agents)\n",
    "- [Data Analyst Assistant](https://towardsai.net/p/machine-learning/create-your-own-data-analyst-assistant-with-langchain-agents)\n",
    "\n",
    "## Proyectos Sugeridos\n",
    "\n",
    "1. **Asistente de Analisis Automatizado**: Sistema que analice automaticamente cualquier CSV\n",
    "2. **Sistema de Recomendaciones**: Agentes que recomiendan acciones basadas en datos\n",
    "3. **Chatbot Analitico**: Interfaz conversacional para explorar datos\n",
    "4. **Pipeline ETL Inteligente**: Agentes que limpian, transforman y analizan datos\n",
    "5. **Sistema de Alertas Predictivas**: Agentes que monitorean metricas y alertan anomalias\n",
    "\n",
    "## Tips Finales\n",
    "\n",
    "1. **Experimenta**: La mejor forma de aprender es construyendo\n",
    "2. **Itera**: Los prompts mejoran con prueba y error\n",
    "3. **Monitorea costos**: Usa modelos economicos (gpt-4o-mini) durante desarrollo\n",
    "4. **Maneja errores**: Los LLMs son no deterministas, siempre valida salidas\n",
    "5. **Comunidad**: Unete al Discord de LangChain para ayuda\n",
    "\n",
    "---\n",
    "\n",
    "**Buena suerte en tu viaje con LangChain!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiagente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
