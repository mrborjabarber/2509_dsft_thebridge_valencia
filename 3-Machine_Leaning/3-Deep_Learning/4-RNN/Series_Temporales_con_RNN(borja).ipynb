{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyxPiK95lslm"
   },
   "source": [
    "Puedes utilizar estos entornos para ejecutar el código (si lo haces así, tienes que subir los datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxx6zaQ1lslm"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml3/blob/main/15_processing_sequences_using_rnns_and_cnns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/15_processing_sequences_using_rnns_and_cnns.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFXIv9qNpKzt",
    "tags": []
   },
   "source": [
    "## REDES RECURRENTES (aplicado a series temporales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IPbJEmZpKzu"
   },
   "source": [
    "Adaptado de [Hands on Machine Learning for Python](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/ch15.html#cnn_chapter), utilizando lo aprendido en las sesiones de series temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJtVEqxfpKzw"
   },
   "source": [
    "Requisitos y recomendaciones:  \n",
    "* Python 3.7 o superior\n",
    "* Tensorflow 2.8 o superior\n",
    "* Es preferible utilizar un entorno con GPU (si se quiere probar la parte LSTM y GRU mejor con Nvida, ya que Keras emplea cuRNN que es una mejora basada en CUDA para Nvidia), por ejemplo Colab de Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EuAez7imABt",
    "outputId": "4e4d7579-e012-424c-cba6-22a74c42fba4"
   },
   "outputs": [],
   "source": "# Instalamos pmdarima: librería para modelos ARIMA automáticos\n# Esta librería nos permite crear modelos ARIMA/SARIMA de forma automática\n# seleccionando los mejores hiperparámetros mediante búsqueda\n!pip install pmdarima"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WD2UDD9am9sY"
   },
   "outputs": [],
   "source": "# IMPORTANTE: pmdarima requiere específicamente numpy 1.26.4 para funcionar correctamente\n# Las versiones más recientes pueden causar incompatibilidades"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NuERDsGQmeob",
    "outputId": "2b6a22dc-eb52-4478-d226-dfb67c86f505"
   },
   "outputs": [],
   "source": "# Instalamos la versión específica de numpy compatible con pmdarima\n!pip install numpy==1.26.4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OVUWjJ7mzEr"
   },
   "outputs": [],
   "source": "# IMPORTANTE: Después de cambiar la versión de numpy, \n# debes reiniciar la sesión/kernel para que los cambios surtan efecto\n# En Google Colab o Jupyter, usa: Runtime > Restart runtime"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "VtwJSzXKmpdq",
    "outputId": "e2567eb2-cbda-43e4-afee-08e73405513f"
   },
   "outputs": [],
   "source": "# Importamos numpy y verificamos que la versión instalada sea la correcta\nimport numpy as np\nnp.__version__  # Debe mostrar 1.26.4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Piq5se2pKzx"
   },
   "outputs": [],
   "source": "# ============================================================================\n# IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n# ============================================================================\n\nimport numpy as np          # Operaciones numéricas y arrays\nimport pandas as pd         # Manipulación de datos tabulares y series temporales\nimport sys                  # Información del sistema\nimport tensorflow as tf     # Framework de Deep Learning\nimport warnings\nwarnings.filterwarnings('ignore')  # Ocultamos warnings para claridad en la salida\n\n# Librerías para modelos ARIMA tradicionales\nfrom pmdarima.arima import ARIMA, auto_arima\n\n# Métricas de evaluación\nfrom sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n\n# Análisis de series temporales\nfrom statsmodels.tsa.seasonal import seasonal_decompose  # Descomposición de series\nfrom statsmodels.tsa.stattools import adfuller           # Test de estacionariedad\n\n# Verificamos que tenemos TensorFlow 2.8 o superior\nfrom packaging import version\nassert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDaDoLQTpKzx"
   },
   "source": [
    "Algunas preconfiguraciones para hacer más \"visibles\" los gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8d4TH3NbpKzx"
   },
   "outputs": [],
   "source": "# ============================================================================\n# CONFIGURACIÓN DE MATPLOTLIB PARA GRÁFICOS MÁS LEGIBLES\n# ============================================================================\n\nimport matplotlib.pyplot as plt\n\n# Configuramos el tamaño de fuentes para que los gráficos sean más visibles\n# Esto es especialmente útil en presentaciones o clases\nplt.rc('font', size=14)           # Tamaño general de fuente\nplt.rc('axes', labelsize=14, titlesize=14)  # Tamaño de etiquetas y títulos\nplt.rc('legend', fontsize=14)     # Tamaño de la leyenda\nplt.rc('xtick', labelsize=10)     # Tamaño de etiquetas eje X\nplt.rc('ytick', labelsize=10)     # Tamaño de etiquetas eje Y"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lKUa-8Ylslo"
   },
   "source": [
    "### El problema: Predecir el uso del transporte público en Chicago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKUsefR7lslp"
   },
   "source": [
    "Como ya hicimos al ver las series temporales con modelos tradicionales, vamos a utilizar los datos diarios de utilización de autobuses y tren de la ciudad de Chicago. Te recuerdo que están extraídos de su portal de datos públicos, y te recomiendo que te des una vuelta por él. Chicago es una de las ciudades más Smart del mundo y llevan recogiendo datos de diversos temas desde hace mucho tiempo... [Chicago's Data Portal](https://data.cityofchicago.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "og1rSr9BnE7c"
   },
   "outputs": [],
   "source": "# Creamos un directorio para almacenar los datos\n# El signo ! ejecuta comandos del sistema operativo desde el notebook\n!mkdir data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEkTdLqpnGfc"
   },
   "outputs": [],
   "source": "# Movemos el archivo CSV de datos al directorio 'data/'\n# Este archivo contiene los datos de uso diario del transporte público en Chicago\n!mv CTA_-_Ridership_-_Daily_Boarding_Totals.csv data/"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5348JBpflslp"
   },
   "source": [
    "Cargemos y preparemos los datos. Recuerda en series temporales conviene convertir a datatime las fechas y usarlas como índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukpcACo5lslp"
   },
   "outputs": [],
   "source": "# ============================================================================\n# CARGA Y PREPARACIÓN DE LOS DATOS\n# ============================================================================\n\nimport pandas as pd\n\nDATA_PATH = \"./data/CTA_-_Ridership_-_Daily_Boarding_Totals.csv\"\n\n# Cargamos el CSV y parseamos automáticamente la columna de fechas\ndf = pd.read_csv(DATA_PATH, parse_dates=[\"service_date\"])\n\n# Renombramos las columnas para que sean más descriptivas y fáciles de usar\ndf.columns = [\"date\", \"day_type\", \"bus\", \"rail\", \"total\"]\n\n# Convertimos la columna 'date' a formato datetime (por si acaso)\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# IMPORTANTE: En series temporales, ordenamos por fecha y la usamos como índice\n# Esto facilita el slicing temporal (ej: df[\"2019-01\":\"2019-06\"])\ndf = df.sort_values(\"date\").set_index(\"date\")\n\n# ============================================================================\n# LIMPIEZA DE DATOS\n# ============================================================================\n\n# Eliminamos la columna 'total' porque es redundante (bus + rail)\ndf = df.drop(\"total\", axis=1)\n\n# Eliminamos filas duplicadas que existen en el dataset original\ndf = df.drop_duplicates()"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3Bj_asjlslp"
   },
   "source": [
    "Como en cualquier otro dataset echamos un vistazo, pero además siendo una serie recuerda que es importante descomponerla y analizar su estacionariedad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "2bjGc6pUlslp",
    "outputId": "99b93118-4643-460a-a7f3-da1b8d07b801"
   },
   "outputs": [],
   "source": "# Visualizamos las primeras filas del dataframe para entender su estructura\n# Columnas: day_type (tipo de día), bus (viajes en bus), rail (viajes en tren/metro)\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "DXsoqV0_lslp",
    "outputId": "1ce2009e-e0ec-438a-f479-4d696a6b2807"
   },
   "outputs": [],
   "source": "# ============================================================================\n# VISUALIZACIÓN DE LA SERIE TEMPORAL\n# ============================================================================\n\n# Graficamos el uso de transporte desde 2010 hasta enero 2020 (pre-COVID)\n# Al tener 'date' como índice, podemos hacer slicing temporal directamente\ndf[\"2010-07-11\":\"2020-01-31\"].plot(\n    grid=True,          # Añadimos una cuadrícula para facilitar la lectura\n    marker=\".\",         # Marcamos cada punto de datos\n    figsize=(15, 5)     # Tamaño del gráfico (ancho, alto)\n)\nplt.tight_layout()      # Ajusta automáticamente el espaciado del gráfico"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0LGJx7Rlslq"
   },
   "source": [
    "Como recordarás de la unidad de series temporales... ok, vale, no lo recuerdas, pero te lo recuerdo yo... estas series tenían tendencia y dos tipos de estacionalidad (semanal y anual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avhYpMuJlslq"
   },
   "source": [
    "Para ver la estacionalidad semanal, pintábamos la serie y su desplazada una semana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "LlA_NjZ-lslq",
    "outputId": "54b65f53-85e5-427a-a3b6-8e15d221954f"
   },
   "outputs": [],
   "source": "# ============================================================================\n# ANÁLISIS DE ESTACIONALIDAD SEMANAL\n# ============================================================================\n\n# Definimos el periodo que queremos visualizar\ncomienzo = \"2019-03\"\nfin = \"2019-05\"\n\nfig, axs = plt.subplots(1, 1, figsize=(8, 5))\n\n# Graficamos la serie original\ndf[comienzo:fin].plot(ax=axs, legend=False, marker=\".\")\n\n# Graficamos la serie desplazada 7 días (una semana)\n# shift(7) mueve todos los valores 7 posiciones hacia adelante\n# Si las curvas se superponen, hay estacionalidad semanal\ndf[comienzo:fin].shift(7).plot(\n    ax=axs, \n    grid=True, \n    legend=False, \n    linestyle=\"--\"  # Línea discontinua para distinguirla\n)\n\n# INTERPRETACIÓN: Si ambas líneas coinciden, significa que el patrón\n# se repite cada 7 días (lunes con lunes, martes con martes, etc.)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKAOF5z0lslq"
   },
   "source": [
    "Se ve claramente como se superponen casi perfectamente, lo que además nos invitaba a generar un modelo \"naive\" como baseline. Pero en este caso haremos uso de un modelo SARIMA como baseline. Antes descompogamos la serie mensualizandola (calculando medias por meses) para ver la estacionalidad anual:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_llqRv2lslq"
   },
   "source": [
    "(Recordemos que podíamos hacer descomposicion multiplicativa o aditiva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "lyha_i4Mlslq",
    "outputId": "1d567603-ead7-4e7c-b85d-4aa0b60759a8"
   },
   "outputs": [],
   "source": "# ============================================================================\n# DESCOMPOSICIÓN DE LA SERIE TEMPORAL PARA VER ESTACIONALIDAD ANUAL\n# ============================================================================\n\n# Para ver la estacionalidad anual, agregamos los datos por mes (promedio mensual)\n# resample(\"M\") agrupa por mes, mean() calcula el promedio\ndf_mensualizada = df.resample(\"M\").mean(numeric_only=True)\n\n# Descomponemos la serie en: Tendencia + Estacionalidad + Residuos\n# model='additive': asume que Serie = Tendencia + Estacionalidad + Ruido\n# (alternativa: 'multiplicative' donde Serie = Tendencia * Estacionalidad * Ruido)\nresult_add = seasonal_decompose(\n    df_mensualizada[\"bus\"][:\"2019-12-31\"],  # Usamos datos hasta 2019\n    model='additive',\n    extrapolate_trend='freq'  # Extrapola la tendencia en los extremos\n)\n\n# Configuramos el tamaño de la figura\nplt.rcParams.update({'figure.figsize': (6, 6)})\n\n# Visualizamos los 4 componentes: Original, Tendencia, Estacionalidad, Residuos\nresult_add.plot()\n\n# INTERPRETACIÓN:\n# - Observed: Serie original\n# - Trend: Tendencia a largo plazo (crecimiento/decrecimiento general)\n# - Seasonal: Patrón que se repite anualmente\n# - Residual: Lo que queda después de quitar tendencia y estacionalidad"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXTceHEzlslq"
   },
   "source": [
    "Se puede observar la repetición del patrón anual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIUVOyYTlslr"
   },
   "source": [
    "Además también existe una tendencia clara. En definitiva no son series estacionarias, aunque por teminar de recordar vamos a hacer el test de Dickey-Fuller aumentado o ADF test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fbs1U9UTlslr",
    "outputId": "8ebbcb87-060a-47fa-84ea-ff130bba2ba4"
   },
   "outputs": [],
   "source": "# ============================================================================\n# TEST DE ESTACIONARIEDAD: AUGMENTED DICKEY-FULLER (ADF)\n# ============================================================================\n\nfrom statsmodels.tsa.stattools import adfuller\n\n# El test ADF comprueba si una serie es estacionaria\n# H0 (hipótesis nula): La serie NO es estacionaria\n# H1 (hipótesis alternativa): La serie SÍ es estacionaria\n\nresult = adfuller(df['bus'].values)\n\n# result[1] es el p-value\n# Si p-value < 0.05 → Rechazamos H0 → La serie ES estacionaria\n# Si p-value >= 0.05 → No podemos rechazar H0 → La serie NO es estacionaria\nresult[1]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfMcebX-lslr"
   },
   "source": [
    "El p-value es mayor de 0.05 así que no podemos rechazar la hipótesis nula de no estacionariedad (como ya esperábamos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyZtgyTglslr"
   },
   "source": [
    "En definitiva, si estuvieramos intentando crear un modelo predictivo de la serie temporal, utilizaríamos un SARIMA con diferenciación (d distinto de 0) y con estacionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1DN6JkVlslr"
   },
   "source": [
    "De hecho, empleemos como baseline el SARIMA que vimos en la unidad de series temporales, primero prediciendo a 14 días vista:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4N_wWCUIlslr"
   },
   "outputs": [],
   "source": "# ============================================================================\n# MODELO BASELINE: SARIMA (Seasonal ARIMA)\n# ============================================================================\n\n# Definimos el periodo de entrenamiento\norigin, today = \"2019-01-01\", \"2019-05-31\"\n\n# Extraemos la serie de datos de 'rail' (uso del tren/metro)\n# asfreq(\"D\") asegura que la frecuencia es diaria, rellenando si faltan datos\nrail_series = df.loc[origin:today][\"rail\"].asfreq(\"D\")\n\n# Datos de validación: junio de 2019\nrail_series_valid = df.loc[\"2019-06\":\"2019-06\"][\"rail\"].asfreq(\"D\")\n\n# ============================================================================\n# CONFIGURACIÓN DEL MODELO SARIMA\n# ============================================================================\n# ARIMA(p, d, q) + Componente Estacional(P, D, Q, s)\n# \n# Parámetros no estacionales:\n#   p=1: orden autoregresivo (usa 1 valor pasado)\n#   d=0: orden de diferenciación (0 = no diferenciamos)\n#   q=1: orden de media móvil\n#\n# Parámetros estacionales:\n#   P=0: componente autoregresivo estacional\n#   D=1: diferenciación estacional (elimina estacionalidad)\n#   Q=1: componente de media móvil estacional\n#   s=7: periodo estacional (7 días = 1 semana)\n\nmodel = ARIMA(\n    order=(1, 0, 1),              # (p, d, q)\n    seasonal_order=(0, 1, 1, 7)   # (P, D, Q, s)\n)\n\n# Entrenamos el modelo con los datos históricos\nmodel = model.fit(rail_series)\n\n# Predicción de 1 día adelante\ny_pred = model.predict(1)  # Devuelve aproximadamente 427,758.6 viajes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "id": "EqSgdv5Ilslr",
    "outputId": "ac4e21d2-2ef7-4a8e-dca2-c40df7208eeb"
   },
   "outputs": [],
   "source": "# ============================================================================\n# EVALUACIÓN DEL MODELO SARIMA: Predicción a 14 días\n# ============================================================================\n\n# Tomamos los primeros 14 días de junio como valores reales\ny_valid = rail_series_valid.iloc[:14]\n\n# Predecimos los próximos 14 días (2 semanas)\ny_pred = model.predict(14)\n\n# Graficamos predicción vs realidad\nplt.plot(y_valid, label=\"Real\")\nplt.plot(y_pred, color='red', label=\"SARIMA\")\nplt.xticks(rotation=45)  # Rotamos etiquetas del eje X para mejor legibilidad\nplt.legend()\n\n# NOTA: Esta será nuestra predicción de referencia (baseline) para comparar\n# con los modelos de Deep Learning que desarrollaremos después"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKCJ1rxflsls",
    "outputId": "8f0e89bb-0214-4ed4-b0aa-1cfb91c21336"
   },
   "outputs": [],
   "source": "# ============================================================================\n# MÉTRICAS DE EVALUACIÓN DEL BASELINE\n# ============================================================================\n\n# RMSE (Root Mean Squared Error): Error cuadrático medio\n# Penaliza más los errores grandes. Mismo orden de magnitud que los datos.\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_valid, y_pred)))\n\n# MAPE (Mean Absolute Percentage Error): Error porcentual absoluto medio\n# Expresa el error como porcentaje, fácil de interpretar\n# Ej: 5% significa que en promedio nos equivocamos un 5%\nprint(\"MAPE:\", mean_absolute_percentage_error(y_valid, y_pred) * 100)\n\n# OBJETIVO: Nuestros modelos de RNN deben superar estas métricas"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "635zLelXlsls"
   },
   "source": [
    "Muy bien ahí tenemos nuestro baseline, ahora veamos que tal lo hacemos usando redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi6RIuOflsls"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4m057iUlsls"
   },
   "source": [
    "### Con MLPs o Redes densas (DNN, deep neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWGN0Nmvlsls"
   },
   "source": [
    "Las redes recurrentes además de su complejidad para \"visualizarlas\" mentalmente y la propia de tener que configurarlas como cualquier otra capa de DL, añaden la farragosa tarea de preparar los dataset de entrada... Tal como vimos al hablar de entrenamiento en las sesiones teóricas.\n",
    "\n",
    "Entre otras cosas porque ahora la red espera batches de secuencias con multiples posibilidades en los targets...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvd3o1Adlsls"
   },
   "source": [
    "Por ejemplo, si tuvieramos una serie temporal de ingresos de una empresa con valores diarios tal como:\n",
    "\n",
    "[12500, 3500, 1234, 111000, 2345, 8889, 12567]\n",
    "\n",
    "Y quisieramos predecir el día siguiente con los tres días anteriores tendríamos que construir el siguiente dataset:\n",
    "\n",
    "[12500,3500,1234], target: [111000]  \n",
    "[3500,1234,111000], target: [2345]  \n",
    "[1234,111000,2345], target: [8889]  \n",
    "[111000,2345,8889], target: [12567]  \n",
    "\n",
    "Donde ahora cada fila es una instancia y una secuencia con su target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnIhKsC3lsls"
   },
   "source": [
    "Para poder preparar los batches y el dataset de entrenamiento a partir del dataframe con los datos, Keras nos da ciertas \"facilidades\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sYGktk6Ylslt",
    "outputId": "45753307-948b-4fa4-8698-b4794939c8c9"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PREPARACIÓN DE DATASETS PARA REDES NEURONALES\n# ============================================================================\n\n# EJEMPLO SIMPLE: Cómo funciona timeseries_dataset_from_array\n# Esta función crea secuencias deslizantes (sliding windows) automáticamente\n\nmy_series = [0, 1, 2, 3, 4, 5]\n\n# Creamos un dataset donde:\n# - Cada secuencia tiene 3 valores consecutivos\n# - El target es el valor 3 pasos adelante\n# - Agrupamos en batches de 2 secuencias\nmy_dataset = tf.keras.utils.timeseries_dataset_from_array(\n    my_series,\n    targets=my_series[3:],  # Los targets están 3 pasos en el futuro\n    sequence_length=3,       # Longitud de cada secuencia de entrada\n    batch_size=2,           # Número de secuencias por batch\n    shuffle=False           # No mezclamos (para ver el orden claramente)\n)\n\n# Visualizamos los batches creados\nlist(my_dataset)\n\n# RESULTADO:\n# Batch 1: secuencias [0,1,2] y [1,2,3] con targets [3] y [4]\n# Batch 2: secuencia [2,3,4] con target [5]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuUDPonTlslt"
   },
   "source": [
    "Ha creado dos batches, el primero con las dos posibles primeras secuencias posibles de 3 intervalos [0,1,2] y [1,2,3] y el segundo con una sóla secuencia porque no tiene más datos para generarla: [2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rB5T6EHzlsly",
    "outputId": "8b22dda0-2255-4f4b-e7b6-bab05dfa2190"
   },
   "outputs": [],
   "source": "# ============================================================================\n# EJEMPLO CON DATOS REALES: Predicción de ingresos empresariales\n# ============================================================================\n\nmy_series = [12500, 3500, 1234, 111000, 2345, 8889, 12567]\n\nmy_dataset = tf.keras.utils.timeseries_dataset_from_array(\n    my_series,\n    targets=my_series[3:],  # Queremos predecir 3 días adelante\n    sequence_length=3,       # Usamos 3 días de historia\n    batch_size=2,\n    shuffle=True  # shuffle=True mezcla las secuencias (útil para entrenamiento)\n)\n\nlist(my_dataset)\n\n# INTERPRETACIÓN:\n# Cada secuencia de 3 días consecutivos predice el día que está 3 pasos adelante\n# Ejemplo: [12500, 3500, 1234] predice [111000]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRxhZSr7lsly"
   },
   "source": [
    "Hacemos tres datasets: entrenamiento, validación y test y aplicamos \"normalización\" casera, aunque al ser una serie univariante (sólo vamos a usar por ahora una única característica) no debería afectarle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbzgoXvPlsly",
    "outputId": "b0f26886-d9ff-46e8-d9e8-233c96064ecf"
   },
   "outputs": [],
   "source": "# Verificamos el valor máximo de la serie 'rail' para decidir la normalización\ndf[\"rail\"].max()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExcRSbXblsly",
    "outputId": "7ffb9a0f-ebf3-4b5c-bc62-e04b33bc15af"
   },
   "outputs": [],
   "source": "# Verificamos el valor máximo de la serie 'bus'\ndf[\"bus\"].max()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtnUS5PFlsly"
   },
   "outputs": [],
   "source": "# ============================================================================\n# DIVISIÓN Y NORMALIZACIÓN DE LOS DATOS\n# ============================================================================\n\n# Dividimos en train/validation/test y normalizamos dividiendo por 1 millón\n# Esto ayuda a que la red neuronal converja más rápido\n\n# TRAIN: Enero 2016 - Mayo 2019 (período de entrenamiento)\nrail_train = df[\"rail\"][\"2016-01\":\"2019-05\"] / 1e6\n\n# VALIDATION: Necesitamos 56 días antes de junio para poder crear secuencias\n# Abril 6 - Junio 2019 (los últimos 56 días servirán como contexto)\nrail_valid = df[\"rail\"][\"2019-04-06\":\"2019-06\"] / 1e6\n\n# TEST: Julio 2019 en adelante\nrail_test = df[\"rail\"][\"2019-07\":] / 1e6\n\n# ¿Por qué normalizar? \n# Los valores originales están en cientos de miles (ej: 500,000 viajes)\n# Dividir por 1e6 los convierte a decimales (0.5), más fáciles de procesar"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCuHdEx4lsly"
   },
   "source": [
    "De los dataframes pasamos a los datasets preparados para entrenar las capas recurrentes usando el comentado timeseries_dataset_from_array y construyendo el dataset de entrada para una predicción al día siguiente después de 56 días (es decir 8 semanas) (esta es una selección arbitraría y podríamos haber empleado otro criterio, pero son más o menos dos meses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0OZahVUlsly"
   },
   "outputs": [],
   "source": "# ============================================================================\n# CREACIÓN DE DATASETS PARA ENTRENAMIENTO Y VALIDACIÓN\n# ============================================================================\n\nseq_length = 56  # Usamos 56 días (8 semanas) para predecir el día siguiente\n\ntf.random.set_seed(42)  # Para reproducibilidad de resultados\n\n# DATASET DE ENTRENAMIENTO\ntrain_ds = tf.keras.utils.timeseries_dataset_from_array(\n    rail_train.to_numpy(),\n    targets=rail_train[seq_length:],  # Target: el día después de cada secuencia\n    sequence_length=seq_length,        # Ventana de 56 días\n    batch_size=32,                     # 32 secuencias por batch\n    shuffle=True,                      # Mezclamos para mejor generalización\n    seed=42\n)\n# IMPORTANTE: shuffle=True significa que NO tomamos secuencias consecutivas\n# sino que las mezclamos aleatoriamente. Esto evita que la red se sobreajuste\n# a patrones temporales específicos del orden de entrenamiento\n\n# DATASET DE VALIDACIÓN\nvalid_ds = tf.keras.utils.timeseries_dataset_from_array(\n    rail_valid.to_numpy(),\n    targets=rail_valid[seq_length:],\n    sequence_length=seq_length,\n    batch_size=32\n)\n# NOTA: En validación NO mezclamos (shuffle=False por defecto)\n# Queremos evaluar en orden cronológico: días 1-56 predicen día 57, etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "od9ttza7lslz",
    "outputId": "82a2fc59-85c2-45bd-adb0-4402be1d85a4"
   },
   "outputs": [],
   "source": "# Visualizamos un batch aleatorio (el número 14) del dataset de entrenamiento\n# Esto nos ayuda a entender la estructura de los datos\nlist(train_ds)[14]\n\n# ESTRUCTURA:\n# (array de secuencias, array de targets)\n# Cada secuencia tiene 56 valores, cada target es 1 valor"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08O_Nu1Slslz",
    "outputId": "21e0a7d2-0a12-4003-d974-d7c8f21ffe32"
   },
   "outputs": [],
   "source": "# Visualizamos el primer batch del dataset de validación\nlist(valid_ds)[0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QzV4lOA9lslz",
    "outputId": "63207931-2f3f-44b9-97bd-adc369dffb44"
   },
   "outputs": [],
   "source": "# Verificamos los primeros 57 días de validación para entender qué predecimos\nrail_valid.iloc[:57]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9IIoW6Jlslz"
   },
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55iyuWpjlslz"
   },
   "source": [
    "### Construyendo el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhGaCjCNlslz"
   },
   "source": [
    "Vamos a construir un modelo supersimple sin capa oculta, sólo una neurona sin función de activación (es decir una capa de salida de un modelo de regresión):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C1Y_4eQ1lslz"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PRIMER MODELO: REGRESIÓN LINEAL SIMPLE (RED DENSA)\n# ============================================================================\n\ntf.random.set_seed(42)  # Reproducibilidad\n\n# Modelo MUY simple: Solo una capa densa con 1 neurona\n# Es equivalente a una regresión lineal sobre los 56 días anteriores\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(1, input_shape=[seq_length])\n    # input_shape=[56]: recibe 56 valores (los 56 días)\n    # 1 neurona: devuelve 1 valor (la predicción del día siguiente)\n    # Sin función de activación = regresión lineal\n])\n\n# Configuración del entrenamiento\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_mae\",           # Monitoreamos el error absoluto en validación\n    patience=50,                 # Esperamos 50 épocas sin mejora antes de parar\n    restore_best_weights=True    # Al terminar, restauramos los mejores pesos\n)\n\n# Optimizador SGD (Stochastic Gradient Descent) con momentum\nopt = tf.keras.optimizers.SGD(learning_rate=0.02, momentum=0.9)\n\n# Compilación del modelo\nmodel.compile(\n    loss=tf.keras.losses.Huber(),  # Función de pérdida robusta a outliers\n    optimizer=opt,\n    metrics=[\"mae\"]  # Mean Absolute Error como métrica de seguimiento\n)\n\n# ¿Por qué Huber Loss?\n# Es más robusta que MSE cuando hay valores atípicos (outliers)\n# Para errores pequeños se comporta como MSE (cuadrática)\n# Para errores grandes se comporta como MAE (lineal)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQhuTT27lsl0"
   },
   "source": [
    "Antes de entrenar, entendamos que estamos haciendo: Una regresión lineal de los 56 días anteriores al que quiero predecir, es parecido a hacer un ARIMA con p = 56, d = 0, q = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gndgF6dZlsl0",
    "outputId": "122da708-79d4-4607-cd32-8019f1f0e64b"
   },
   "outputs": [],
   "source": "# Entrenamos el modelo\n# Máximo 500 épocas, pero el early stopping probablemente parará antes\nhistory = model.fit(\n    train_ds, \n    validation_data=valid_ds, \n    epochs=500,\n    callbacks=[early_stopping_cb]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O94qr-w2lsl0",
    "outputId": "6b183615-6eee-4880-8a0a-e6eb3921b095"
   },
   "outputs": [],
   "source": "# Evaluamos el modelo en el conjunto de validación\nvalid_loss, valid_mae = model.evaluate(valid_ds)\n\n# Desnormalizamos el error multiplicando por 1e6\n# Recordemos que dividimos por 1e6, ahora multiplicamos para volver a escala original\nvalid_mae * 1e6\n\n# IMPORTANTE: Este MAE es para predicciones de 1 día adelante\n# No es directamente comparable con el SARIMA que predecía 14 días"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWwRgjx3lsl0"
   },
   "source": [
    "Hmm, parece mucho mejor... ?verdad? Pues no exactamente, porque si te fijas esta validación es día a día (es decir no prediciendo con los datos hasta hoy los próximos 14 días, sino prediciendo cada uno de los próximos 30 días, los de junio de 2019 que es el validation set, con los 56 días anteriores a cada día). En sesiones posteriores veremos la comparación correcta con el baseline de SARIMA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_q3xgoulsl0"
   },
   "source": [
    "Por otro lado y aunque lo parezca, no hemos hecho uso del orden de los datos, es decir tal como hemos entrenado podríamos haber desordenado las secuencias internamente (cambiando el orden de la misma forma en todas, por ejemplo intercambiando el día 23 con el 47 y el 12 con el 2, etc) que hubiera salido el mismo resultado. No estamos teniendo en cuenta el orden y para eso introduciremos las redes recurrentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HptTqIYGlsl0"
   },
   "source": [
    "Pero antes de terminar un pequeño inciso para introducir la función de pérdida Huber (sí, si te has fijado bien es la que hemos usado):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZCDGpGulsl0"
   },
   "source": [
    "### Función de pérdida Huber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVUfBv6mlsl1"
   },
   "source": [
    "La función de pérdida de Huber es una combinación de error cuadrático medio (MSE, por sus siglas en inglés) y error absoluto medio (MAE, por sus siglas en inglés), diseñada para ser robusta a outliers en los datos. La principal diferencia entre la pérdida de Huber y la MSE radica en cómo tratan los errores grandes:\n",
    "\n",
    "**MSE (Mean Squared Error)**: Calcula el promedio de los cuadrados de los errores entre los valores predichos y los reales. Tiende a penalizar mucho los errores grandes, lo que puede llevar a una sensibilidad excesiva a outliers en el conjunto de datos.  \n",
    "\n",
    "**Pérdida de Huber**: Es menos sensible a los outliers que MSE. Para errores pequeños, funciona como MSE, y para errores grandes, se comporta como MAE, haciendo que la pérdida sea lineal en vez de cuadrática con respecto a la diferencia entre el valor predicho y el real. Esto se logra mediante un (hiper)parámetro delta (δ), que define el umbral entre tratar un error como grande o pequeño."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SA0GcrXwlsl1"
   },
   "source": [
    "#### ¿Cuándo usar la pérdida de Huber frente a la MSE?\n",
    "\n",
    "La pérdida de Huber se prefiere sobre MSE en situaciones donde hay una expectativa de outliers en los datos, o cuando no se desea que los errores grandes dominen la función de pérdida. Es decir, si tu conjunto de datos incluye valores anómalos que podrían afectar negativamente el proceso de entrenamiento del modelo con MSE, la pérdida de Huber puede ofrecer un enfoque más equilibrado y robusto.\n",
    "\n",
    "Por otro lado, MSE puede ser preferible en situaciones donde todos los errores se consideran igualmente importantes, y se desea penalizar más fuertemente los errores grandes para enfocarse en minimizar estos errores específicos durante el entrenamiento.\n",
    "\n",
    "En nuestro caso podríamos haber empleado una MSE, pero como hay variaciones fuertes entre los años usamos una Huber, además por defecto delta (δ) vale 1, al haber normalizado al millón lo que estamos diciendo es que penalice más los errores superiores al millón (considerándolos como outliers y penalizándolos de forma cuadrática) y los errores inferiores al millón los considere equilibrados (pesándolos en términos absolutos). Si tu rango de valores es diferente tendrás que ajustar delta de forma adecuada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TioV5xW6lsl1"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_7GS_CMlsl1"
   },
   "source": [
    "## Using a Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSMeJYGZlsl1"
   },
   "source": [
    "Ahora vamos a emplear una única capa con una única celda o neurona recurrente sencilla (el hidden_stat(t) = output(t-1) y hidden_state(0) = 0)\n",
    "Es interesante darse cuenta de que la función de activación no es una relu es una tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0FLh_w6lsl1"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PRIMERA RED RECURRENTE: SimpleRNN con 1 neurona\n# ============================================================================\n\ntf.random.set_seed(42)  # Reproducibilidad\n\nmodel = tf.keras.Sequential([\n    # SimpleRNN: Capa recurrente básica\n    # 1 neurona recurrente que procesa la secuencia paso a paso\n    # input_shape=[None, 1]: \n    #   - None: secuencias de longitud variable\n    #   - 1: una característica por paso temporal (solo el valor de 'rail')\n    tf.keras.layers.SimpleRNN(1, input_shape=[None, 1])\n])\n\n# DIFERENCIA CLAVE con la red densa anterior:\n# - Red densa: trata los 56 días como 56 características independientes\n# - SimpleRNN: procesa los 56 días SECUENCIALMENTE, manteniendo memoria\n#\n# La RNN tiene \"memoria\" porque el estado oculto de t-1 influye en t\n# Fórmula: h(t) = tanh(W_x * x(t) + W_h * h(t-1) + b)\n# \n# NOTA: La función de activación por defecto es tanh (no ReLU)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfjMFqU0lsl1"
   },
   "outputs": [],
   "source": "# ============================================================================\n# FUNCIÓN AUXILIAR: Entrenar y Evaluar modelos\n# ============================================================================\n\ndef fit_and_evaluate(model, train_set, valid_set, learning_rate, epochs=500, patience=None):\n    \"\"\"\n    Función reutilizable para entrenar y evaluar modelos de series temporales\n    \n    Args:\n        model: Modelo de Keras a entrenar\n        train_set: Dataset de entrenamiento\n        valid_set: Dataset de validación\n        learning_rate: Tasa de aprendizaje para SGD\n        epochs: Número máximo de épocas (default: 500)\n        patience: Épocas sin mejora antes de parar (default: 10% de epochs)\n    \n    Returns:\n        MAE en validación (desnormalizado, en escala original)\n    \"\"\"\n    # Si no especifican patience, usamos el 10% de las épocas\n    patience = int(epochs // 10) if patience == None else patience\n    \n    # Early stopping para evitar sobreajuste\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_mae\", \n        patience=patience, \n        restore_best_weights=True\n    )\n    \n    # Optimizador SGD con momentum (ayuda a escapar de mínimos locales)\n    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n    \n    # Compilamos el modelo\n    model.compile(\n        loss=tf.keras.losses.Huber(),  # Robusta a outliers\n        optimizer=opt, \n        metrics=[\"mae\"]\n    )\n    \n    # Entrenamos\n    history = model.fit(\n        train_set, \n        validation_data=valid_set, \n        epochs=epochs,\n        callbacks=[early_stopping_cb]\n    )\n    \n    # Evaluamos y desnormalizamos\n    valid_loss, valid_mae = model.evaluate(valid_set)\n    return valid_mae * 1e6  # Volvemos a escala original"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZYQQHhrlsl2",
    "outputId": "72a23b03-dbea-44e9-e666-0c417b1a7202"
   },
   "outputs": [],
   "source": "# Entrenamos y evaluamos la SimpleRNN de 1 neurona\nfit_and_evaluate(model, train_ds, valid_ds, learning_rate=0.02)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puQRyNbdlsl2"
   },
   "source": [
    "Y ahora ya vamos a emplear una capa con 32 neurona y luego una capa densa que nos de la regresión, sin función de activación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YoiS69Ulsl2"
   },
   "outputs": [],
   "source": "# ============================================================================\n# RED RECURRENTE MEJORADA: SimpleRNN con 32 neuronas + Capa Densa\n# ============================================================================\n\ntf.random.set_seed(42)\n\nunivar_model = tf.keras.Sequential([\n    # Capa recurrente con 32 neuronas\n    # Esto permite capturar 32 \"características temporales\" diferentes\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),\n    \n    # Capa densa de salida\n    # Combina las 32 salidas de la RNN en una única predicción\n    tf.keras.layers.Dense(1)  # Sin activación = regresión\n])\n\n# ARQUITECTURA:\n# Input: secuencia de 56 días → SimpleRNN(32 neuronas) → Dense(1) → Predicción\n#\n# La capa SimpleRNN devuelve solo el último estado oculto (32 valores)\n# La capa Dense convierte esos 32 valores en 1 predicción"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YJSSTLjlsl2",
    "outputId": "4167eec7-63b7-476f-fe21-4ceda434941f"
   },
   "outputs": [],
   "source": "# Entrenamos el modelo con mayor learning rate (0.05 vs 0.02)\n# Más neuronas permiten un learning rate más alto sin divergir\nfit_and_evaluate(univar_model, train_ds, valid_ds, learning_rate=0.05)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBEPrJfwlsl2"
   },
   "source": [
    "Esto ya es otra cosa. Mejor que una capa con una sola neurona, veamos ahora (bueno en la siguiente sesión) como funciona con varias capas recurrentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7Xdn1BElsl2"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y57qn_3zlsl2"
   },
   "source": [
    "### Deep RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag61twwYlsl2"
   },
   "source": [
    "Hora de aplicar unas cuantas capas de recurrentes a ver si captan más patrones temporales y mejora nuestro regresor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g83pSgp8lsl3"
   },
   "outputs": [],
   "source": "# ============================================================================\n# DEEP RNN: Múltiples capas recurrentes apiladas\n# ============================================================================\n\ntf.random.set_seed(42)\n\ndeep_model = tf.keras.Sequential([\n    # Primera capa RNN: return_sequences=True\n    # Esto hace que devuelva TODOS los estados ocultos, no solo el último\n    # Salida: (batch, 56, 32) - para cada uno de los 56 pasos, 32 valores\n    tf.keras.layers.SimpleRNN(32, return_sequences=True, input_shape=[None, 1]),\n    \n    # Segunda capa RNN: también devuelve todas las secuencias\n    # Entrada: (batch, 56, 32) → Salida: (batch, 56, 32)\n    tf.keras.layers.SimpleRNN(32, return_sequences=True),\n    \n    # Tercera capa RNN: return_sequences=False (por defecto)\n    # Solo devuelve el último estado oculto\n    # Entrada: (batch, 56, 32) → Salida: (batch, 32)\n    tf.keras.layers.SimpleRNN(32),\n    \n    # Capa de salida\n    # Entrada: (batch, 32) → Salida: (batch, 1)\n    tf.keras.layers.Dense(1)\n])\n\n# ¿POR QUÉ return_sequences=True?\n# Cada capa RNN necesita una secuencia como entrada (excepto la última)\n# Si usáramos False en las primeras capas, solo pasaríamos 32 valores\n# a la siguiente capa, perdiendo la información temporal\n#\n# SOLO en la última capa RNN usamos return_sequences=False\n# porque la capa Dense no necesita secuencias, solo un vector"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znVPlX-blsl3",
    "outputId": "b36455c5-d193-4976-9022-768c03bd1079"
   },
   "outputs": [],
   "source": "# Entrenamos la Deep RNN\n# Learning rate más bajo (0.01) porque el modelo es más profundo\n# Modelos más profundos necesitan learning rates más conservadores\nfit_and_evaluate(deep_model, train_ds, valid_ds, learning_rate=0.01)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqQugoBBlsl3"
   },
   "source": [
    "### Series multivariantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2Oh7d5jlsl3"
   },
   "source": [
    "Pues como tenemos más series temporales que covarian con la de \"rail\", vamos a usarlas para hacer un modelo multivariante, parecido a usar ARIMAX o SARIMAX (aunque más potente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMdxuCJhlsl3"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PREPARACIÓN DE DATOS MULTIVARIANTES\n# ============================================================================\n\n# Ahora usaremos MÚLTIPLES series temporales como entrada\n# No solo 'rail', también 'bus' y el tipo de día siguiente\n\n# Seleccionamos las columnas 'bus' y 'rail' y normalizamos\ndf_mulvar = df[[\"bus\", \"rail\"]] / 1e6\n\n# FEATURE ENGINEERING: Añadimos el tipo de día SIGUIENTE\n# shift(-1) desplaza la columna hacia arriba (miramos 1 día al futuro)\n# Esto es información valiosa: saber si mañana es laboral/fin de semana/festivo\ndf_mulvar[\"next_day_type\"] = df[\"day_type\"].shift(-1)\n\n# Convertimos la variable categórica a variables dummy (one-hot encoding)\n# 'day_type' tiene valores como 'W' (weekday), 'A' (saturday), 'U' (sunday/holiday)\n# get_dummies crea columnas binarias: next_day_type_W, next_day_type_A, etc.\ndf_mulvar = pd.get_dummies(df_mulvar, dtype=float)  # float para compatibilidad con TF\n\n# RESULTADO: Ahora tenemos 5 columnas en lugar de 2:\n# - bus (normalizado)\n# - rail (normalizado)  \n# - next_day_type_W (1 si mañana es laboral, 0 si no)\n# - next_day_type_A (1 si mañana es sábado, 0 si no)\n# - next_day_type_U (1 si mañana es domingo/festivo, 0 si no)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "d8PFmtNXlsl3",
    "outputId": "8cd07b73-9558-4cbd-c590-b08b7267e7d7"
   },
   "outputs": [],
   "source": "# Visualizamos las primeras filas del dataframe multivariante\ndf_mulvar.head(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_KXi58Clsl3"
   },
   "outputs": [],
   "source": "# Dividimos los datos multivariantes en train/valid/test\n# Mismos periodos que antes\nmulvar_train = df_mulvar[\"2016-01\":\"2019-05\"]\nmulvar_valid = df_mulvar[\"2019-04-06\":\"2019-06\"]  # 56 días antes de junio\nmulvar_test = df_mulvar[\"2019-07\":]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad3wX08olsl4"
   },
   "outputs": [],
   "source": "# ============================================================================\n# CREACIÓN DE DATASETS MULTIVARIANTES\n# ============================================================================\n\ntf.random.set_seed(42)\n\n# DATASET DE ENTRENAMIENTO\ntrain_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_train.to_numpy(),  # Usamos TODAS las 5 columnas como entrada\n    targets=mulvar_train[\"rail\"][seq_length:],  # Target: solo predecimos 'rail'\n    sequence_length=seq_length,\n    batch_size=32,\n    shuffle=True,\n    seed=42\n)\n\n# DATASET DE VALIDACIÓN  \nvalid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_valid.to_numpy(),\n    targets=mulvar_valid[\"rail\"][seq_length:],\n    sequence_length=seq_length,\n    batch_size=32\n)\n\n# IMPORTANTE: \n# - INPUT: 5 features (bus, rail, next_day_type_W, next_day_type_A, next_day_type_U)\n# - OUTPUT: 1 target (rail)\n# \n# Estamos usando información de 'bus' y tipo de día para mejorar\n# la predicción de 'rail' (uso del tren/metro)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8G72TEyklsl4",
    "outputId": "0eabd16f-0156-4c2b-8997-e97be24f11bf"
   },
   "outputs": [],
   "source": "# Contamos cuántos batches hay en el dataset de entrenamiento\nprint(len(list(train_mulvar_ds)))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxoKW9Xjlsl4"
   },
   "source": [
    "Veamos que pinta tiene un batch cualquiera de los 38 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ctAe5eTWlsl4",
    "outputId": "a0c53c26-a07d-4c39-8122-5dd3389c31f2"
   },
   "outputs": [],
   "source": "# Visualizamos un batch aleatorio (el 23) para entender la estructura\nlist(train_mulvar_ds)[23]\n\n# ESTRUCTURA:\n# - Primer elemento: array de shape (batch_size, 56, 5)\n#   → 32 secuencias, cada una con 56 pasos temporales, cada paso con 5 features\n# - Segundo elemento: array de shape (batch_size,)\n#   → 32 targets (un valor de 'rail' por cada secuencia)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bh7QUU7Nlsl4"
   },
   "source": [
    "Y el último"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIYNg9Sjlsl4",
    "outputId": "68bc1ef4-dd44-4f56-e1c2-9852fcb4f20c"
   },
   "outputs": [],
   "source": "# Visualizamos el último batch del dataset\n# Puede tener menos de 32 secuencias si no había suficientes datos\nlist(train_mulvar_ds)[-1]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVD2weo0lsl4"
   },
   "outputs": [],
   "source": "# ============================================================================\n# MODELO MULTIVARIANTE: SimpleRNN con 5 features de entrada\n# ============================================================================\n\ntf.random.set_seed(42)\n\nmulvar_model = tf.keras.Sequential([\n    # IMPORTANTE: input_shape=[None, 5]\n    # - None: longitud de secuencia variable\n    # - 5: ahora tenemos 5 características por paso temporal (antes era 1)\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n    \n    tf.keras.layers.Dense(1)  # Salida: una predicción\n])\n\n# VENTAJA del modelo multivariante:\n# Puede aprender correlaciones entre las variables\n# Ej: Si el uso de bus baja, quizás el de tren sube\n# Ej: Si mañana es fin de semana, el uso será diferente"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R0L3vI-ulsl4",
    "outputId": "85c5f1df-066d-4be8-d230-f5763cf2e4ff"
   },
   "outputs": [],
   "source": "# Entrenamos y evaluamos el modelo multivariante\nfit_and_evaluate(\n    mulvar_model, \n    train_mulvar_ds, \n    valid_mulvar_ds,\n    learning_rate=0.05\n)\n\n# Esperamos que mejore respecto al modelo univariante\n# porque tiene más información disponible"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJBvpIlMlsl5"
   },
   "source": [
    "Hmm, ha mejorado (cosa que no logramos con los SARIMAX en su día), pero siempre con predicción a un día. En la siguiente sesión ampliaremos el horizonte temporal y ya compararemos con nuestro baseline inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBb3vkAhlsl5"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDCrA9kblsl5"
   },
   "source": [
    "## Predecir varios intervalos temporales en el futuro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPp1DVCxlsl5"
   },
   "source": [
    "Pero al igual que hicimos con el autoarima, nos gustaría predecir no sólo al día siguiente sino los x días siguientes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nia5muV-lsl5"
   },
   "source": [
    "**Caso 1**: Predecir tipo ARIMA (seq2vec)\n",
    "\n",
    "Como hacemos con el predict(num_intervalos) del autoarima (vamos prediciendo añadiendo las predicciones de días futuros como elementos de la secuencia a predecir )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6QzGgl3lsl5",
    "tags": []
   },
   "outputs": [],
   "source": "# ============================================================================\n# PREDICCIÓN SECUENCIAL: Método seq2vec (como ARIMA)\n# ============================================================================\n\n# Preparamos los datos de entrada: los primeros 56 días de validación\n# np.newaxis añade dimensiones para que tenga shape (1, 56, 1)\n# - 1er dimensión: batch size = 1\n# - 2da dimensión: secuencia de 56 días\n# - 3ra dimensión: 1 feature (solo 'rail')\nX = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Hu2Ulr1lsl5",
    "outputId": "7d6fd01b-c31c-4063-f1a9-3c1262913a6f"
   },
   "outputs": [],
   "source": "# Verificamos la forma del array de entrada\nX.shape  # Debe ser (1, 56, 1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M8-KJPNglsl5",
    "outputId": "98350095-6143-44b9-ebeb-01ba55a78263"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PREDICCIÓN ITERATIVA: Predecir 14 días hacia el futuro\n# ============================================================================\n\n# MÉTODO: Predecir un día, añadirlo a la secuencia, predecir el siguiente\n# Es como el método predict() de ARIMA\n\nfor step_ahead in range(14):\n    # Predecimos el siguiente día basándonos en la secuencia actual\n    y_pred_one = univar_model.predict(X)\n    \n    # Añadimos la predicción al final de la secuencia\n    # reshape(1, 1, 1): convertimos el valor predicho al formato correcto\n    # concatenate: pegamos la nueva predicción al final de X\n    # axis=1: concatenamos a lo largo del eje temporal (la secuencia)\n    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)\n    \n# RESULTADO: X ahora tiene 56 + 14 = 70 valores\n# Los últimos 14 son nuestras predicciones\n\n# DESVENTAJA: Los errores se acumulan. Si nos equivocamos en día 1,\n# ese error afecta a la predicción del día 2, y así sucesivamente"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "5bQPGJ3tlsl6",
    "outputId": "12622b4b-397c-41bd-8474-c76cf9968c55"
   },
   "outputs": [],
   "source": "# ============================================================================\n# VISUALIZACIÓN DE RESULTADOS: Predicción vs Realidad\n# ============================================================================\n\n# Extraemos las últimas 14 predicciones y creamos una Serie de pandas\nY_pred = pd.Series(\n    X[0, -14:, 0],  # Últimos 14 valores de X\n    index=pd.date_range(\"2019-06-01\", \"2019-06-14\")  # Fechas de junio\n)\n\n# Creamos el gráfico\nfig, ax = plt.subplots(figsize=(8, 3.5))\n\n# Graficamos los valores reales (contexto + período a predecir)\n(rail_valid * 1e6)[\"2019-04-06\":\"2019-06-14\"].plot(\n    label=\"True\", \n    marker=\".\", \n    ax=ax\n)\n\n# Graficamos las predicciones (desnormalizando)\n(Y_pred * 1e6).plot(\n    label=\"Predictions\", \n    grid=True, \n    marker=\"x\", \n    color=\"r\", \n    ax=ax\n)\n\n# Línea vertical marcando \"hoy\" (31 de mayo, último día de entrenamiento)\nax.vlines(\"2019-05-31\", 0, 1e6, color=\"k\", linestyle=\"--\", label=\"Today\")\n\n# Ajustamos los límites del eje Y para ver mejor las predicciones\nax.set_ylim([200_000, 800_000])\n\nplt.legend(loc=\"center left\")\nplt.show()\n\n# INTERPRETACIÓN:\n# - Azul: valores reales\n# - Rojo: predicciones del modelo\n# - Línea discontinua negra: marca el inicio de las predicciones"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My0Aobm6lsl6"
   },
   "outputs": [],
   "source": "# Extraemos los valores reales de junio 1-14 para comparar\ny_valid = rail_valid[\"2019-06-01\":\"2019-06-14\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mD6X9SNclsl6",
    "outputId": "815819d5-af6b-4947-d94a-eb2053e160fc"
   },
   "outputs": [],
   "source": "# ============================================================================\n# EVALUACIÓN DE LA PREDICCIÓN A 14 DÍAS\n# ============================================================================\n\n# RMSE: Error cuadrático medio (penaliza más los errores grandes)\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_valid, Y_pred)) * 1e06)\n\n# MAPE: Error porcentual absoluto medio (fácil de interpretar)\nprint(\"MAPE:\", mean_absolute_percentage_error(y_valid, Y_pred) * 100)\n\n# AHORA SÍ podemos comparar con el baseline SARIMA\n# Ambos predicen 14 días hacia el futuro"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19QxAls-lsl6"
   },
   "source": [
    "Hmmm, no está mal, ¿no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQjlnzIGlsl6"
   },
   "source": [
    "**Caso 2**: Predecir seq2seq\n",
    "\n",
    "Vamos a crear una red que entrene para predecir los 14 días siguientes de una vez.  \n",
    "\n",
    "Para ello preparamos los datos de entrada de forma que el target serán ahora los 14 días siguientes a cada instante...   \n",
    "\n",
    "Por tanto, si nuestras secuencias tienen 56 días, ahora el target serán 56 vectores de 14 valores (ejemplo 2 del surf, pero con 56 de tamaño de secuencia y 14 de predicción)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5aeLEC1Tlsl6"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PREPARACIÓN DE DATOS PARA PREDICCIÓN SEQ2SEQ\n# ============================================================================\n\ntf.random.set_seed(42)\n\n# Función auxiliar para dividir las secuencias en entrada y target\ndef split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):\n    \"\"\"\n    Divide cada secuencia en:\n    - Input: todos los valores excepto los últimos 'ahead' días\n    - Target: los últimos 'ahead' valores de la columna 'target_col'\n    \n    Args:\n        mulvar_series: secuencia de shape (batch, seq_length, features)\n        ahead: cuántos pasos predecir (default: 14)\n        target_col: índice de la columna a predecir (1 = 'rail')\n    \n    Returns:\n        (inputs, targets)\n    \"\"\"\n    # [:, :-ahead]: todos los valores excepto los últimos 14\n    # [:, -ahead:, target_col]: los últimos 14 valores de la columna 'rail'\n    return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]\n\n# DATASET DE ENTRENAMIENTO\nahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_train.to_numpy(),\n    targets=None,  # No especificamos targets aquí\n    sequence_length=seq_length + 14,  # Secuencias más largas (56+14=70)\n    batch_size=32,\n    shuffle=True,\n    seed=42\n).map(split_inputs_and_targets)  # Aplicamos la función de división\n\n# DATASET DE VALIDACIÓN\nahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(\n    mulvar_valid.to_numpy(),\n    targets=None,\n    sequence_length=seq_length + 14,\n    batch_size=32\n).map(split_inputs_and_targets)\n\n# ESTRUCTURA FINAL:\n# Input: (batch, 56, 5) - 56 días con 5 features cada uno\n# Target: (batch, 14) - 14 valores de 'rail' a predecir"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gpC0Zycxlsl6",
    "outputId": "f6e1d969-6c5a-46fa-f02d-88eb67df869b"
   },
   "outputs": [],
   "source": "# Visualizamos un batch de entrenamiento para entender la estructura\nlist(ahead_train_ds)[0]\n\n# ESTRUCTURA:\n# - Input: (32, 56, 5) - 32 secuencias de 56 días con 5 features\n# - Target: (32, 14) - 32 secuencias de 14 predicciones cada una"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Chg42RPIlsl6",
    "outputId": "a337015a-76b9-46a9-8fbd-ee1c107d07c8"
   },
   "outputs": [],
   "source": "# Visualizamos el primer batch de validación\nlist(ahead_valid_ds)[0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuhDqOXmlsl7"
   },
   "outputs": [],
   "source": "# ============================================================================\n# MODELO SEQ2SEQ: Predice 14 días directamente\n# ============================================================================\n\ntf.random.set_seed(42)\n\nahead_model = tf.keras.Sequential([\n    # Capa recurrente: procesa la secuencia de 56 días\n    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),\n    \n    # Capa de salida: 14 neuronas (una por cada día a predecir)\n    tf.keras.layers.Dense(14)\n])\n\n# DIFERENCIA con el método anterior:\n# - Método anterior (seq2vec): predicción iterativa, 1 día a la vez\n# - Este método (seq2seq): predice LOS 14 DÍAS DE UNA VEZ\n#\n# VENTAJAS:\n# - Los errores no se acumulan tanto\n# - El modelo aprende a predecir directamente 14 días\n# - Más rápido en inferencia (una sola pasada)\n#\n# DESVENTAJAS:\n# - Más difícil de entrenar\n# - Necesita más datos de entrenamiento"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2JoqTMVXlsl7",
    "outputId": "e80ec2e1-f2c1-4101-8097-8139ba791f72",
    "scrolled": true
   },
   "outputs": [],
   "source": "# Entrenamos el modelo seq2seq\nfit_and_evaluate(\n    ahead_model, \n    ahead_train_ds, \n    ahead_valid_ds,\n    learning_rate=0.02\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqoxISEKlsl7",
    "outputId": "3d497bbd-5800-4644-b0ef-77ea88231c17"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PREDICCIÓN CON EL MODELO SEQ2SEQ\n# ============================================================================\n\n# Preparamos la entrada: los primeros 56 días de validación con 5 features\nX = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]\n\n# Predicción: una sola llamada para obtener los 14 días\nY_pred = ahead_model.predict(X)  # shape [1, 14]\n\nY_pred  # Visualizamos las predicciones"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "sbqNEvI2lsl7",
    "outputId": "7086c7f5-c5ad-4d75-815d-213251a2e257"
   },
   "outputs": [],
   "source": "# ============================================================================\n# VISUALIZACIÓN SEQ2SEQ: Predicción vs Realidad\n# ============================================================================\n\n# Convertimos las predicciones a una Serie de pandas\nY_pred = pd.Series(\n    Y_pred[0],  # Extraemos el array de 14 predicciones\n    index=pd.date_range(\"2019-06-01\", \"2019-06-14\")\n)\n\n# Creamos el gráfico\nfig, ax = plt.subplots(figsize=(8, 3.5))\n\n# Valores reales (desnormalizados)\n(rail_valid * 1e6)[\"2019-04-06\":\"2019-06-14\"].plot(\n    label=\"True\", \n    marker=\".\", \n    ax=ax\n)\n\n# Predicciones (desnormalizadas)\n(Y_pred * 1e6).plot(\n    label=\"Predictions\", \n    grid=True, \n    marker=\"x\", \n    color=\"r\", \n    ax=ax\n)\n\n# Marcamos el día de \"hoy\" (fin del período de entrenamiento)\nax.vlines(\"2019-05-31\", 0, 1e6, color=\"k\", linestyle=\"--\", label=\"Today\")\nax.set_ylim([200_000, 800_000])\n\nplt.legend(loc=\"center left\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3j40vAR8lsl7",
    "outputId": "408109dc-b02b-46d9-fcbb-2a4cf70f4917"
   },
   "outputs": [],
   "source": "# ============================================================================\n# EVALUACIÓN DEL MODELO SEQ2SEQ\n# ============================================================================\n\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_valid, Y_pred)) * 1e06)\nprint(\"MAPE:\", mean_absolute_percentage_error(y_valid, Y_pred) * 100)\n\n# Compara estos resultados con:\n# 1. El baseline SARIMA\n# 2. El método seq2vec anterior\n# ¿Cuál funciona mejor?"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C35S3yelsl7"
   },
   "source": [
    "Una mejora sobre el baseline aunque no sobre la predicción sobre la predicción. La mejora puede ser debida a la predicción a 14 días entrenando con secuencias de targets o bien a que hemos empleado el modelo de series multivariante. Te dejo como ejercicio el que pruebes con el modelo univariante y predicción a 14 días.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnLbcZ57lsl7"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOqPVjKLlsl7"
   },
   "source": [
    "Las celdas sencillas recurrentes no son las más utilizadas hoy en día, sino que existen otros dos tipos de celdas/neuronas recurrentes que buscan mejorar ese mecanismo de hipotética memoria.   \n",
    "\n",
    "Estas dos celdas son la LSTM (Long-short term memory) que intenta regular el impacto de periodos o elementos de la secuencia más lejanos y de los más cercanos al punto tratado. Por otro lado la celda GRU (Gated Recurrent Unit), simplifica la anterior pero también intentando regular el impacto de los diferentes elementos de la secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3lZ3RrAlsl7"
   },
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeUJip-6lsl8"
   },
   "source": [
    "Las celdas LSTM (Long-Short Term Memory), buscan aumentar la capacidad de \"memoria\". Para ello ahora además de un hidden_state, devuelven un c_state en lo que vendría a ser hidden_state -> memoria a corto plazo, c_state -> memoria a largo plazo.\n",
    "\n",
    "\n",
    "Por otro lado, las celdas LSTM se pueden incorpar como tal o usando una capa especial LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AhgSr-Alsl8",
    "scrolled": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# LSTM: Long Short-Term Memory\n# ============================================================================\n\ntf.random.set_seed(42)\n\nlstm_model = tf.keras.models.Sequential([\n    # Capa LSTM con 32 unidades\n    # LSTM es más sofisticada que SimpleRNN\n    tf.keras.layers.LSTM(32, input_shape=[None, 5]),\n    \n    # Capa de salida: 14 predicciones\n    tf.keras.layers.Dense(14)\n])\n\n# ¿QUÉ ES LSTM?\n# Long Short-Term Memory - Memoria a Corto y Largo Plazo\n#\n# PROBLEMA DE SimpleRNN:\n# - Olvida información antigua (gradient vanishing)\n# - Difícil aprender dependencias a largo plazo\n#\n# SOLUCIÓN DE LSTM:\n# - Tiene \"puertas\" (gates) que controlan qué información mantener/olvidar\n# - 3 puertas principales:\n#   1. Forget gate: decide qué olvidar del estado anterior\n#   2. Input gate: decide qué nueva información añadir\n#   3. Output gate: decide qué información sacar\n#\n# - Mantiene 2 estados:\n#   1. Cell state (c): memoria a largo plazo\n#   2. Hidden state (h): memoria a corto plazo\n#\n# VENTAJAS:\n# - Mejor para secuencias largas\n# - Captura dependencias a largo plazo\n# - Menos problemas de gradient vanishing\n#\n# DESVENTAJAS:\n# - Más parámetros (más lento de entrenar)\n# - Más complejo de entender"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JoCTb9XWlsl8",
    "outputId": "e84ec4bc-285c-4d37-fd65-4e035c7d3b33",
    "scrolled": true
   },
   "outputs": [],
   "source": "# Entrenamos el modelo LSTM\n# Learning rate más alto (0.1) porque LSTM es más estable\nfit_and_evaluate(\n    lstm_model, \n    ahead_train_ds, \n    ahead_valid_ds,\n    learning_rate=0.1, \n    epochs=500\n)\n\n# NOTA: LSTM suele converger más rápido que SimpleRNN"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rAaaJPfHlsl8",
    "outputId": "33259551-a86b-4a76-a3ec-f3e8aee26e7e"
   },
   "outputs": [],
   "source": "# Hacemos predicciones con el modelo LSTM\nX = mulvar_valid.to_numpy()[np.newaxis, :seq_length]  # shape [1, 56, 5]\nY_pred = lstm_model.predict(X)  # shape [1, 14]\nY_pred"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "mCVwcYAJlsl8",
    "outputId": "cd42d2cd-f909-4262-81e7-c5f8326fc8f2",
    "scrolled": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# VISUALIZACIÓN LSTM: Predicción vs Realidad\n# ============================================================================\n\nY_pred = pd.Series(\n    Y_pred[0],\n    index=pd.date_range(\"2019-06-01\", \"2019-06-14\")\n)\n\nfig, ax = plt.subplots(figsize=(8, 3.5))\n\n# Valores reales\n(rail_valid * 1e6)[\"2019-04-06\":\"2019-06-14\"].plot(\n    label=\"True\", \n    marker=\".\", \n    ax=ax\n)\n\n# Predicciones LSTM\n(Y_pred * 1e6).plot(\n    label=\"Predictions\", \n    grid=True, \n    marker=\"x\", \n    color=\"r\", \n    ax=ax\n)\n\nax.vlines(\"2019-05-31\", 0, 1e6, color=\"k\", linestyle=\"--\", label=\"Today\")\nax.set_ylim([200_000, 800_000])\n\nplt.legend(loc=\"center left\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vnzbgi2rlsl8",
    "outputId": "1b0295a1-84e5-411f-f3b7-27419c53ab47"
   },
   "outputs": [],
   "source": "# ============================================================================\n# EVALUACIÓN DEL MODELO LSTM\n# ============================================================================\n\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_valid, Y_pred)) * 1e06)\nprint(\"MAPE:\", mean_absolute_percentage_error(y_valid, Y_pred) * 100)\n\n# ¿Mejora LSTM respecto a SimpleRNN?\n# Compara con los resultados anteriores"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U7wwxEFlsl9"
   },
   "source": [
    "# GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3O-b64Vlsl9"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Las LSTM y las GRU son mucho más efectivas que las simples RNN y son las que se usan hoy en día. Como las LSTM, las GRU tienen su propia capa aunque puedes utilizarlas como celdas en una RNN layer (no en una SimpleRNN) y el funcionamiento es similar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hiJFxLxClsl9",
    "scrolled": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# GRU: Gated Recurrent Unit\n# ============================================================================\n\ntf.random.set_seed(42)\n\ngru_model = tf.keras.Sequential([\n    # Capa GRU con 32 unidades\n    # GRU es una simplificación de LSTM\n    tf.keras.layers.GRU(32, input_shape=[None, 1]),\n    \n    # Capa de salida\n    tf.keras.layers.Dense(1)\n])\n\n# ¿QUÉ ES GRU?\n# Gated Recurrent Unit - Unidad Recurrente con Puertas\n#\n# RELACIÓN CON LSTM:\n# - Versión simplificada de LSTM\n# - Solo 2 puertas (vs 3 en LSTM):\n#   1. Reset gate: decide qué olvidar del pasado\n#   2. Update gate: decide cuánta información nueva añadir\n#\n# - Solo 1 estado (vs 2 en LSTM):\n#   - Hidden state (h) que combina memoria corto y largo plazo\n#\n# VENTAJAS vs LSTM:\n# - Menos parámetros (más rápido de entrenar)\n# - Más fácil de entrenar\n# - Generalmente similar rendimiento a LSTM\n# - Mejor para datasets pequeños\n#\n# VENTAJAS vs SimpleRNN:\n# - Mejor memoria a largo plazo\n# - Menos gradient vanishing\n#\n# CUÁNDO USAR:\n# - Prueba GRU primero (más rápido)\n# - Si no funciona bien, prueba LSTM (más potente)\n# - SimpleRNN solo para casos muy simples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d-Z4OkBlsl9",
    "outputId": "07fd7195-8a50-4dec-de22-993ff1ec992c",
    "scrolled": true
   },
   "outputs": [],
   "source": "# Entrenamos el modelo GRU\n# Usamos el dataset univariante (solo 'rail')\nfit_and_evaluate(\n    gru_model, \n    train_ds, \n    valid_ds, \n    learning_rate=0.1, \n    epochs=500\n)\n\n# NOTA: Comparamos GRU univariante para ver si con menos datos\n# puede competir con los modelos multivariantes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnCgUwwYlsl9"
   },
   "outputs": [],
   "source": "# ============================================================================\n# PREDICCIÓN ITERATIVA CON GRU\n# ============================================================================\n\n# Preparamos la entrada inicial\nX = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33QQtb0Ilsl9",
    "outputId": "2d392839-4edd-45d2-aa81-56a505ac168c"
   },
   "outputs": [],
   "source": "# Predecimos 14 días de forma iterativa (como con SimpleRNN)\nfor step_ahead in range(14):\n    y_pred_one = gru_model.predict(X)\n    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "2wKPhG_Glsl9",
    "outputId": "90e32633-b4cc-4af6-e8af-30267f4165fd"
   },
   "outputs": [],
   "source": "# ============================================================================\n# VISUALIZACIÓN GRU: Predicción vs Realidad\n# ============================================================================\n\nY_pred = pd.Series(\n    X[0, -14:, 0],\n    index=pd.date_range(\"2019-06-01\", \"2019-06-14\")\n)\n\nfig, ax = plt.subplots(figsize=(8, 3.5))\n\n# Valores reales\n(rail_valid * 1e6)[\"2019-04-06\":\"2019-06-14\"].plot(\n    label=\"True\", \n    marker=\".\", \n    ax=ax\n)\n\n# Predicciones GRU\n(Y_pred * 1e6).plot(\n    label=\"Predictions\", \n    grid=True, \n    marker=\"x\", \n    color=\"r\", \n    ax=ax\n)\n\nax.vlines(\"2019-05-31\", 0, 1e6, color=\"k\", linestyle=\"--\", label=\"Today\")\nax.set_ylim([200_000, 800_000])\n\nplt.legend(loc=\"center left\")\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqU7pMRFlsl-",
    "outputId": "a8613e6a-df1f-445b-dc7b-f1dbabd8141e"
   },
   "outputs": [],
   "source": "# ============================================================================\n# EVALUACIÓN FINAL: GRU\n# ============================================================================\n\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_valid, Y_pred)) * 1e06)\nprint(\"MAPE:\", mean_absolute_percentage_error(y_valid, Y_pred) * 100)\n\n# COMPARACIÓN FINAL:\n# ------------------\n# Ahora tienes resultados de:\n# 1. SARIMA (baseline estadístico)\n# 2. SimpleRNN (red recurrente básica)\n# 3. Deep SimpleRNN (múltiples capas)\n# 4. LSTM (memoria a largo plazo)\n# 5. GRU (versión simplificada de LSTM)\n#\n# PREGUNTAS PARA REFLEXIONAR:\n# - ¿Cuál modelo tiene mejor RMSE?\n# - ¿Cuál modelo tiene mejor MAPE?\n# - ¿Vale la pena la complejidad extra de LSTM/GRU?\n# - ¿Los modelos multivariantes mejoran significativamente?\n# - ¿El método seq2seq es mejor que la predicción iterativa?\n#\n# CONCLUSIONES TÍPICAS:\n# - GRU y LSTM suelen ser mejores que SimpleRNN\n# - Modelos multivariantes suelen superar a univariantes\n# - La predicción iterativa acumula errores\n# - El método seq2seq es más estable pero más difícil de entrenar\n# - A veces, un buen SARIMA puede competir con redes neuronales simples"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9e9I86QKlsl-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMQcogBNlsl-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}