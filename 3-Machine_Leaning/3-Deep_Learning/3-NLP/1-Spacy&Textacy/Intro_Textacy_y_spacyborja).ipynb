{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dM0AU5JmpZa3"
   },
   "source": [
    "# Textacy & Spacy\n",
    "Librerias de procesado de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Actualizamos pip a la última versión\n# pip es el gestor de paquetes de Python\n# Comando desactivado (comentado con #) - ejecutar solo si es necesario\n#!pip install --user  --upgrade pip "
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# INSTALACIÓN DE SPACY Y MODELOS DE LENGUAJE\n# ========================================\n\n# Instalamos la librería spacy\n# Comando desactivado - ejecutar solo en la primera instalación\n#!pip install -U spacy\n\n# Descargamos el modelo pequeño de inglés\n# 'sm' = small (modelo pequeño, más rápido pero menos preciso)\n#!python -m spacy download en_core_web_sm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instalamos textacy, una librería que complementa spacy con funcionalidades adicionales\n# Comando desactivado - ejecutar solo en la primera instalación\n#%pip install textacy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Descargamos el modelo GRANDE de inglés\n# 'lg' = large (modelo grande con mejor precisión y vectores de palabras)\n# Este modelo es más pesado pero tiene mejor rendimiento\n#!python -m spacy download en_core_web_lg"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Descargamos el modelo GRANDE de español\n# Este modelo permite procesar textos en español con alta precisión\n#!python -m spacy download es_core_news_lg"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# INSTRUCCIONES DE INSTALACIÓN\n# ========================================\n\n'''\nVersiones compatibles\nDespues hay que reiniciar el entorno de ejecución\n'''\n\n# Instalamos spacy y textacy\n# Estos comandos están desactivados - solo ejecutar una vez\n# !pip install spacy\n# !pip install textacy\n\n# Para siguientes ejecuciones, solo ejecutar estos comandos para descargar los modelos\n# Después de ejecutar, REINICIAR el entorno de ejecución (Runtime > Restart Runtime)\n# !python -m spacy download en_core_web_lg  # Modelo grande de inglés\n# !python -m spacy download es_core_news_lg  # Modelo grande de español"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A5rKVBF0pKa6"
   },
   "source": [
    "# Spacy\n",
    "https://spacy.io/\n",
    "\n",
    "NOTA: Recuerda reiniciar el entorno de ejecucion despues de la instalacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5__omBtznMg"
   },
   "outputs": [],
   "source": "# Importamos la librería spacy, que es una de las mejores herramientas de NLP\nimport spacy\n\n# Cargamos el modelo pre-entrenado para inglés (large = modelo grande con mejor precisión)\n# 'en_core_web_lg' incluye vectores de palabras, reconocimiento de entidades, POS tagging, etc.\nnlp = spacy.load('en_core_web_lg')"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XvUcxUvRx3CT"
   },
   "source": [
    "## Text basics\n",
    "Veamos como trabajar cn estos primeros ejemplos con la libreria ´spacy´. Cosas que podemos hacer:\n",
    "1. Tokenizar en frases\n",
    "2. Tokenizar en palabras\n",
    "3. Acceder a los atributos de cada token\n",
    "4. Acceder a las entidades del texto\n",
    "5. Visualizar las entidades del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yx7f0Wv9ul01",
    "outputId": "da5d47cf-de20-4e1e-ebcc-d4ee4e09b77b"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 1: TOKENIZACIÓN DE FRASES\n# ========================================\n\n# Definimos el texto que queremos analizar\n# Este texto habla sobre Londres y será nuestro ejemplo de entrada\ntext = \"\"\"\nLondon is the capital and most populous city of England and \nthe United Kingdom.  Standing on the River Thames in the south east \nof the island of Great Britain, London has been a major settlement \nfor two millennia. It was founded by the Romans, who named it Londinium.\n\"\"\"\n\n# Procesamos el texto con spacy\n# Al aplicar nlp(text), spacy realiza automáticamente:\n# - Tokenización (división en palabras)\n# - POS tagging (etiquetado gramatical)\n# - Reconocimiento de entidades\n# - Análisis de dependencias sintácticas\ndoc = nlp(text)\n\n# Imprimimos el documento completo\nprint(doc)\n\n# Iteramos sobre cada frase detectada automáticamente por spacy\n# spacy usa el modelo de lenguaje para identificar dónde termina cada frase\nfor num, sentence in enumerate(doc.sents):\n  print(num, sentence)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Contamos cuántas frases tiene nuestro documento\n# doc.sents es un generador, por eso lo convertimos a lista\n# Resultado esperado: 4 frases\nlen(list(doc.sents))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 2: ANÁLISIS DE TOKENS (PALABRAS)\n# ========================================\n\n# Iteramos sobre las primeras 20 palabras del documento\n# Cada token (palabra) tiene múltiples atributos útiles:\nfor word in doc[:20]:\n  # word.text = la palabra original tal como aparece en el texto\n  # word.lemma_ = forma base de la palabra (ej: \"running\" -> \"run\")\n  # word.pos_ = Part Of Speech, la categoría gramatical (NOUN, VERB, ADJ, etc.)\n  # word.is_stop = True si es una stopword (palabras comunes como \"the\", \"is\", \"a\")\n  print(word.text, word.lemma_, word.pos_, word.is_stop)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JIe3TYa9RNfm",
    "outputId": "f90a4cd9-f411-41e5-a000-750fb5f88e7d"
   },
   "outputs": [],
   "source": "# Verificamos el tipo de objeto que devuelve spacy\n# Es un objeto de tipo Doc (documento procesado) que contiene toda la información lingüística\ntype(doc)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UpBWJ2gby81T"
   },
   "source": [
    "## Syntactic analysis\n",
    "Doing the school homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "9N42WYyyy86b",
    "outputId": "f2d56011-4783-4711-f8f7-445f2a46dfd2"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 3: ANÁLISIS SINTÁCTICO (DEPENDENCY PARSING)\n# ========================================\n\n# Importamos displacy para visualizaciones interactivas\nfrom spacy import displacy\n\n# Creamos un nuevo documento con una frase más simple para visualizar mejor\ndoc2 = nlp(\"London is the capital and most populous city of England and the United Kingdom\")\n\n# Visualizamos el árbol de dependencias sintácticas\n# style=\"dep\" muestra las relaciones gramaticales entre palabras\n# (sujeto, verbo, objeto, modificadores, etc.)\n# jupyter=True permite que se muestre correctamente en Jupyter Notebook\ndisplacy.render(doc2, jupyter=True, style=\"dep\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BhcKxFvisxNj"
   },
   "source": [
    "## Entities in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GoUNKXsSsxXy",
    "outputId": "522e7782-4aa7-45e1-ddcc-d1fdd9f6ff72"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 4: RECONOCIMIENTO DE ENTIDADES (NER)\n# ========================================\n\n# Iteramos sobre todas las entidades nombradas que spacy ha detectado automáticamente\n# Las entidades son cosas como nombres de lugares, personas, fechas, organizaciones, etc.\nfor entity in doc.ents:\n  # entity.text = el texto de la entidad\n  # entity.label_ = el tipo de entidad (GPE, PERSON, DATE, etc.)\n  # spacy.explain() nos da una descripción humana del tipo de entidad\n  print(entity.text, entity.label_, spacy.explain(entity.label_))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8o01ox8YylRj",
    "outputId": "ae38b8fb-6575-4009-96af-80fdb9ab0550"
   },
   "outputs": [],
   "source": "# Si tenemos dudas sobre qué significa alguna etiqueta, podemos usar explain()\n# Por ejemplo, ¿qué es 'ORG'?\nspacy.explain('ORG')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "UwfOcDgCtEue",
    "outputId": "eadd1e47-cebe-453a-808a-35d2904aec5b"
   },
   "outputs": [],
   "source": "# Visualizamos las entidades de forma gráfica y coloreada\n# style='ent' muestra las entidades resaltadas con diferentes colores según su tipo\n# Esto es muy útil para presentaciones y para verificar visualmente el NER\ndisplacy.render(doc, style='ent', jupyter=True)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "k4J_qGHusl9H"
   },
   "source": [
    "## Replacing names\n",
    "Hide names for GDPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8zHtOLvSsmEp",
    "outputId": "99bd35f8-5eff-4622-f119-5b8bd6426e0f"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 5: ANONIMIZACIÓN DE DATOS (GDPR COMPLIANCE)\n# ========================================\n\n# Esta función reemplaza un token individual si es un nombre de persona\ndef replace_name_with_placeholder(token):\n    # token.ent_iob indica si el token es parte de una entidad (0 = no es parte)\n    # token.ent_type_ indica el tipo de entidad (PERSON, GPE, ORG, etc.)\n    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n        # Si es una persona, la reemplazamos por \"GDPR\" para proteger la privacidad\n        return \"GDPR\"\n    else:\n        # Si no es una persona, devolvemos el texto original\n        return token.text\n\n# Esta función procesa un texto completo y oculta todos los nombres de personas\ndef scrub(text):\n    # Procesamos el texto con spacy\n    doc = nlp(text)\n    \n    # Retokenizamos para fusionar las entidades multi-palabra en un solo token\n    # Por ejemplo: \"Alan Turing\" se convierte en un solo token en lugar de dos\n    with doc.retokenize() as retokenizer:\n        for ent in doc.ents:\n            retokenizer.merge(ent)\n    \n    # Aplicamos la función de reemplazo a cada token\n    tokens = map(replace_name_with_placeholder, doc)\n    \n    # Unimos todos los tokens de vuelta en un string\n    return \" \".join(tokens)\n\n# Texto de prueba con nombres de personas famosas\ns = \"\"\"\nIn 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky's \nSyntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n\"\"\"\n\n# Aplicamos la anonimización y mostramos el resultado\n# Resultado esperado: \"Alan Turing\" y \"Noam Chomsky\" serán reemplazados por \"GDPR\"\nprint(scrub(s))"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oGvpaIAqzwOH"
   },
   "source": [
    "## Lematize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKra7djBzwXj",
    "outputId": "a1e019b7-3aad-40d1-e5b2-a4203c49f250"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 6: LEMATIZACIÓN\n# ========================================\n\n# La lematización convierte las palabras a su forma base/diccionario\n# Ejemplos:\n# - \"running\" -> \"run\"\n# - \"better\" -> \"good\"\n# - \"was\" -> \"be\"\n\n# Iteramos sobre cada palabra del documento\nfor w in doc:\n  # w.text = palabra original\n  # w.lemma_ = forma base de la palabra (lema)\n  # w.pos_ = categoría gramatical\n  print(w.text, w.lemma_, w.pos_)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "o_e1pnbgz_0X"
   },
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_M6OKU3z__q",
    "outputId": "701e21b9-d50e-4903-ca0e-c73746e56180"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 7: STOPWORDS (PALABRAS VACÍAS)\n# ========================================\n\n# Las stopwords son palabras muy comunes que generalmente no aportan mucho significado\n# Ejemplos: \"the\", \"is\", \"a\", \"an\", \"in\", \"to\", \"for\", etc.\n\n# Importamos la lista de stopwords en inglés\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# Mostramos las primeras 20 stopwords de la lista\n# Spacy tiene ~300+ stopwords en inglés\nprint(list(STOP_WORDS)[:20])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXNvYLeD0QJ0",
    "outputId": "48725150-7661-4395-a27c-2a4782b52da4"
   },
   "outputs": [],
   "source": "# Creamos una lista limpia removiendo stopwords y puntuación\n# Esta es una técnica común en NLP para reducir el ruido en el análisis\n\n# List comprehension que filtra:\n# - not palabra.is_stop: elimina stopwords (the, is, a, etc.)\n# - not palabra.is_punct: elimina signos de puntuación (. , ! ?)\nlista_clean = [palabra for palabra in doc if not palabra.is_stop and not palabra.is_punct]\n\n# Resultado: solo palabras con significado relevante\nprint(lista_clean)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_3oBFA3XacUf"
   },
   "source": [
    "# Spanish\n",
    "## Spacy  and entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WyDUzV-eax_7",
    "outputId": "e4575ab9-ce92-4d3c-a20d-a4b297eab994"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 8: PROCESAMIENTO EN ESPAÑOL\n# ========================================\n\n# Cargamos el modelo pre-entrenado para español (large)\nnlp_es = spacy.load('es_core_news_lg')\n\n# Texto de ejemplo en español (artículo sobre Londres de Wikipedia)\ntext = '''Londres (en inglés, London, pronunciado /ˈlʌndən/ ( escuchar)) es la capital y mayor ciudad de Inglaterra y del Reino Unido.2​3​ Situada a orillas del río Támesis, Londres es un importante asentamiento humano desde que fue fundada por los romanos con el nombre de Londinium hace casi dos milenios.4​ El núcleo antiguo de la urbe, la City de Londres, conserva básicamente su perímetro medieval de una milla cuadrada. Desde el siglo XIX el nombre «Londres» también hace referencia a toda la metrópolis desarrollada alrededor de este núcleo.5​ El grueso de esta conurbación forma la región de Londres y el área administrativa del Gran Londres,6​ gobernado por el alcalde y la asamblea de Londres.7​\nLondres es una ciudad global, uno de los centros neurálgicos en el ámbito de las artes, el comercio, la educación, el entretenimiento, la moda, las finanzas, los medios de comunicación, la investigación, el turismo o el transporte.8​ Es el principal centro financiero del mundo9​10​11​ y una de las áreas metropolitanas con mayor PIB.12​13​ Londres es también una capital cultural mundial,14​15​16​17​ la ciudad más visitada considerando el número de visitas internacionales18​ y tiene el mayor sistema aeroportuario del mundo según el tráfico de pasajeros.19​ Asimismo, las 43 universidades de la ciudad conforman la mayor concentración de centros de estudios superiores de toda Europa.20​ En el año 2012 Londres se convirtió en la única ciudad en albergar la celebración de tres Juegos Olímpicos de Verano.21​\nEn esta ciudad multirracial convive gente de un gran número de culturas que hablan más de trescientos idiomas distintos.22​ La Autoridad del Gran Londres estima que en 2015 la ciudad tiene 8,63 millones de habitantes,23​ que supone el 12,5 % del total de habitantes del Reino Unido.24​ El área urbana del Gran Londres, con 10 470 00025​ habitantes, es la segunda más grande de Europa, pero su área metropolitana, con una población estimada de entre 12 y 14 millones,26​27​ es la mayor del continente. Desde 1831 a 1925 Londres, como capital del Imperio británico, fue la ciudad más poblada del mundo.'''\n\n# Procesamos el texto en español\ndoc = nlp_es(text)\n\n# Extraemos y mostramos todas las entidades nombradas detectadas\n# En español, las etiquetas pueden ser: LOC (lugar), PER (persona), ORG (organización), MISC (misceláneo)\nfor entity in doc.ents:\n    print(f\"{entity.text} ({entity.label_})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos qué significa la etiqueta 'MISC' en el modelo de español\n# MISC = Miscellaneous (eventos, nacionalidades, productos, obras de arte, etc.)\nspacy.explain('MISC')"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5GwuAvRS1TLb"
   },
   "source": [
    "## Most frequent words\n",
    "In a Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instalamos la librería wikipedia para obtener contenido de artículos\n# Comando desactivado - ejecutar solo si no está instalada\n#%pip install wikipedia"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos las stopwords en español\n# Similar al inglés, pero adaptadas al español\nfrom spacy.lang.es.stop_words import STOP_WORDS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos las primeras 20 stopwords en español\n# Ejemplos: \"el\", \"la\", \"de\", \"que\", \"en\", \"y\", etc.\nlist(STOP_WORDS)[:20]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "zmuqxci01TTh",
    "outputId": "8f5d5796-9ab8-4b8b-d89c-2c6f17e89442"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 9: ANÁLISIS DE PALABRAS FRECUENTES CON WIKIPEDIA\n# ========================================\n\n# Importamos la librería wikipedia para obtener contenido de artículos\nimport wikipedia\n\n# Configuramos el idioma a inglés\nwikipedia.set_lang(\"en\")\n\n# Obtenemos el artículo completo sobre \"Data Science\"\nwiki = wikipedia.page(title=\"Data Science\")\n\n# Extraemos el contenido del artículo\ntext = wiki.content\n\n# Mostramos los primeros 1000 caracteres como muestra\ntext[0:1000]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8s0R3vU3NAZ",
    "outputId": "3c71df2d-5abe-488f-fb50-fbe8b242e97a"
   },
   "outputs": [],
   "source": "# Extraemos solo los SUSTANTIVOS del texto\n# Filtramos:\n# - Que NO sean stopwords\n# - Que NO sean signos de puntuación\n# - Que SÍ sean sustantivos (NOUN)\n\n# List comprehension con múltiples condiciones\nnombres = [w.text.lower() for w in nlp(text) if ((not w.is_stop) and (not w.is_punct) and (w.pos_ == 'NOUN'))]\n\n# Mostramos los primeros 10 sustantivos encontrados\nnombres[:10]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Contamos la frecuencia de cada palabra usando Counter\n# Counter es una estructura de datos que cuenta automáticamente elementos\nfrom collections import Counter\n\n# Creamos un contador con todas las palabras\nword_freq = Counter(nombres)\n\n# Mostramos las 10 palabras más comunes (excluyendo la primera con [1:])\n# most_common(10) devuelve una lista de tuplas (palabra, frecuencia)\nword_freq.most_common(10)[1:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 10: ANÁLISIS DE ADJETIVOS EN ESPAÑOL\n# ========================================\n\n# Cambiamos el idioma de wikipedia a español\nwikipedia.set_lang(\"es\")\n\n# Obtenemos el artículo sobre \"Regresión Lineal\" en español\nwiki = wikipedia.page(title=\"Regresión Lineal\")\n\n# Extraemos el contenido\ntext = wiki.content\n\n# Mostramos los primeros 1000 caracteres\ntext[0:1000]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Ahora extraemos ADJETIVOS en lugar de sustantivos\n# Usamos el lema (lemma_) en lugar del texto para normalizar las palabras\n# Por ejemplo: \"lineal\", \"lineales\", \"linealmente\" -> todos se convierten a \"lineal\"\n\n# Filtramos por:\n# - NO stopwords\n# - NO puntuación  \n# - SÍ adjetivos (ADJ)\nnombres = [w.lemma_ for w in nlp_es(text) if ((not w.is_stop) and (not w.is_punct) and (w.pos_ == 'ADJ'))]\n\n# Mostramos los primeros 10 adjetivos\nnombres[:10]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0D-pRly22_H",
    "outputId": "5bc9c178-6438-40ac-b2a6-ece714865a69"
   },
   "outputs": [],
   "source": "# Contamos la frecuencia de los adjetivos\nfrom collections import Counter\n\n# Creamos el contador\nword_freq = Counter(nombres)\n\n# Mostramos los 10 adjetivos más frecuentes (excluyendo el primero)\n# Esto nos ayuda a entender qué características se mencionan más en el artículo\nword_freq.most_common(10)[1:]"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xWDuikM7VJtc"
   },
   "source": [
    "## Textacy London text\n",
    "Now we want to know things about London, from our previous text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instalamos una versión específica de numpy compatible con textacy\n# numpy 1.26.4 es necesario para evitar conflictos de versiones\n# Después de instalar, REINICIAR el entorno de ejecución\n#%pip install numpy==1.26.4\n# then restart"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Instalamos textacy versión 0.12.0 específicamente\n# Esta versión es compatible con las versiones actuales de spacy y numpy\n!pip install textacy==0.12.0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Si hay problemas de compatibilidad, instalar numpy 1.26.4\n# Comando desactivado - ejecutar solo si es necesario\n#%pip install numpy==1.26.4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOiN7HAmvNV2",
    "outputId": "cc8d904b-3cc0-4588-f6d5-95ce6ad6fe7a"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 11: EXTRACCIÓN DE INFORMACIÓN CON TEXTACY\n# ========================================\n\n# Textacy es una capa sobre spacy que facilita tareas más avanzadas\nimport textacy.extract\n\n# Definimos un texto con múltiples afirmaciones sobre Londres\ntext = \"\"\"London is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia. London was founded by the Romans, \nwho named it Londinium. London is a huge city. London has a lot of cute restaurants. Jaimito is my best friend.\n\"\"\"\n\n# Procesamos el texto con spacy\ndoc = nlp(text)\n\n# Extraemos AFIRMACIONES SEMIESTRUCTURADAS sobre \"London\"\n# Buscamos frases donde \"London\" es el sujeto y el verbo es \"be\" (ser/estar)\n# Esto nos permite extraer hechos específicos sobre Londres\nstatements = textacy.extract.semistructured_statements(doc, entity=\"London\", cue='be')\n\n# Mostramos los resultados\nprint(\"Here are the things I know about London:\")\n\n# Cada statement es una tupla de (sujeto, verbo, hecho)\nfor statement in statements:\n    subject, verb, fact = statement\n    # Imprimimos solo el hecho extraído\n    print(f\" - {fact}\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hFwnqZzIaSNQ"
   },
   "source": [
    "## Textacy with Wikipedia API\n",
    "We don't want the most frequent words, we want sentences about London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za6v7SL_2Ywo"
   },
   "outputs": [],
   "source": "# ========================================\n# EJEMPLO 12: EXTRACCIÓN DE INFORMACIÓN DE WIKIPEDIA\n# ========================================\n\n# Configuramos el idioma a inglés\nwikipedia.set_lang(\"en\")\n\n# Obtenemos el artículo completo de Londres\nlondon = wikipedia.page(\"London\")\n\n# Extraemos el contenido del artículo\ntext = london.content"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwOqJO902pO6",
    "outputId": "9d856c03-1993-4c18-976a-39a58d7680a3"
   },
   "outputs": [],
   "source": "# Procesamos el artículo de Wikipedia sobre Londres\ndoc = nlp(text)\n\n# Ahora buscamos información específica sobre \"London Underground\" (el metro de Londres)\n# Extraemos afirmaciones donde \"London Underground\" es el sujeto y \"be\" es el verbo\nstatements = textacy.extract.semistructured_statements(doc, entity=\"London Underground\", cue='be')\n\n# Mostramos los resultados\nprint(\"Here are the things I know about London:\")\n\n# Iteramos sobre cada afirmación encontrada\nfor statement in statements:\n    subject, verb, fact = statement\n    # Imprimimos solo el hecho extraído\n    # Por ejemplo: \"London Underground is the world's oldest rapid transit system\"\n    print(f\" - {fact}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Textacy_y_spacy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}