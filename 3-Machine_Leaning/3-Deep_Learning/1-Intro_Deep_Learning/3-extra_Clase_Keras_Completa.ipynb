{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción a Keras: Deep Learning Simplificado\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos de la Clase\n",
    "\n",
    "En esta clase aprenderemos:\n",
    "\n",
    "1. **¿Qué es Keras?** - Conceptos fundamentales\n",
    "2. **Clasificación de imágenes** - Reconocimiento de dígitos con MNIST\n",
    "3. **Arquitectura de redes neuronales** - Capas, activaciones y parámetros\n",
    "4. **Entrenamiento y evaluación** - Métricas y validación\n",
    "5. **Callbacks** - Control inteligente del entrenamiento\n",
    "6. **Regresión** - Predicción de valores continuos\n",
    "7. **Persistencia** - Guardar y cargar modelos\n",
    "\n",
    "---\n",
    "\n",
    "## ¿Qué es Keras?\n",
    "\n",
    "**Keras** es una API de alto nivel para construir y entrenar redes neuronales profundas. Características principales:\n",
    "\n",
    "- **Simplicidad**: Código claro y conciso\n",
    "- **Potencia**: Construido sobre TensorFlow\n",
    "- **Flexibilidad**: Desde prototipos rápidos hasta producción\n",
    "- **Documentación**: Excelente y con muchos ejemplos\n",
    "\n",
    "```\n",
    "Keras + TensorFlow = Deep Learning accesible y potente\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECCION 1: Configuración Inicial\n",
    "\n",
    "## Importación de Librerías\n",
    "\n",
    "Comenzamos importando todas las herramientas necesarias para la clase.\n",
    "\n",
    "En esta sección importaremos:\n",
    "- TensorFlow y Keras: framework principal para construir redes neuronales\n",
    "- NumPy: para operaciones numéricas y manipulación de arrays\n",
    "- Pandas: para trabajar con datos tabulares\n",
    "- Matplotlib: para crear visualizaciones y gráficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTACIONES PRINCIPALES\n",
    "# ============================================\n",
    "\n",
    "# TensorFlow: Framework de Deep Learning desarrollado por Google\n",
    "# Es el backend que ejecuta todas las operaciones de bajo nivel\n",
    "# Proporciona optimización de cálculos, soporte para GPU/TPU, y grafos computacionales\n",
    "import tensorflow as tf\n",
    "\n",
    "# Keras: API de alto nivel integrada en TensorFlow\n",
    "# Simplifica enormemente la construcción de redes neuronales\n",
    "# Permite crear modelos complejos con pocas líneas de código\n",
    "# Abstrae la complejidad matemática manteniendo la flexibilidad\n",
    "from tensorflow import keras\n",
    "\n",
    "# NumPy: Librería fundamental para computación científica en Python\n",
    "# Proporciona:\n",
    "#   - Arrays multidimensionales eficientes\n",
    "#   - Funciones matemáticas vectorizadas (operaciones rápidas sin loops)\n",
    "#   - Álgebra lineal, transformadas de Fourier, números aleatorios\n",
    "#   - Interoperabilidad con TensorFlow (convierte automáticamente entre tipos)\n",
    "import numpy as np\n",
    "\n",
    "# Pandas: Librería para análisis y manipulación de datos estructurados\n",
    "# Características principales:\n",
    "#   - DataFrames: tablas con filas y columnas etiquetadas\n",
    "#   - Funciones para limpiar, transformar y analizar datos\n",
    "#   - Lectura/escritura de múltiples formatos (CSV, Excel, SQL, etc.)\n",
    "#   - Operaciones de agrupación, pivoteo y fusión de datos\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib: Librería de visualización más popular en Python\n",
    "# pyplot es el módulo que proporciona interfaz similar a MATLAB\n",
    "# Permite crear:\n",
    "#   - Gráficos de líneas, barras, dispersión, histogramas\n",
    "#   - Imágenes y mapas de calor\n",
    "#   - Subplots para múltiples gráficos\n",
    "#   - Personalización completa de estilos y colores\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Comando mágico de Jupyter/IPython\n",
    "# %matplotlib inline indica que los gráficos se muestren directamente en el notebook\n",
    "# Sin esto, los gráficos aparecerían en ventanas separadas\n",
    "%matplotlib inline\n",
    "\n",
    "# Configurar el estilo visual de matplotlib para gráficos más profesionales\n",
    "# 'seaborn-v0_8-darkgrid' proporciona:\n",
    "#   - Fondo gris claro para mejor contraste\n",
    "#   - Rejilla para facilitar lectura de valores\n",
    "#   - Colores más suaves y agradables a la vista\n",
    "#   - Tipografía optimizada para presentaciones\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# ============================================\n",
    "# VERIFICACIÓN DE VERSIONES\n",
    "# ============================================\n",
    "# Es importante verificar las versiones para:\n",
    "#   - Reproducibilidad: asegurar que el código funcione igual en diferentes entornos\n",
    "#   - Compatibilidad: algunas APIs cambian entre versiones\n",
    "#   - Debugging: facilita identificar problemas relacionados con versiones\n",
    "\n",
    "print(\"VERIFICACION DE VERSIONES DE LIBRERIAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# __version__ es un atributo especial que contiene la versión de la librería\n",
    "# TensorFlow incluye Keras, por lo que sus versiones están relacionadas\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURACIÓN DE SEMILLAS ALEATORIAS\n",
    "# ============================================\n",
    "# Las redes neuronales usan números aleatorios en múltiples lugares:\n",
    "#   - Inicialización de pesos (valores iniciales de las conexiones)\n",
    "#   - Dropout (desactivación aleatoria de neuronas)\n",
    "#   - Shuffling de datos durante entrenamiento\n",
    "#   - División de datos train/validation\n",
    "#\n",
    "# Establecer semillas hace que estos procesos sean REPRODUCIBLES:\n",
    "# Con la misma semilla, obtendremos los mismos \"números aleatorios\"\n",
    "# Esto es crucial para:\n",
    "#   - Debugging: poder replicar errores exactamente\n",
    "#   - Investigación: comparar experimentos de forma justa\n",
    "#   - Educación: que todos los estudiantes vean los mismos resultados\n",
    "\n",
    "# Semilla para NumPy\n",
    "# Afecta a: np.random.rand(), np.random.choice(), etc.\n",
    "# El número 42 es arbitrario (tradición en ciencia: \"Hitchhiker's Guide to the Galaxy\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Semilla para TensorFlow\n",
    "# Afecta a: inicialización de pesos, operaciones aleatorias en GPU\n",
    "# TensorFlow tiene su propio generador de números aleatorios independiente de NumPy\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"\\nSemilla aleatoria establecida: 42\")\n",
    "print(\"Todos los experimentos seran reproducibles\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECCION 2: Dataset MNIST - Dígitos Manuscritos\n",
    "\n",
    "## ¿Qué es MNIST?\n",
    "\n",
    "**MNIST** (Modified National Institute of Standards and Technology) es el dataset \"Hello World\" del Deep Learning:\n",
    "\n",
    "- **70,000 imágenes** de dígitos escritos a mano (0-9)\n",
    "- **Tamaño**: 28x28 píxeles en escala de grises\n",
    "- **División**: 60,000 entrenamiento + 10,000 test\n",
    "- **Objetivo**: Clasificar cada imagen en una de 10 clases (dígitos 0-9)\n",
    "\n",
    "## Estructura de los Datos\n",
    "\n",
    "```\n",
    "Imagen (28x28) → 784 píxeles → Valor entre 0-255 (intensidad)\n",
    "Label → Número entero 0-9\n",
    "```\n",
    "\n",
    "## Importancia de MNIST\n",
    "\n",
    "MNIST es fundamental en el aprendizaje de Deep Learning porque:\n",
    "- Es suficientemente complejo para requerir redes neuronales\n",
    "- Es suficientemente simple para entrenar rápidamente\n",
    "- Permite validar que nuestro código funciona correctamente\n",
    "- Es el benchmark estándar para comparar algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGA DEL DATASET MNIST\n",
    "# ============================================\n",
    "# Keras proporciona varios datasets populares precargados\n",
    "# Esto facilita enormemente comenzar con Deep Learning\n",
    "# \n",
    "# Datasets disponibles en keras.datasets:\n",
    "#   - mnist: dígitos manuscritos\n",
    "#   - fashion_mnist: prendas de ropa\n",
    "#   - cifar10/cifar100: imágenes de objetos\n",
    "#   - imdb: reseñas de películas (texto)\n",
    "#   - reuters: noticias (texto)\n",
    "\n",
    "print(\"Cargando dataset MNIST desde Keras...\")\n",
    "print(\"(La primera vez descargara ~11 MB)\")\n",
    "print()\n",
    "\n",
    "# load_data() retorna dos tuplas: (train_data, train_labels) y (test_data, test_labels)\n",
    "# La función automáticamente:\n",
    "#   1. Descarga el dataset si no existe localmente (se guarda en ~/.keras/datasets/)\n",
    "#   2. Descomprime los archivos\n",
    "#   3. Carga los datos en memoria como arrays de NumPy\n",
    "#   4. Ya viene con división train/test predefinida\n",
    "# \n",
    "# X = features (imágenes)\n",
    "# y = labels (etiquetas, el número que representa cada imagen)\n",
    "# \"_full\" porque luego separaremos una porción para validación\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Dataset MNIST cargado correctamente\")\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# EXPLORACIÓN DETALLADA DE LOS DATOS\n",
    "# ============================================\n",
    "# Antes de entrenar cualquier modelo, es CRÍTICO entender los datos:\n",
    "#   - Dimensiones y formas de los arrays\n",
    "#   - Tipo de datos y rango de valores\n",
    "#   - Distribución de las clases (¿está balanceado?)\n",
    "#   - Presencia de valores faltantes o anómalos\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANALISIS EXPLORATORIO DEL DATASET MNIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# -------- ANÁLISIS DE DIMENSIONES --------\n",
    "print(\"\\n[1] DIMENSIONES DE LOS DATOS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# .shape retorna una tupla con las dimensiones del array\n",
    "# Para X_train_full: (60000, 28, 28) significa:\n",
    "#   - 60,000 muestras (imágenes)\n",
    "#   - 28 filas de píxeles\n",
    "#   - 28 columnas de píxeles\n",
    "print(f\"Datos de Entrenamiento:\")\n",
    "print(f\"  X_train_full.shape = {X_train_full.shape}\")\n",
    "print(f\"    -> {X_train_full.shape[0]:,} imágenes de {X_train_full.shape[1]}x{X_train_full.shape[2]} píxeles\")\n",
    "\n",
    "# Para y_train_full: (60000,) significa:\n",
    "#   - 60,000 etiquetas (un número por cada imagen)\n",
    "#   - Array 1D (unidimensional)\n",
    "print(f\"  y_train_full.shape = {y_train_full.shape}\")\n",
    "print(f\"    -> {y_train_full.shape[0]:,} etiquetas (una por imagen)\")\n",
    "\n",
    "print(f\"\\nDatos de Test:\")\n",
    "print(f\"  X_test.shape = {X_test.shape}\")\n",
    "print(f\"    -> {X_test.shape[0]:,} imágenes de {X_test.shape[1]}x{X_test.shape[2]} píxeles\")\n",
    "print(f\"  y_test.shape = {y_test.shape}\")\n",
    "print(f\"    -> {y_test.shape[0]:,} etiquetas\")\n",
    "\n",
    "# -------- ANÁLISIS DE TIPOS Y RANGOS --------\n",
    "print(\"\\n[2] TIPOS DE DATOS Y RANGOS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# .dtype muestra el tipo de dato de los elementos del array\n",
    "# uint8 = entero sin signo de 8 bits (rango: 0-255)\n",
    "# Es eficiente en memoria: cada pixel ocupa solo 1 byte\n",
    "print(f\"Tipo de dato de las imágenes: {X_train_full.dtype}\")\n",
    "print(f\"  -> uint8: entero sin signo de 8 bits\")\n",
    "print(f\"  -> Rango posible: 0 a 255\")\n",
    "\n",
    "# .min() y .max() encuentran el valor mínimo y máximo en todo el array\n",
    "# Útil para detectar:\n",
    "#   - Si ya está normalizado (0-1) o no (0-255)\n",
    "#   - Outliers o valores incorrectos\n",
    "#   - Necesidad de preprocesamiento\n",
    "print(f\"\\nRango real de valores en los píxeles:\")\n",
    "print(f\"  Mínimo: {X_train_full.min()}\")\n",
    "print(f\"  Máximo: {X_train_full.max()}\")\n",
    "print(f\"  -> Interpretacion: 0 = negro (fondo), 255 = blanco (trazo del dígito)\")\n",
    "\n",
    "# Tipo de las etiquetas\n",
    "print(f\"\\nTipo de dato de las etiquetas: {y_train_full.dtype}\")\n",
    "print(f\"  -> Cada etiqueta es un número entero del 0 al 9\")\n",
    "\n",
    "# -------- ANÁLISIS DE CLASES --------\n",
    "print(\"\\n[3] DISTRIBUCIÓN DE CLASES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# np.unique() retorna los valores únicos en un array\n",
    "# Con return_counts=True, también retorna cuántas veces aparece cada valor\n",
    "# Esto nos dice:\n",
    "#   - Cuántas clases tenemos\n",
    "#   - Si el dataset está balanceado (similar cantidad de ejemplos por clase)\n",
    "unique_classes, class_counts = np.unique(y_train_full, return_counts=True)\n",
    "\n",
    "print(f\"Clases presentes en el dataset: {unique_classes}\")\n",
    "print(f\"Número de clases: {len(unique_classes)}\")\n",
    "print(f\"\\nDistribución de muestras por clase (train):\")\n",
    "\n",
    "# Iterar sobre cada clase y su conteo\n",
    "total_samples = len(y_train_full)\n",
    "for digit, count in zip(unique_classes, class_counts):\n",
    "    # Calcular el porcentaje que representa cada clase\n",
    "    percentage = (count / total_samples) * 100\n",
    "    # Crear una barra visual simple con caracteres\n",
    "    bar = '█' * int(percentage)\n",
    "    print(f\"  Dígito {digit}: {count:>5} imágenes ({percentage:>5.2f}%) {bar}\")\n",
    "\n",
    "# Verificar si está balanceado\n",
    "# Un dataset está balanceado si todas las clases tienen similar cantidad de muestras\n",
    "# Desbalance causa que el modelo se sesgue hacia clases mayoritarias\n",
    "min_count = class_counts.min()\n",
    "max_count = class_counts.max()\n",
    "balance_ratio = min_count / max_count\n",
    "\n",
    "print(f\"\\nAnálisis de balance:\")\n",
    "print(f\"  Clase con menos muestras: {min_count}\")\n",
    "print(f\"  Clase con más muestras: {max_count}\")\n",
    "print(f\"  Ratio de balance: {balance_ratio:.3f}\")\n",
    "\n",
    "if balance_ratio > 0.9:\n",
    "    print(f\"  -> Dataset BIEN BALANCEADO (ratio > 0.9)\")\n",
    "    print(f\"  -> No necesitamos técnicas especiales de balanceo\")\n",
    "elif balance_ratio > 0.7:\n",
    "    print(f\"  -> Dataset MODERADAMENTE BALANCEADO\")\n",
    "    print(f\"  -> Podríamos considerar pesos de clase\")\n",
    "else:\n",
    "    print(f\"  -> Dataset DESBALANCEADO\")\n",
    "    print(f\"  -> RECOMENDADO: usar class weights o técnicas de resampling\")\n",
    "\n",
    "# -------- ESTADÍSTICAS ADICIONALES --------\n",
    "print(\"\\n[4] ESTADÍSTICAS ADICIONALES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Calcular el tamaño total en memoria\n",
    "# .nbytes retorna el número de bytes que ocupa el array en memoria\n",
    "train_memory_mb = X_train_full.nbytes / (1024 * 1024)\n",
    "test_memory_mb = X_test.nbytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Uso de memoria:\")\n",
    "print(f\"  Training set: {train_memory_mb:.2f} MB\")\n",
    "print(f\"  Test set: {test_memory_mb:.2f} MB\")\n",
    "print(f\"  Total: {train_memory_mb + test_memory_mb:.2f} MB\")\n",
    "\n",
    "# Estadísticas de los píxeles\n",
    "# .mean() calcula el promedio de todos los valores\n",
    "# .std() calcula la desviación estándar (qué tan dispersos están los valores)\n",
    "print(f\"\\nEstadísticas de intensidad de píxeles (training):\")\n",
    "print(f\"  Media: {X_train_full.mean():.2f}\")\n",
    "print(f\"  Desviación estándar: {X_train_full.std():.2f}\")\n",
    "print(f\"  -> La mayoría de píxeles son cercanos a 0 (fondo negro)\")\n",
    "print(f\"  -> Solo los trazos del dígito tienen valores altos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de Ejemplos\n",
    "\n",
    "Veamos cómo se ven algunos dígitos del dataset. La visualización es crucial para:\n",
    "- Entender qué estamos intentando clasificar\n",
    "- Detectar problemas en los datos\n",
    "- Verificar que la carga fue correcta\n",
    "- Apreciar la variabilidad en la escritura manuscrita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN: CUADRÍCULA DE IMÁGENES\n",
    "# ============================================\n",
    "# Mostraremos una cuadrícula de 5x5 (25 imágenes) para tener una visión general\n",
    "\n",
    "# plt.subplots() crea una figura con múltiples subplots (gráficos)\n",
    "# Parámetros:\n",
    "#   5, 5: crear una cuadrícula de 5 filas x 5 columnas = 25 subplots\n",
    "#   figsize=(10, 10): tamaño de la figura completa en pulgadas\n",
    "# Retorna:\n",
    "#   fig: objeto figura completa\n",
    "#   axes: array 2D de objetos axes (cada subplot individual)\n",
    "fig, axes = plt.subplots(5, 5, figsize=(10, 10))\n",
    "\n",
    "# suptitle: título superior para toda la figura\n",
    "# fontsize controla el tamaño del texto\n",
    "# fontweight='bold' hace el texto en negrita para mayor énfasis\n",
    "fig.suptitle('Ejemplos del Dataset MNIST', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Seleccionar 25 índices aleatorios del training set\n",
    "# np.random.choice() selecciona aleatoriamente de un rango\n",
    "# Parámetros:\n",
    "#   len(X_train_full): seleccionar del rango [0, 59999]\n",
    "#   25: seleccionar 25 índices\n",
    "#   replace=False: sin reemplazo (no repetir el mismo índice)\n",
    "indices = np.random.choice(len(X_train_full), 25, replace=False)\n",
    "\n",
    "# axes.flat convierte el array 2D de axes en un iterador 1D\n",
    "# Esto facilita iterar sobre todos los subplots en orden\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # enumerate() da tanto el índice (i) como el elemento (ax)\n",
    "    idx = indices[i]  # Obtener el índice aleatorio para esta posición\n",
    "    \n",
    "    # -------- MOSTRAR LA IMAGEN --------\n",
    "    # ax.imshow() muestra una imagen en el subplot\n",
    "    # Parámetros:\n",
    "    #   X_train_full[idx]: la imagen (array 28x28)\n",
    "    #   cmap='gray': usar mapa de colores en escala de grises\n",
    "    #     - cmap = colormap\n",
    "    #     - 'gray': negro=0, blanco=255\n",
    "    #     - Otras opciones: 'viridis', 'plasma', 'hot', etc.\n",
    "    ax.imshow(X_train_full[idx], cmap='gray')\n",
    "    \n",
    "    # -------- AÑADIR TÍTULO --------\n",
    "    # Mostrar la etiqueta real encima de cada imagen\n",
    "    # Esto nos permite verificar visualmente si tiene sentido\n",
    "    ax.set_title(f'Label: {y_train_full[idx]}', fontsize=12)\n",
    "    \n",
    "    # -------- QUITAR EJES --------\n",
    "    # ax.axis('off') oculta los ejes x e y\n",
    "    # Los ejes no son necesarios para imágenes y distraen\n",
    "    # Hace la visualización más limpia y enfocada\n",
    "    ax.axis('off')\n",
    "\n",
    "# tight_layout() ajusta automáticamente los subplots para evitar solapamiento\n",
    "# Optimiza el espaciado entre subplots, títulos y etiquetas\n",
    "plt.tight_layout()\n",
    "\n",
    "# show() muestra la figura en pantalla\n",
    "# En Jupyter con %matplotlib inline, se muestra automáticamente\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZACIÓN: IMAGEN INDIVIDUAL DETALLADA\n",
    "# ============================================\n",
    "# Ahora veamos una sola imagen en detalle para entender mejor la estructura\n",
    "\n",
    "# Crear una figura más grande para ver detalles\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Mostrar la primera imagen del training set\n",
    "# Usamos interpolation='nearest' para:\n",
    "#   - Evitar suavizado/blur al hacer zoom\n",
    "#   - Ver exactamente los valores de píxeles\n",
    "#   - Mantener los bordes nítidos\n",
    "plt.imshow(X_train_full[0], cmap='gray', interpolation='nearest')\n",
    "\n",
    "# Título descriptivo con información de la etiqueta\n",
    "plt.title(f'Primera imagen del training set - Dígito: {y_train_full[0]}', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# -------- AÑADIR COLORBAR --------\n",
    "# colorbar() añade una barra de colores al lado de la imagen\n",
    "# Muestra la escala de valores (0=negro, 255=blanco)\n",
    "# Parámetros:\n",
    "#   label: etiqueta descriptiva para la barra\n",
    "#   shrink: factor de escala (0.8 = 80% del tamaño del subplot)\n",
    "plt.colorbar(label='Intensidad del pixel (0-255)', shrink=0.8)\n",
    "\n",
    "# grid(False) desactiva la rejilla\n",
    "# Para imágenes, la rejilla suele ser distractora\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# ANÁLISIS DETALLADO DE LA PRIMERA IMAGEN\n",
    "# ============================================\n",
    "print(\"\\nANALISIS DETALLADO DE LA PRIMERA IMAGEN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Información básica\n",
    "print(f\"Etiqueta (label): {y_train_full[0]}\")\n",
    "print(f\"Forma (shape): {X_train_full[0].shape}\")\n",
    "print(f\"  -> {X_train_full[0].shape[0]} filas x {X_train_full[0].shape[1]} columnas\")\n",
    "print(f\"  -> Total de píxeles: {X_train_full[0].shape[0] * X_train_full[0].shape[1]}\")\n",
    "\n",
    "# Estadísticas de píxeles de esta imagen específica\n",
    "print(f\"\\nEstadísticas de píxeles:\")\n",
    "print(f\"  Valor mínimo: {X_train_full[0].min()}\")\n",
    "print(f\"  Valor máximo: {X_train_full[0].max()}\")\n",
    "print(f\"  Valor medio: {X_train_full[0].mean():.2f}\")\n",
    "print(f\"  Desviación estándar: {X_train_full[0].std():.2f}\")\n",
    "\n",
    "# Contar píxeles \"activos\" (parte del dígito) vs \"fondo\"\n",
    "# Usamos un umbral arbitrario de 50 para distinguir\n",
    "active_pixels = np.sum(X_train_full[0] > 50)\n",
    "background_pixels = np.sum(X_train_full[0] <= 50)\n",
    "total_pixels = X_train_full[0].size\n",
    "\n",
    "print(f\"\\nDistribución de píxeles:\")\n",
    "print(f\"  Píxeles activos (>50): {active_pixels} ({active_pixels/total_pixels*100:.1f}%)\")\n",
    "print(f\"  Píxeles de fondo (<=50): {background_pixels} ({background_pixels/total_pixels*100:.1f}%)\")\n",
    "print(f\"  -> La mayoría de píxeles son fondo (el dígito es pequeño)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECCION 3: Preprocesamiento de Datos\n",
    "\n",
    "## ¿Por qué preprocesar?\n",
    "\n",
    "Las redes neuronales funcionan mejor cuando:\n",
    "\n",
    "1. **Los datos están normalizados** (rango [0, 1] o [-1, 1])\n",
    "   - Los pesos se inicializan en valores pequeños\n",
    "   - Valores grandes (255) causan gradientes inestables\n",
    "   - La normalización acelera la convergencia\n",
    "\n",
    "2. **Los datos están balanceados** (similar cantidad de ejemplos por clase)\n",
    "   - Previene sesgo hacia clases mayoritarias\n",
    "   - En MNIST ya está balanceado\n",
    "\n",
    "3. **Hay un conjunto de validación** (para monitorear el entrenamiento)\n",
    "   - Detecta overfitting tempranamente\n",
    "   - Permite ajustar hiperparámetros\n",
    "   - No debe usarse para entrenamiento\n",
    "\n",
    "## Pasos de Preprocesamiento\n",
    "\n",
    "1. División: Train / Validation / Test\n",
    "2. Normalización: Escalar píxeles de [0, 255] a [0, 1]\n",
    "3. Conversión de tipos: Asegurar float32 para eficiencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# PASO 1: DIVISIÓN TRAIN / VALIDATION / TEST\n",
    "# ============================================\n",
    "# \n",
    "# División típica en Machine Learning:\n",
    "#   - Training Set (70-80%): Entrenar el modelo (ajustar pesos)\n",
    "#   - Validation Set (10-15%): Monitorear durante entrenamiento, ajustar hiperparámetros\n",
    "#   - Test Set (10-20%): Evaluación final, nunca se usa antes\n",
    "#\n",
    "# En MNIST:\n",
    "#   - Ya tenemos test set separado (10,000 imágenes)\n",
    "#   - Del training original (60,000), tomaremos 5,000 para validation\n",
    "#   - Quedará: 55,000 train, 5,000 validation, 10,000 test\n",
    "\n",
    "print(\"DIVISION DE DATOS: TRAIN / VALIDATION / TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# -------- SEPARACIÓN DE VALIDATION --------\n",
    "# Slicing de NumPy: array[inicio:fin]\n",
    "#   - [:5000] = desde el inicio hasta el índice 5000 (no incluido)\n",
    "#   - [5000:] = desde el índice 5000 hasta el final\n",
    "#\n",
    "# ¿Por qué los primeros 5000?\n",
    "#   - MNIST ya viene mezclado aleatoriamente\n",
    "#   - Tomar los primeros es equivalente a tomar una muestra aleatoria\n",
    "#   - Es más rápido que shuffle + split\n",
    "#\n",
    "# IMPORTANTE: Hacemos lo mismo para X (imágenes) y y (etiquetas)\n",
    "# para mantener la correspondencia\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "# Mostrar resultados de la división\n",
    "print(\"Division completada:\")\n",
    "print()\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  X_train: {X_train.shape} - {X_train.shape[0]:,} imágenes\")\n",
    "print(f\"  y_train: {y_train.shape} - {y_train.shape[0]:,} etiquetas\")\n",
    "print(f\"  Uso: ENTRENAR el modelo (ajustar pesos de la red neuronal)\")\n",
    "print()\n",
    "print(f\"Validation Set:\")\n",
    "print(f\"  X_valid: {X_valid.shape} - {X_valid.shape[0]:,} imágenes\")\n",
    "print(f\"  y_valid: {y_valid.shape} - {y_valid.shape[0]:,} etiquetas\")\n",
    "print(f\"  Uso: MONITOREAR entrenamiento, detectar overfitting, ajustar hiperparámetros\")\n",
    "print()\n",
    "print(f\"Test Set:\")\n",
    "print(f\"  X_test: {X_test.shape} - {X_test.shape[0]:,} imágenes\")\n",
    "print(f\"  y_test: {y_test.shape} - {y_test.shape[0]:,} etiquetas\")\n",
    "print(f\"  Uso: EVALUACIÓN FINAL (solo se usa al final, una vez)\")\n",
    "print()\n",
    "\n",
    "# Calcular porcentajes\n",
    "total = len(X_train) + len(X_valid) + len(X_test)\n",
    "train_pct = len(X_train) / total * 100\n",
    "valid_pct = len(X_valid) / total * 100\n",
    "test_pct = len(X_test) / total * 100\n",
    "\n",
    "print(f\"Porcentajes:\")\n",
    "print(f\"  Training:   {train_pct:.1f}%\")\n",
    "print(f\"  Validation: {valid_pct:.1f}%\")\n",
    "print(f\"  Test:       {test_pct:.1f}%\")\n",
    "print(f\"  Total:      {total:,} imágenes (100.0%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PASO 2: NORMALIZACIÓN DE PÍXELES\n",
    "# ============================================\n",
    "#\n",
    "# ¿Por qué normalizar?\n",
    "#\n",
    "# 1. CONVERGENCIA MÁS RÁPIDA:\n",
    "#    - Los optimizadores (Adam, SGD) funcionan mejor con valores pequeños\n",
    "#    - Valores grandes requieren learning rates muy pequeños\n",
    "#    - Con normalización, el learning rate estándar (0.001) funciona bien\n",
    "#\n",
    "# 2. ESTABILIDAD NUMÉRICA:\n",
    "#    - Valores grandes pueden causar overflow en los cálculos\n",
    "#    - Gradientes explotan o desaparecen\n",
    "#    - Normalización mantiene valores en rango manejable\n",
    "#\n",
    "# 3. FUNCIONAMIENTO DE ACTIVACIONES:\n",
    "#    - Funciones como sigmoid/tanh saturan con valores grandes\n",
    "#    - Inputs normalizados mantienen las activaciones en zona activa\n",
    "#    - Mejor flujo de gradientes en backpropagation\n",
    "#\n",
    "# 4. COMPARABILIDAD:\n",
    "#    - Si tenemos múltiples features con diferentes escalas,\n",
    "#      el modelo se sesga hacia los valores más grandes\n",
    "#    - Normalización pone todos en la misma escala\n",
    "\n",
    "print(\"\\nNORMALIZACION DE PIXELES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"Estado ANTES de normalizar:\")\n",
    "print(f\"  Tipo de dato: {X_train.dtype}\")\n",
    "print(f\"  Rango de valores: [{X_train.min()}, {X_train.max()}]\")\n",
    "print(f\"  Media: {X_train.mean():.2f}\")\n",
    "print(f\"  Desviación estándar: {X_train.std():.2f}\")\n",
    "\n",
    "# -------- PROCESO DE NORMALIZACIÓN --------\n",
    "#\n",
    "# Método: Min-Max Scaling al rango [0, 1]\n",
    "# Fórmula: X_normalized = (X - min) / (max - min)\n",
    "# En nuestro caso: min=0, max=255, entonces: X_normalized = X / 255.0\n",
    "#\n",
    "# Proceso en dos pasos:\n",
    "#\n",
    "# 1. .astype('float32'):\n",
    "#    - Convierte de uint8 (entero 0-255) a float32 (decimal)\n",
    "#    - ¿Por qué float32 y no float64?\n",
    "#      * float64 (8 bytes) es más preciso pero usa más memoria\n",
    "#      * float32 (4 bytes) tiene precisión suficiente para ML\n",
    "#      * GPUs son más eficientes con float32\n",
    "#      * Reduce uso de memoria a la mitad\n",
    "#\n",
    "# 2. / 255.0:\n",
    "#    - Divide cada píxel por 255 para escalar a [0, 1]\n",
    "#    - Usamos 255.0 (float) en lugar de 255 (int) para asegurar división flotante\n",
    "#    - Resultado: 0 sigue siendo 0 (negro), 255 se convierte en 1.0 (blanco)\n",
    "\n",
    "print(\"\\nAplicando normalización...\")\n",
    "print(\"  Formula: pixel_normalizado = pixel / 255.0\")\n",
    "print(\"  Conversion: uint8 [0-255] -> float32 [0.0-1.0]\")\n",
    "\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_valid = X_valid.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# NOTA IMPORTANTE:\n",
    "# Aplicamos la MISMA transformación a train, valid y test\n",
    "# Usamos los mismos parámetros (dividir por 255) para todos\n",
    "# Esto es crucial para que el modelo funcione correctamente\n",
    "\n",
    "print(\"\\nEstado DESPUÉS de normalizar:\")\n",
    "print(f\"  Tipo de dato: {X_train.dtype}\")\n",
    "print(f\"  Rango de valores: [{X_train.min():.4f}, {X_train.max():.4f}]\")\n",
    "print(f\"  Media: {X_train.mean():.4f}\")\n",
    "print(f\"  Desviación estándar: {X_train.std():.4f}\")\n",
    "\n",
    "# -------- VERIFICACIÓN --------\n",
    "# Asegurémonos de que la normalización fue correcta\n",
    "print(\"\\nVerificación de normalización:\")\n",
    "\n",
    "# Verificar que todos los conjuntos tienen el mismo rango\n",
    "print(f\"  Train - Min: {X_train.min():.4f}, Max: {X_train.max():.4f}\")\n",
    "print(f\"  Valid - Min: {X_valid.min():.4f}, Max: {X_valid.max():.4f}\")\n",
    "print(f\"  Test  - Min: {X_test.min():.4f}, Max: {X_test.max():.4f}\")\n",
    "\n",
    "# Verificar que no hay valores NaN (Not a Number) o Inf (Infinito)\n",
    "# Estos valores causarían errores durante el entrenamiento\n",
    "has_nan_train = np.isnan(X_train).any()\n",
    "has_inf_train = np.isinf(X_train).any()\n",
    "\n",
    "print(f\"\\n  ¿Contiene NaN (valores inválidos)? {has_nan_train}\")\n",
    "print(f\"  ¿Contiene Inf (infinitos)? {has_inf_train}\")\n",
    "\n",
    "if not has_nan_train and not has_inf_train:\n",
    "    print(\"  -> Normalización EXITOSA: datos limpios y listos para entrenar\")\n",
    "else:\n",
    "    print(\"  -> ADVERTENCIA: hay valores inválidos en los datos\")\n",
    "\n",
    "# -------- IMPACTO EN MEMORIA --------\n",
    "print(\"\\nImpacto en uso de memoria:\")\n",
    "memory_before_mb = X_train_full.nbytes / (1024 * 1024)\n",
    "memory_after_mb = X_train.nbytes / (1024 * 1024)\n",
    "\n",
    "print(f\"  Antes (uint8): {memory_before_mb:.2f} MB\")\n",
    "print(f\"  Después (float32): {memory_after_mb:.2f} MB\")\n",
    "print(f\"  Aumento: {(memory_after_mb / memory_before_mb - 1) * 100:.1f}%\")\n",
    "print(f\"  -> float32 usa 4 bytes por píxel vs 1 byte de uint8\")\n",
    "print(f\"  -> El aumento de memoria es aceptable para las ventajas en entrenamiento\")\n",
    "\n",
    "# ============================================\n",
    "# RESUMEN FINAL DEL PREPROCESAMIENTO\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN DE PREPROCESAMIENTO\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nTransformaciones aplicadas:\")\n",
    "print(\"  1. Division de datos: 55k train / 5k valid / 10k test\")\n",
    "print(\"  2. Conversion de tipo: uint8 -> float32\")\n",
    "print(\"  3. Normalizacion: [0, 255] -> [0.0, 1.0]\")\n",
    "print(\"\\nDatos listos para entrenar el modelo de red neuronal\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECCION 4: Construcción del Modelo\n",
    "\n",
    "## Arquitectura de Red Neuronal\n",
    "\n",
    "Vamos a construir una **red neuronal totalmente conectada** (Fully Connected Neural Network o Multilayer Perceptron):\n",
    "\n",
    "```\n",
    "INPUT (28×28)     FLATTEN (784)     DENSE (300)      DENSE (100)      OUTPUT (10)\n",
    "  Imagen      →      Vector 1D    →   Capa oculta  →   Capa oculta  →   Probabilidades\n",
    "28×28 píxeles       784 features      300 neuronas     100 neuronas     10 clases (0-9)\n",
    "                                      + ReLU           + ReLU           + Softmax\n",
    "```\n",
    "\n",
    "### Componentes clave:\n",
    "\n",
    "1. **Flatten**: Convierte matriz 28×28 en vector de 784 elementos\n",
    "2. **Dense (300)**: Primera capa oculta con 300 neuronas y activación ReLU\n",
    "3. **Dense (100)**: Segunda capa oculta con 100 neuronas y activación ReLU\n",
    "4. **Dense (10)**: Capa de salida con 10 neuronas (una por dígito) y activación Softmax\n",
    "\n",
    "### Funciones de Activación:\n",
    "\n",
    "- **ReLU** (Rectified Linear Unit): f(x) = max(0, x)\n",
    "  - Introduce no-linealidad en la red\n",
    "  - Rápida de calcular\n",
    "  - No sufre de vanishing gradient\n",
    "  \n",
    "- **Softmax**: Convierte valores en probabilidades que suman 1\n",
    "  - Necesaria para clasificación multiclase\n",
    "  - Produce distribución de probabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONSTRUCCIÓN DEL MODELO SEQUENTIAL\n",
    "# ============================================\n",
    "#\n",
    "# Keras proporciona dos APIs principales para construir modelos:\n",
    "#\n",
    "# 1. Sequential API (la que usaremos):\n",
    "#    - Para modelos lineales: una capa tras otra\n",
    "#    - Simple e intuitiva\n",
    "#    - Suficiente para el 80% de casos\n",
    "#    - Limitación: solo puede tener una entrada y una salida\n",
    "#\n",
    "# 2. Functional API:\n",
    "#    - Para arquitecturas complejas (múltiples entradas/salidas, skip connections)\n",
    "#    - Más flexible pero más verbosa\n",
    "#    - Necesaria para redes residuales, inception, etc.\n",
    "\n",
    "print(\"CONSTRUCCION DEL MODELO DE RED NEURONAL\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# -------- CREACIÓN DEL MODELO --------\n",
    "# keras.models.Sequential() crea un modelo secuencial\n",
    "# Recibe una lista de capas que se ejecutarán en orden\n",
    "# El output de cada capa se convierte en el input de la siguiente\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    # ============================================\n",
    "    # CAPA 1: FLATTEN (APLANAMIENTO)\n",
    "    # ============================================\n",
    "    # Propósito: Convertir imágenes 2D en vectores 1D\n",
    "    #\n",
    "    # ¿Por qué necesitamos Flatten?\n",
    "    #   - Las capas Dense esperan input 1D (vector)\n",
    "    #   - Nuestras imágenes son 2D (matriz 28×28)\n",
    "    #   - Flatten reorganiza: (28, 28) -> (784,)\n",
    "    #\n",
    "    # Proceso de aplanamiento:\n",
    "    #   - Toma fila por fila y las concatena\n",
    "    #   - Orden: [fila0, fila1, ..., fila27]\n",
    "    #   - Ejemplo 3×3: [[1,2,3], [4,5,6], [7,8,9]] -> [1,2,3,4,5,6,7,8,9]\n",
    "    #\n",
    "    # Parámetros:\n",
    "    #   input_shape=[28, 28]:\n",
    "    #     - Define la forma de cada muestra individual\n",
    "    #     - NO incluimos el batch size (se añade automáticamente)\n",
    "    #     - [28, 28] = imagen de 28 filas × 28 columnas\n",
    "    #   name='flatten_layer':\n",
    "    #     - Nombre descriptivo para identificar la capa\n",
    "    #     - Útil para debugging y visualización\n",
    "    #\n",
    "    # Parámetros entrenables: 0 (solo reorganiza, no aprende)\n",
    "    keras.layers.Flatten(input_shape=[28, 28], name='flatten_layer'),\n",
    "    \n",
    "    # ============================================\n",
    "    # CAPA 2: PRIMERA CAPA OCULTA (DENSE)\n",
    "    # ============================================\n",
    "    # Propósito: Aprender representaciones abstractas de los píxeles\n",
    "    #\n",
    "    # Dense = Capa completamente conectada (Fully Connected)\n",
    "    #   - Cada neurona está conectada a TODAS las entradas\n",
    "    #   - Para 300 neuronas y 784 entradas: 300 × 784 = 235,200 conexiones\n",
    "    #\n",
    "    # Cálculo en cada neurona:\n",
    "    #   1. Producto punto: suma ponderada de todas las entradas\n",
    "    #      output = w1*x1 + w2*x2 + ... + w784*x784 + bias\n",
    "    #   2. Aplicar función de activación: ReLU(output)\n",
    "    #\n",
    "    # Parámetros:\n",
    "    #   300: número de neuronas en esta capa\n",
    "    #     - Cada neurona puede aprender un \"patrón\" diferente\n",
    "    #     - Más neuronas = más capacidad de aprendizaje\n",
    "    #     - Pero también más riesgo de overfitting y más cómputo\n",
    "    #     - 300 es un buen equilibrio para MNIST\n",
    "    #\n",
    "    #   activation='relu':\n",
    "    #     - ReLU (Rectified Linear Unit): f(x) = max(0, x)\n",
    "    #     - Si x > 0: devuelve x (identidad)\n",
    "    #     - Si x <= 0: devuelve 0 (\"desactiva\" la neurona)\n",
    "    #     - Ventajas:\n",
    "    #       * Rápida de calcular (solo una comparación)\n",
    "    #       * No sufre vanishing gradient (gradiente no desaparece)\n",
    "    #       * Introduce no-linealidad (permite aprender patrones complejos)\n",
    "    #       * Esparsa: muchas neuronas se \"apagan\" (output = 0)\n",
    "    #     - Alternativas: sigmoid, tanh, leaky_relu, elu\n",
    "    #\n",
    "    # Parámetros entrenables: 784 × 300 + 300 = 235,500\n",
    "    #   - Pesos (weights): 784 × 300 = 235,200\n",
    "    #     Cada conexión tiene un peso que se ajusta durante entrenamiento\n",
    "    #   - Sesgos (biases): 300\n",
    "    #     Cada neurona tiene un sesgo (threshold) propio\n",
    "    keras.layers.Dense(300, activation='relu', name='hidden_layer_1'),\n",
    "    \n",
    "    # ============================================\n",
    "    # CAPA 3: SEGUNDA CAPA OCULTA (DENSE)\n",
    "    # ============================================\n",
    "    # Propósito: Combinar los patrones de la capa anterior en patrones más complejos\n",
    "    #\n",
    "    # Esta capa recibe como input el output de la capa anterior (300 valores)\n",
    "    # Cada una de las 100 neuronas aquí está conectada a las 300 de antes\n",
    "    #\n",
    "    # Jerarquía de abstracción:\n",
    "    #   - Capa 1: detecta bordes, curvas básicas\n",
    "    #   - Capa 2: combina bordes en formas (círculos, líneas)\n",
    "    #   - Capa 3: combina formas en dígitos completos\n",
    "    #\n",
    "    # Parámetros:\n",
    "    #   100: número de neuronas\n",
    "    #     - Menos que la capa anterior (300 -> 100)\n",
    "    #     - Esto crea un \"cuello de botella\"\n",
    "    #     - Fuerza a la red a comprimir información importante\n",
    "    #     - Actúa como regularización implícita\n",
    "    #\n",
    "    #   activation='relu':\n",
    "    #     - Misma activación que la capa anterior\n",
    "    #     - Mantiene la no-linealidad en capas profundas\n",
    "    #     - Permite gradientes que fluyen bien en backpropagation\n",
    "    #\n",
    "    # Parámetros entrenables: 300 × 100 + 100 = 30,100\n",
    "    #   - Pesos: 300 × 100 = 30,000\n",
    "    #   - Sesgos: 100\n",
    "    keras.layers.Dense(100, activation='relu', name='hidden_layer_2'),\n",
    "    \n",
    "    # ============================================\n",
    "    # CAPA 4: CAPA DE SALIDA (OUTPUT)\n",
    "    # ============================================\n",
    "    # Propósito: Producir probabilidades para cada una de las 10 clases (dígitos 0-9)\n",
    "    #\n",
    "    # Esta es la capa final que produce la predicción del modelo\n",
    "    # Su diseño depende del tipo de problema:\n",
    "    #   - Clasificación binaria: 1 neurona + sigmoid\n",
    "    #   - Clasificación multiclase: N neuronas + softmax (nuestro caso)\n",
    "    #   - Regresión: 1 neurona + sin activación\n",
    "    #\n",
    "    # Parámetros:\n",
    "    #   10: número de neuronas = número de clases\n",
    "    #     - Una neurona por cada dígito (0, 1, 2, ..., 9)\n",
    "    #     - Cada neurona produce un \"score\" para su clase\n",
    "    #\n",
    "    #   activation='softmax':\n",
    "    #     - Convierte los scores en probabilidades\n",
    "    #     - Fórmula: p_i = exp(score_i) / sum(exp(score_j) for all j)\n",
    "    #     - Propiedades:\n",
    "    #       * Todas las probabilidades están entre 0 y 1\n",
    "    #       * La suma de todas las probabilidades = 1.0\n",
    "    #       * El score más alto se convierte en la probabilidad más alta\n",
    "    #     - Ejemplo:\n",
    "    #       Scores: [2.0, 1.0, 0.1, 0.3, ...]\n",
    "    #       Softmax: [0.65, 0.24, 0.01, 0.01, ...] (suman 1.0)\n",
    "    #       Interpretación: 65% probabilidad de ser clase 0, 24% de ser clase 1, etc.\n",
    "    #\n",
    "    # Parámetros entrenables: 100 × 10 + 10 = 1,010\n",
    "    #   - Pesos: 100 × 10 = 1,000\n",
    "    #   - Sesgos: 10\n",
    "    keras.layers.Dense(10, activation='softmax', name='output_layer')\n",
    "    \n",
    "], name='MNIST_Classifier')  # Nombre del modelo completo\n",
    "\n",
    "print(\"Modelo construido exitosamente\")\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# RESUMEN DEL MODELO\n",
    "# ============================================\n",
    "# model.summary() muestra una tabla con:\n",
    "#   - Nombre de cada capa\n",
    "#   - Tipo de capa\n",
    "#   - Shape del output de cada capa\n",
    "#   - Número de parámetros en cada capa\n",
    "#   - Total de parámetros (entrenables y no entrenables)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ARQUITECTURA DEL MODELO - RESUMEN\")\n",
    "print(\"=\"*70)\n",
    "model.summary()\n",
    "\n",
    "# -------- CÁLCULO MANUAL DE PARÁMETROS --------\n",
    "# Verifiquemos los números manualmente para entender de dónde vienen\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DESGLOSE DETALLADO DE PARAMETROS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Capa Flatten\n",
    "flatten_params = 0\n",
    "print(f\"\\nCapa 1 - Flatten:\")\n",
    "print(f\"  Input:  (28, 28) = 784 píxeles\")\n",
    "print(f\"  Output: (784,) = 784 valores\")\n",
    "print(f\"  Parámetros: {flatten_params} (solo reorganiza, no aprende)\")\n",
    "\n",
    "# Primera capa Dense\n",
    "dense1_weights = 784 * 300  # Una peso por cada conexión\n",
    "dense1_biases = 300  # Un sesgo por cada neurona\n",
    "dense1_params = dense1_weights + dense1_biases\n",
    "print(f\"\\nCapa 2 - Dense(300, relu):\")\n",
    "print(f\"  Input:  784 valores (de Flatten)\")\n",
    "print(f\"  Output: 300 valores (una por neurona)\")\n",
    "print(f\"  Pesos (weights):  784 × 300 = {dense1_weights:,}\")\n",
    "print(f\"  Sesgos (biases):  300 = {dense1_biases}\")\n",
    "print(f\"  Total parámetros: {dense1_params:,}\")\n",
    "print(f\"  Memoria: ~{dense1_params * 4 / (1024*1024):.2f} MB (en float32)\")\n",
    "\n",
    "# Segunda capa Dense\n",
    "dense2_weights = 300 * 100\n",
    "dense2_biases = 100\n",
    "dense2_params = dense2_weights + dense2_biases\n",
    "print(f\"\\nCapa 3 - Dense(100, relu):\")\n",
    "print(f\"  Input:  300 valores (de Dense anterior)\")\n",
    "print(f\"  Output: 100 valores\")\n",
    "print(f\"  Pesos:  300 × 100 = {dense2_weights:,}\")\n",
    "print(f\"  Sesgos: 100 = {dense2_biases}\")\n",
    "print(f\"  Total parámetros: {dense2_params:,}\")\n",
    "\n",
    "# Capa de salida\n",
    "output_weights = 100 * 10\n",
    "output_biases = 10\n",
    "output_params = output_weights + output_biases\n",
    "print(f\"\\nCapa 4 - Dense(10, softmax):\")\n",
    "print(f\"  Input:  100 valores\")\n",
    "print(f\"  Output: 10 probabilidades (una por dígito 0-9)\")\n",
    "print(f\"  Pesos:  100 × 10 = {output_weights:,}\")\n",
    "print(f\"  Sesgos: 10 = {output_biases}\")\n",
    "print(f\"  Total parámetros: {output_params:,}\")\n",
    "\n",
    "# Total\n",
    "total_params = flatten_params + dense1_params + dense2_params + output_params\n",
    "print(f\"\\n\" + \"-\"*70)\n",
    "print(f\"TOTAL DE PARAMETROS ENTRENABLES: {total_params:,}\")\n",
    "print(f\"Memoria aproximada del modelo: {total_params * 4 / (1024*1024):.2f} MB\")\n",
    "print(f\"\\nEstos {total_params:,} parámetros se ajustarán durante el entrenamiento\")\n",
    "print(f\"para minimizar el error de clasificación.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del Flujo de Datos\n",
    "\n",
    "Veamos cómo cambia la forma de los datos al pasar por cada capa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZACIÓN DEL FLUJO DE DATOS\n",
    "# ============================================\n",
    "# Vamos a \"pasar\" una imagen a través del modelo para ver\n",
    "# cómo se transforma en cada capa\n",
    "#\n",
    "# Esto es útil para:\n",
    "#   - Entender dimensiones en cada paso\n",
    "#   - Debuggear problemas de shape\n",
    "#   - Visualizar la arquitectura\n",
    "\n",
    "print(\"\\nFLUJO DE DATOS A TRAVES DEL MODELO\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "# -------- PREPARAR DATOS DE EJEMPLO --------\n",
    "# Tomamos la primera imagen del training set\n",
    "# X_train[0] tiene shape (28, 28)\n",
    "# Pero el modelo espera un batch: (batch_size, 28, 28)\n",
    "#\n",
    "# np.expand_dims() añade una dimensión:\n",
    "#   axis=0: añadir al principio\n",
    "#   (28, 28) -> (1, 28, 28)\n",
    "# El \"1\" representa un batch de tamaño 1 (una imagen)\n",
    "sample_input = np.expand_dims(X_train[0], axis=0)\n",
    "\n",
    "print(f\"Imagen de entrada:\")\n",
    "print(f\"  Shape: {sample_input.shape}\")\n",
    "print(f\"  Interpretación: (batch_size=1, alto=28, ancho=28)\")\n",
    "print(f\"  Es el dígito: {y_train[0]}\")\n",
    "print()\n",
    "\n",
    "# -------- TABLA DE TRANSFORMACIONES --------\n",
    "# Crearemos una tabla que muestra cómo cambia el shape en cada capa\n",
    "\n",
    "# Encabezado de la tabla\n",
    "print(f\"{'Capa':<25} {'Shape Input':<20} {'Shape Output':<20} {'Parámetros':<15}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Inicialmente, tenemos nuestro sample_input\n",
    "current_output = sample_input\n",
    "print(f\"{'INPUT (imagen)':<25} {'':<20} {str(current_output.shape):<20} {'0':<15}\")\n",
    "\n",
    "# -------- ITERAR SOBRE CADA CAPA --------\n",
    "# model.layers contiene todas las capas del modelo en orden\n",
    "for layer in model.layers:\n",
    "    # Guardar el shape de entrada (output actual)\n",
    "    input_shape = current_output.shape\n",
    "    \n",
    "    # Pasar los datos por esta capa\n",
    "    # layer(input) ejecuta el forward pass de la capa\n",
    "    # Esto aplica:\n",
    "    #   - Pesos y sesgos (si es Dense)\n",
    "    #   - Función de activación\n",
    "    #   - Cualquier otra operación de la capa\n",
    "    current_output = layer(current_output)\n",
    "    \n",
    "    # Obtener el shape de salida\n",
    "    output_shape = current_output.shape\n",
    "    \n",
    "    # Contar parámetros de esta capa\n",
    "    # count_params() retorna el número total de parámetros entrenables\n",
    "    params = layer.count_params()\n",
    "    \n",
    "    # Nombre de la capa (con tipo si no tiene nombre personalizado)\n",
    "    layer_name = f\"{layer.name} ({layer.__class__.__name__})\"\n",
    "    \n",
    "    # Imprimir fila de la tabla\n",
    "    print(f\"{layer_name:<25} {str(input_shape):<20} {str(output_shape):<20} {params:<15,}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# -------- ANÁLISIS DE LAS TRANSFORMACIONES --------\n",
    "print(\"\\nANALISIS DE LAS TRANSFORMACIONES:\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "print(\"\\n1. INPUT -> FLATTEN:\")\n",
    "print(\"   (1, 28, 28) -> (1, 784)\")\n",
    "print(\"   - Convierte matriz 2D en vector 1D\")\n",
    "print(\"   - 28 × 28 = 784 píxeles\")\n",
    "print(\"   - NO aprende nada (0 parámetros), solo reorganiza\")\n",
    "\n",
    "print(\"\\n2. FLATTEN -> DENSE(300):\")\n",
    "print(\"   (1, 784) -> (1, 300)\")\n",
    "print(\"   - Reduce dimensionalidad: 784 -> 300\")\n",
    "print(\"   - Cada una de las 300 neuronas mira TODOS los 784 píxeles\")\n",
    "print(\"   - Aprende patrones de bajo nivel (bordes, curvas)\")\n",
    "print(f\"   - Muchos parámetros: {784 * 300 + 300:,} (784×300 pesos + 300 sesgos)\")\n",
    "\n",
    "print(\"\\n3. DENSE(300) -> DENSE(100):\")\n",
    "print(\"   (1, 300) -> (1, 100)\")\n",
    "print(\"   - Comprime aún más: 300 -> 100\")\n",
    "print(\"   - Crea un 'cuello de botella' que fuerza a extraer características importantes\")\n",
    "print(\"   - Aprende patrones de nivel medio (combinaciones de bordes)\")\n",
    "print(f\"   - Parámetros: {300 * 100 + 100:,}\")\n",
    "\n",
    "print(\"\\n4. DENSE(100) -> OUTPUT(10):\")\n",
    "print(\"   (1, 100) -> (1, 10)\")\n",
    "print(\"   - Expande a 10 valores: uno por cada dígito (0-9)\")\n",
    "print(\"   - Softmax convierte estos valores en probabilidades\")\n",
    "print(\"   - Ejemplo de output: [0.01, 0.02, 0.05, 0.80, 0.03, 0.02, 0.01, 0.02, 0.02, 0.02]\")\n",
    "print(\"                       -> 80% probabilidad de ser el dígito 3\")\n",
    "print(f\"   - Parámetros: {100 * 10 + 10:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVACIONES CLAVE:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. El batch size (primera dimensión) se mantiene en todo el modelo\")\n",
    "print(\"2. La segunda dimensión cambia en cada capa (784 -> 300 -> 100 -> 10)\")\n",
    "print(\"3. La mayoría de parámetros están en la primera capa Dense (235,500)\")\n",
    "print(\"4. El flujo es: Píxeles -> Features -> Representaciones -> Probabilidades\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SECCION 5: Compilación del Modelo\n",
    "\n",
    "## ¿Qué es la compilación?\n",
    "\n",
    "La **compilación** es el paso donde configuramos el proceso de aprendizaje antes de entrenar. Especificamos tres componentes esenciales:\n",
    "\n",
    "### 1. Optimizador (Optimizer)\n",
    "\n",
    "El optimizador es el algoritmo que actualiza los pesos de la red para minimizar el error.\n",
    "\n",
    "**Optimizadores comunes:**\n",
    "\n",
    "- **SGD** (Stochastic Gradient Descent):\n",
    "  - El algoritmo clásico de descenso de gradiente\n",
    "  - Simple pero efectivo\n",
    "  - Requiere ajuste cuidadoso del learning rate\n",
    "  - Con momentum puede ser muy potente\n",
    "  - Fórmula: peso_nuevo = peso_viejo - learning_rate × gradiente\n",
    "\n",
    "- **Adam** (Adaptive Moment Estimation):\n",
    "  - El optimizador más popular actualmente\n",
    "  - Combina las ventajas de AdaGrad y RMSprop\n",
    "  - Ajusta automáticamente el learning rate para cada parámetro\n",
    "  - Funciona bien \"out of the box\" sin mucho tuning\n",
    "  - Mantiene promedios móviles de gradientes y su cuadrado\n",
    "  - Learning rate típico: 0.001\n",
    "\n",
    "- **RMSprop**:\n",
    "  - Diseñado para redes recurrentes\n",
    "  - Adapta el learning rate basándose en promedios de gradientes\n",
    "  - Bueno para problemas no estacionarios\n",
    "\n",
    "### 2. Función de Pérdida (Loss Function)\n",
    "\n",
    "La métrica que queremos minimizar durante el entrenamiento.\n",
    "\n",
    "**Para clasificación:**\n",
    "\n",
    "- **Binary Crossentropy**: Clasificación binaria (2 clases)\n",
    "  - Ejemplo: spam/no-spam, gato/perro\n",
    "  - Usa sigmoid en la salida\n",
    "\n",
    "- **Categorical Crossentropy**: Clasificación multiclase con one-hot encoding\n",
    "  - Labels como vectores: [0, 0, 1, 0, 0]\n",
    "  - Usa softmax en la salida\n",
    "\n",
    "- **Sparse Categorical Crossentropy**: Clasificación multiclase con integers\n",
    "  - Labels como enteros: 0, 1, 2, ..., 9\n",
    "  - Más eficiente en memoria\n",
    "  - Internamente es equivalente a categorical crossentropy\n",
    "  - NUESTRA ELECCIÓN para MNIST\n",
    "\n",
    "**Para regresión:**\n",
    "\n",
    "- **MSE** (Mean Squared Error): Penaliza errores grandes\n",
    "- **MAE** (Mean Absolute Error): Más robusto a outliers\n",
    "- **Huber**: Combinación de MSE y MAE\n",
    "\n",
    "### 3. Métricas (Metrics)\n",
    "\n",
    "Valores adicionales para monitorear el entrenamiento. NO afectan al entrenamiento, solo informan.\n",
    "\n",
    "**Métricas comunes:**\n",
    "\n",
    "- **Accuracy**: Porcentaje de predicciones correctas\n",
    "  - Fácil de interpretar\n",
    "  - Puede engañar en datasets desbalanceados\n",
    "\n",
    "- **Precision**: De las predicciones positivas, cuántas son correctas\n",
    "  - Importante cuando el costo de falsos positivos es alto\n",
    "\n",
    "- **Recall**: De los casos positivos reales, cuántos detectamos\n",
    "  - Importante cuando el costo de falsos negativos es alto\n",
    "\n",
    "- **F1-Score**: Media armónica de precision y recall\n",
    "  - Balance entre precision y recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\\n",
    "\\n",
    "# SECCION 5: Compilación del Modelo\\n",
    "\\n",
    "## ¿Qué es la compilación?\\n",
    "\\n",
    "La **compilación** es el paso donde configuramos el proceso de aprendizaje antes de entrenar. Especificamos tres componentes esenciales:\\n",
    "\\n",
    "### 1. Optimizador (Optimizer)\\n",
    "\\n",
    "El optimizador es el algoritmo que actualiza los pesos de la red para minimizar el error.\\n",
    "\\n",
    "**Optimizadores comunes:**\\n",
    "\\n",
    "- **Adam** (Adaptive Moment Estimation): El más popular\\n",
    "  - Ajusta automáticamente el learning rate\\n",
    "  - Combina momentum y escalado adaptativo\\n",
    "  - Funciona bien sin mucho ajuste\\n",
    "  - Learning rate típico: 0.001\\n",
    "\\n",
    "- **SGD** (Stochastic Gradient Descent): Clásico\\n",
    "  - Simple pero efectivo\\n",
    "  - Requiere ajuste del learning rate\\n",
    "  - Con momentum puede ser muy potente\\n",
    "\\n",
    "### 2. Función de Pérdida (Loss Function)\\n",
    "\\n",
    "La métrica a minimizar durante el entrenamiento.\\n",
    "\\n",
    "**Para clasificación:**\\n",
    "- **Sparse Categorical Crossentropy**: Para labels enteros (nuestro caso)\\n",
    "- **Categorical Crossentropy**: Para labels one-hot encoded\\n",
    "- **Binary Crossentropy**: Para clasificación binaria\\n",
    "\\n",
    "**Para regresión:**\\n",
    "- **MSE** (Mean Squared Error)\\n",
    "- **MAE** (Mean Absolute Error)\\n",
    "\\n",
    "### 3. Métricas (Metrics)\\n",
    "\\n",
    "Valores para monitorear (no afectan el entrenamiento):\\n",
    "- **Accuracy**: Porcentaje de aciertos\\n",
    "- **Precision/Recall**: Para datasets desbalanceados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\\n",
    "# COMPILACIÓN DEL MODELO\\n",
    "# ============================================\\n",
    "# Configuramos los tres componentes esenciales del entrenamiento:\\n",
    "#   1. Optimizer: cómo actualizar los pesos\\n",
    "#   2. Loss function: qué minimizar\\n",
    "#   3. Metrics: qué monitorear\\n",
    "\\n",
    "print(\\\"COMPILACION DEL MODELO\\\")\\n",
    "print(\\\"=\\\"*70)\\n",
    "print()\\n",
    "\\n",
    "# -------- PROCESO DE COMPILACIÓN --------\\n",
    "# model.compile() configura el modelo para entrenamiento\\n",
    "# IMPORTANTE: Este paso NO entrena el modelo, solo lo prepara\\n",
    "# Después de compile(), el modelo está listo para fit()\\n",
    "\\n",
    "model.compile(\\n",
    "    # ============================================\\n",
    "    # PARAMETRO 1: OPTIMIZER (ADAM)\\n",
    "    # ============================================\\n",
    "    # Adam (Adaptive Moment Estimation):\\n",
    "    # Es el optimizador más popular por estas razones:\\n",
    "    #\\n",
    "    # Funcionamiento de Adam:\\n",
    "    # 1. Mantiene dos promedios móviles para cada parámetro:\\n",
    "    #    - m (momentum): promedio móvil del gradiente\\n",
    "    #    - v (velocity): promedio móvil del gradiente al cuadrado\\n",
    "    #\\n",
    "    # 2. Usa estos promedios para ajustar el learning rate:\\n",
    "    #    - Si un parámetro tiene gradientes consistentes -> más actualización\\n",
    "    #    - Si un parámetro tiene gradientes variables -> menos actualización\\n",
    "    #\\n",
    "    # 3. Corrección de sesgo al inicio del entrenamiento\\n",
    "    #\\n",
    "    # Ventajas de Adam:\\n",
    "    #   - Converge rápido: menos épocas necesarias\\n",
    "    #   - Robusto: funciona bien con configuración por defecto\\n",
    "    #   - Adaptativo: ajusta learning rate para cada parámetro\\n",
    "    #   - Eficiente en memoria: solo necesita almacenar m y v\\n",
    "    #\\n",
    "    # Hiperparámetros de Adam (valores por defecto):\\n",
    "    #   - learning_rate: 0.001 (1e-3)\\n",
    "    #   - beta_1: 0.9 (decay rate para momentum)\\n",
    "    #   - beta_2: 0.999 (decay rate para velocity)\\n",
    "    #   - epsilon: 1e-7 (para estabilidad numérica)\\n",
    "    #\\n",
    "    # ¿Cuándo usar Adam?\\n",
    "    #   - Casi siempre es una buena elección inicial\\n",
    "    #   - Especialmente en deep learning\\n",
    "    #   - Cuando no quieres ajustar muchos hiperparámetros\\n",
    "    #\\n",
    "    # Alternativas y cuándo usarlas:\\n",
    "    #   - SGD + momentum: cuando necesitas mejor generalización\\n",
    "    #   - RMSprop: para redes recurrentes\\n",
    "    #   - AdaGrad: cuando los features tienen frecuencias muy diferentes\\n",
    "    optimizer='adam',\\n",
    "    \\n",
    "    # ============================================\\n",
    "    # PARAMETRO 2: LOSS FUNCTION\\n",
    "    # ============================================\\n",
    "    # Sparse Categorical Crossentropy:\\n",
    "    # Función de pérdida para clasificación multiclase\\n",
    "    #\\n",
    "    # ¿Qué significa \\\"Sparse\\\"?\\n",
    "    #   - Las etiquetas son enteros: 0, 1, 2, ..., 9\\n",
    "    #   - NO son one-hot encoded: [0,0,0,1,0,0,0,0,0,0]\\n",
    "    #   - Ahorra memoria y es más eficiente\\n",
    "    #\\n",
    "    # ¿Cómo funciona Categorical Crossentropy?\\n",
    "    #\\n",
    "    # 1. El modelo produce probabilidades para cada clase:\\n",
    "    #    Ejemplo: [0.05, 0.10, 0.02, 0.70, 0.03, 0.05, 0.01, 0.02, 0.01, 0.01]\\n",
    "    #    Interpretación: 70% probabilidad de ser clase 3\\n",
    "    #\\n",
    "    # 2. Tomamos el logaritmo de la probabilidad de la clase correcta:\\n",
    "    #    Si la etiqueta real es 3:\\n",
    "    #    loss = -log(0.70) = 0.357\\n",
    "    #\\n",
    "    # 3. Penalizaciones según confianza:\\n",
    "    #    - Si predice correctamente con 99% confianza: loss ≈ 0.01 (muy bajo)\\n",
    "    #    - Si predice correctamente con 50% confianza: loss ≈ 0.69 (medio)\\n",
    "    #    - Si predice correctamente con 10% confianza: loss ≈ 2.30 (alto)\\n",
    "    #    - Si predice incorrectamente: loss puede ser infinito\\n",
    "    #\\n",
    "    # ¿Por qué usar logaritmo?\\n",
    "    #   - Penaliza fuertemente las predicciones incorrectas\\n",
    "    #   - Gradientes bien definidos para backpropagation\\n",
    "    #   - Matemáticamente equivalente a maximizar likelihood\\n",
    "    #\\n",
    "    # Comparación con otras loss functions:\\n",
    "    #   - Mean Squared Error (MSE): \\n",
    "    #     * NO recomendado para clasificación\\n",
    "    #     * Gradientes problemáticos con softmax\\n",
    "    #     * Bueno para regresión\\n",
    "    #   - Categorical Crossentropy (no-sparse):\\n",
    "    #     * Funcionalmente idéntico pero requiere one-hot encoding\\n",
    "    #     * Usa más memoria\\n",
    "    #   - Binary Crossentropy:\\n",
    "    #     * Solo para 2 clases\\n",
    "    #     * Usa sigmoid en lugar de softmax\\n",
    "    loss='sparse_categorical_crossentropy',\\n",
    "    \\n",
    "    # ============================================\\n",
    "    # PARAMETRO 3: METRICS\\n",
    "    # ============================================\\n",
    "    # Accuracy: métrica de evaluación\\n",
    "    #\\n",
    "    # ¿Qué es accuracy?\\n",
    "    #   - Porcentaje de predicciones correctas\\n",
    "    #   - Fórmula: (predicciones_correctas / total_predicciones) × 100\\n",
    "    #   - Rango: 0% (terrible) a 100% (perfecto)\\n",
    "    #\\n",
    "    # Ejemplo de cálculo:\\n",
    "    #   Si tenemos 100 imágenes:\\n",
    "    #   - Modelo predice correctamente 95\\n",
    "    #   - Accuracy = 95/100 = 0.95 = 95%\\n",
    "    #\\n",
    "    # IMPORTANTE: Diferencia entre loss y accuracy:\\n",
    "    #   - Loss: mide qué tan \\\"seguros\\\" están las predicciones incorrectas\\n",
    "    #     * Penaliza predicciones incorrectas con alta confianza\\n",
    "    #     * Es lo que el modelo MINIMIZA durante entrenamiento\\n",
    "    #   - Accuracy: solo cuenta aciertos vs errores\\n",
    "    #     * Ignora la confianza de las predicciones\\n",
    "    #     * Solo para MONITOREAR, no para entrenar\\n",
    "    #\\n",
    "    # ¿Por qué no entrenar directamente con accuracy?\\n",
    "    #   - Accuracy NO es diferenciable (gradientes indefinidos)\\n",
    "    #   - Cambia en \\\"saltos\\\" discretos\\n",
    "    #   - No proporciona dirección para mejorar\\n",
    "    #\\n",
    "    # Limitaciones de accuracy:\\n",
    "    #   - Engañosa en datasets desbalanceados\\n",
    "    #     Ejemplo: 95% clase A, 5% clase B\\n",
    "    #     Un modelo que siempre predice A tiene 95% accuracy\\n",
    "    #     ¡Pero es inútil para detectar clase B!\\n",
    "    #   - No distingue entre diferentes tipos de errores\\n",
    "    #     Falso positivo vs falso negativo cuenta igual\\n",
    "    #\\n",
    "    # Métricas alternativas (más avanzadas):\\n",
    "    #   - Precision: De las predicciones positivas, cuántas son correctas\\n",
    "    #   - Recall: De los casos positivos reales, cuántos detectamos\\n",
    "    #   - F1-Score: Media armónica de precision y recall\\n",
    "    #   - AUC-ROC: Area under curve para diferentes thresholds\\n",
    "    #   - Confusion Matrix: Tabla detallada de errores\\n",
    "    #\\n",
    "    # Para MNIST, accuracy es suficiente porque:\\n",
    "    #   - Dataset balanceado (similar cantidad por clase)\\n",
    "    #   - Todas las clases son igualmente importantes\\n",
    "    #   - Métrica intuitiva para principiantes\\n",
    "    metrics=['accuracy']\\n",
    ")\\n",
    "\\n",
    "print(\\\"Modelo compilado exitosamente\\\")\\n",
    "print()\\n",
    "\\n",
    "# ============================================\\n",
    "# VERIFICACIÓN DE LA CONFIGURACIÓN\\n",
    "# ============================================\\n",
    "# Mostremos la configuración del modelo compilado\\n",
    "\\n",
    "print(\\\"=\\\"*70)\\n",
    "print(\\\"CONFIGURACION DEL ENTRENAMIENTO\\\")\\n",
    "print(\\\"=\\\"*70)\\n",
    "print()\\n",
    "\\n",
    "# Información del optimizer\\n",
    "print(f\\\"Optimizer: {model.optimizer.__class__.__name__}\\\")\\n",
    "print(f\\\"  - Tipo: Optimizador adaptativo\\\")\\n",
    "print(f\\\"  - Learning Rate: {model.optimizer.learning_rate.numpy()}\\\")\\n",
    "print(f\\\"  - Beta 1 (momentum decay): {model.optimizer.beta_1.numpy()}\\\")\\n",
    "print(f\\\"  - Beta 2 (velocity decay): {model.optimizer.beta_2.numpy()}\\\")\\n",
    "print(f\\\"  - Epsilon: {model.optimizer.epsilon}\\\")\\n",
    "print()\\n",
    "\\n",
    "# Información de la loss function\\n",
    "print(f\\\"Loss Function: {model.loss}\\\")\\n",
    "print(f\\\"  - Uso: Clasificacion multiclase\\\")\\n",
    "print(f\\\"  - Entrada: Etiquetas como enteros (0-9)\\\")\\n",
    "print(f\\\"  - Salida: Valor escalar a minimizar\\\")\\n",
    "print()\\n",
    "\\n",
    "# Información de las métricas\\n",
    "print(f\\\"Metrics: {model.metrics_names}\\\")\\n",
    "print(f\\\"  - Accuracy: Porcentaje de predicciones correctas\\\")\\n",
    "print(f\\\"  - Solo para monitoreo (no afecta el entrenamiento)\\\")\\n",
    "print()\\n",
    "\\n",
    "print(\\\"=\\\"*70)\\n",
    "print(\\\"El modelo está listo para comenzar el entrenamiento\\\")\\n",
    "print(\\\"=\\\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}