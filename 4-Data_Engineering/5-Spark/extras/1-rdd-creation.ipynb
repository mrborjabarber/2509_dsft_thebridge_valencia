{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Introducción a Spark con Python, por Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook introduciremos dos formas diferentes de cargar datos en la estructura básica de Spark, el **Resilient Distributed Dataset** o **RDD**. Un RDD es una colección distribuida de elementos. Todo el trabajo en Spark se expresa como la creación de nuevos RDDs, la transformación de RDDs existentes, o la ejecución de acciones sobre RDDs para calcular un resultado. Spark distribuye automáticamente los datos contenidos en los RDDs a través de tu clúster y paraleliza las operaciones que realizas sobre ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El libro de referencia para estos y otros temas relacionados con Spark es *Learning Spark* de Holden Karau, Andy Konwinski, Patrick Wendell, y Matei Zaharia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos de la competición KDD Cup 1999 está descrito en detalle [aquí](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería pyspark para trabajar con Spark\n",
    "import pyspark\n",
    "\n",
    "# Obtenemos el SparkContext desde la SparkSession existente\n",
    "# El SparkContext es el punto de entrada principal para la funcionalidad de Spark\n",
    "sc = spark.sparkContext # spark es la SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el SparkContext para verificar que está activo\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo los archivos de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook usaremos el conjunto de datos reducido (10 por ciento) proporcionado para la KDD Cup 1999, que contiene casi medio millón de interacciones de red. El archivo se proporciona como un archivo *Gzip* que descargaremos localmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la URL del archivo de datos que queremos descargar\n",
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "\n",
    "# Importamos SparkFiles para gestionar archivos distribuidos\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "# Añadimos el archivo al contexto de Spark para que esté disponible en todos los nodos\n",
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un RDD leyendo el archivo de texto comprimido\n",
    "# SparkFiles.get() obtiene la ruta local del archivo descargado\n",
    "# textFile() puede manejar archivos comprimidos directamente\n",
    "myRDD = sc.textFile(\"file://\" + SparkFiles.get(\"kddcup.data_10_percent.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos la clase SparkFiles\n",
    "SparkFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos los archivos disponibles en el sistema de archivos de Databricks\n",
    "display(dbutils.fs.ls(\"/databricks-datasets/COVID/USAFacts/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos las primeras 5 líneas del RDD para visualizar los datos\n",
    "# take() es una acción que devuelve los primeros n elementos\n",
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un RDD desde un archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma más común de crear un RDD es cargarlo desde un archivo. Observa que el método `textFile` de Spark puede manejar archivos comprimidos directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el tipo de objeto que hemos creado\n",
    "type(myRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos nuestro archivo de datos cargado en el RDD `raw_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin entrar en las *transformaciones* y *acciones* de Spark, lo más básico que podemos hacer para verificar que obtuvimos el contenido correcto del RDD es usar `count()` para contar el número de líneas cargadas desde el archivo al RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos el número total de filas en el RDD\n",
    "# count() es una acción que devuelve el número de elementos\n",
    "myRDD.count() # Cantidad de filas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos verificar las primeras entradas en nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos las primeras 2 líneas del RDD\n",
    "myRDD.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los siguientes notebooks, usaremos estos datos crudos para aprender sobre las diferentes transformaciones y acciones de Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando un RDD usando `parallelize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de crear un RDD es paralelizar una lista ya existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un rango de 100 números (0 a 99)\n",
    "a = range(100)\n",
    "\n",
    "# Paralelizamos el rango para crear un RDD distribuido\n",
    "# Cuando no especificamos el número de particiones, depende de los cores disponibles\n",
    "data = sc.parallelize(a) # leer del range y me va a devolver un RDD, ¿cuántas particiones? Cuando no digo nada, depende de los cores\n",
    "\n",
    "# Verificamos el tipo de objeto creado\n",
    "type(data) # todo esto es lazy (perezoso), se lo ha apuntado pero no se ha ejecutado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hicimos antes, podemos usar `count()` para contar el número de elementos en el RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos los elementos en el RDD\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como antes, podemos acceder a los primeros elementos de nuestro RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los primeros 5 elementos del RDD\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener datos y particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recopilamos todos los datos del RDD al nodo driver\n",
    "# ¡CUIDADO! collect() trae TODOS los datos a memoria, puede ser peligroso con datasets grandes\n",
    "rddCollect = data.collect() # data RDD (Resilient Distributed Dataset) es una estructura. Con collect lee todas las particiones y se meten en el nodo driver, y se puede dañar\n",
    "\n",
    "# Mostramos el número de particiones en las que está dividido el RDD\n",
    "print(\"Number of Partitions: \" + str(data.getNumPartitions()))\n",
    "\n",
    "# Obtenemos el primer elemento del RDD (es una acción)\n",
    "print(\"Action: First element: \" + str(data.first()))\n",
    "\n",
    "# Imprimimos todos los datos recopilados\n",
    "print(rddCollect)\n",
    "\n",
    "# Nota: Para especificar el número de particiones:\n",
    "# data = sc.parallelize(a, p) # hace p particiones"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
