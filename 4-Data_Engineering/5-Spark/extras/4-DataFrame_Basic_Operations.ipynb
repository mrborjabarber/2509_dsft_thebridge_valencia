{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones Básicas\n",
    "\n",
    "Esta lección cubrirá algunas operaciones básicas con DataFrames de Spark.\n",
    "\n",
    "Vamos a trabajar con algunos datos de aerolíneas de una muestra pequeña."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display() es una función específica de Databricks para visualizar datos de forma interactiva\n",
    "# dbutils.fs.ls() lista los archivos y directorios en el sistema de archivos de Databricks\n",
    "# Exploramos qué archivos hay disponibles en el directorio especificado\n",
    "display(dbutils.fs.ls(\"/databricks-datasets/asa/small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos un archivo CSV desde el sistema de archivos de Databricks\n",
    "# inferSchema=True: Spark analizará los datos para detectar automáticamente los tipos de dato\n",
    "# header=True: indica que la primera fila del CSV contiene los nombres de las columnas\n",
    "# Esto es más conveniente pero menos eficiente que especificar el esquema manualmente\n",
    "df = spark.read.csv(\"dbfs:/databricks-datasets/asa/small/small.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimimos el esquema del DataFrame para entender su estructura\n",
    "# Esto nos muestra todas las columnas, sus tipos de datos y si aceptan valores nulos\n",
    "# Vemos que tenemos 29 columnas relacionadas con datos de vuelos\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take(n) devuelve las primeras n filas del DataFrame como una lista de objetos Row\n",
    "# display() muestra estos datos en un formato visual interactivo de Databricks\n",
    "# Esto nos permite inspeccionar rápidamente algunos ejemplos de los datos\n",
    "display(df.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtrando Datos\n",
    "\n",
    "Una parte importante del trabajo con DataFrames es la capacidad de filtrar rápidamente datos basados en condiciones. Los DataFrames de Spark están construidos sobre la plataforma Spark SQL, lo que significa que si ya conoces SQL, puedes obtener rápida y fácilmente esos datos usando comandos SQL, o usando los métodos de DataFrame (que es en lo que nos enfocamos en este curso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter() permite filtrar filas basándose en una condición\n",
    "# Aquí usamos sintaxis SQL: seleccionamos solo vuelos donde ActualElapsedTime < 120 minutos\n",
    "# El resultado es un nuevo DataFrame con solo las filas que cumplen la condición\n",
    "# display() muestra estos datos filtrados en formato visual\n",
    "display(df.filter(\"ActualElapsedTime<120\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encadenamos operaciones: primero filtramos, luego seleccionamos una columna específica\n",
    "# filter() reduce las filas según la condición\n",
    "# select() selecciona solo la columna \"DepTime\" del resultado filtrado\n",
    "# show() muestra las primeras 20 filas en formato tabla ASCII\n",
    "df.filter(\"ActualElapsedTime<120\").select(\"DepTime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sintaxis alternativa usando notación tipo pandas con corchetes\n",
    "# df.ActualElapsedTime accede a la columna sin comillas (requiere que el nombre sea válido en Python)\n",
    "# Esta sintaxis es útil cuando la columna no está distribuida o cuando prefieres sintaxis Python\n",
    "# El resultado es idéntico al ejemplo anterior\n",
    "df[df.ActualElapsedTime<120].select(\"DepTime\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinamos filtro con selección de múltiples columnas\n",
    "# select() puede recibir una lista de nombres de columnas\n",
    "# Aquí seleccionamos \"DayOfWeek\" y \"DayOfMonth\" de los vuelos filtrados\n",
    "# Útil para análisis donde necesitamos ver varias columnas relacionadas\n",
    "df.filter(\"ActualElapsedTime<120\").select([\"DayOfWeek\", \"DayOfMonth\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usar operadores de comparación normales de Python es otra forma de hacer esto, se verán muy similares a los operadores SQL, excepto que necesitas asegurarte de que estás llamando a la columna completa dentro del dataframe, usando el formato: df[\"nombre_columna\"]\n",
    "\n",
    "Veamos algunos ejemplos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos operadores de comparación de Python directamente sobre columnas\n",
    "# df[\"ActualElapsedTime\"] < 200 crea una expresión booleana\n",
    "# filter() evalúa esta expresión para cada fila y mantiene solo las que son True\n",
    "# Esta sintaxis es más \"pythónica\" que la sintaxis SQL con comillas\n",
    "display(df.filter(df[\"ActualElapsedTime\"] < 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJEMPLO DE ERROR: intentamos usar 'and' de Python con columnas de Spark\n",
    "# Esto falla porque 'and' es un operador de Python, no de Spark\n",
    "# En PySpark debemos usar operadores bitwise: & para AND, | para OR, ~ para NOT\n",
    "# Este código está comentado porque genera un ValueError\n",
    "df.filter(df[\"DayOfWeek\"]<5 and df[\"DayOfMonth\"]> 15).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMA CORRECTA: usamos & en lugar de 'and' para combinar condiciones\n",
    "# Los paréntesis son OBLIGATORIOS alrededor de cada condición\n",
    "# (df[\"DayOfWeek\"]<5) filtra días laborables (lunes=1 a jueves=4)\n",
    "# (df[\"DayOfMonth\"]> 15) filtra la segunda mitad del mes\n",
    "# & combina ambas condiciones: ambas deben ser True\n",
    "display(df.filter((df[\"DayOfWeek\"]<5) & (df[\"DayOfMonth\"]> 15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos por una sola condición y seleccionamos múltiples columnas\n",
    "# DayOfWeek < 5 significa lunes (1) a jueves (4), excluyendo viernes-domingo\n",
    "# select() con lista nos da solo las columnas de interés del DataFrame filtrado\n",
    "# Útil para análisis enfocados en variables específicas\n",
    "df.filter(df[\"DayOfWeek\"] < 5).select([\"ActualElapsedTime\", \"TailNum\", \"AirTime\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo combinado: filtro con dos condiciones y selección limitada\n",
    "# & requiere que AMBAS condiciones sean verdaderas\n",
    "# take(5) limita el resultado a solo 5 filas (más eficiente que .show(5))\n",
    "# display() muestra estas 5 filas en formato visual interactivo\n",
    "display(df.filter((df[\"DayOfWeek\"] < 5) & (df[\"DayOfMonth\"] > 20)).take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operador | (pipe) representa OR lógico en PySpark\n",
    "# Una fila se incluye si CUALQUIERA de las condiciones es verdadera\n",
    "# DayOfWeek < 5 (días laborables) O DayOfMonth > 20 (final del mes)\n",
    "# Esto incluirá más filas que usar & (AND)\n",
    "display(df.filter((df[\"DayOfWeek\"] < 5) |(df[\"DayOfMonth\"] > 20)).take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operador ~ (tilde) representa NOT lógico en PySpark\n",
    "# ~(df[\"DayOfMonth\"] < 20) es equivalente a df[\"DayOfMonth\"] >= 20\n",
    "# Combinamos: días laborables (DayOfWeek < 5) Y día 20 o posterior del mes\n",
    "# El NOT es útil cuando la condición negativa es más natural de expresar\n",
    "display(df.filter((df[\"DayOfWeek\"] < 5) & ~(df[\"DayOfMonth\"] < 20)).take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro por igualdad exacta usando ==\n",
    "# DayOfWeek == 2 selecciona solo los martes (donde 1=lunes, 2=martes, etc.)\n",
    "# take(2) devuelve solo las primeras 2 filas que cumplen la condición\n",
    "# Útil para análisis de patrones específicos de días de la semana\n",
    "display(df.filter(df[\"DayOfWeek\"] == 2).take(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect() trae TODAS las filas filtradas a la memoria del driver como lista\n",
    "# CUIDADO: usar collect() con grandes datasets puede causar problemas de memoria\n",
    "# Solo usar collect() cuando estés seguro de que el resultado es pequeño\n",
    "# display() muestra todas las filas del día 3 del mes\n",
    "display(df.filter(df[\"DayOfMonth\"] ==3).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtramos vuelos del miércoles (DayOfWeek == 3) y guardamos el resultado\n",
    "# collect() devuelve una lista de objetos Row en Python\n",
    "# Almacenamos esta lista en 'result' para procesamiento posterior\n",
    "# Es importante distinguir: result es una lista de Python, no un DataFrame\n",
    "result = df.filter(df[\"DayOfWeek\"] == 3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos el tipo del primer elemento de la lista\n",
    "# result[0] es un objeto Row de PySpark\n",
    "# Row es una estructura que representa una fila con acceso por nombre de columna\n",
    "# Similar a un diccionario pero optimizado para Spark\n",
    "type(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos la primera fila (Row) en la variable 'row' para análisis detallado\n",
    "# Esto nos permite trabajar con una fila individual de forma más conveniente\n",
    "# Podemos acceder a los valores de la fila usando row['nombre_columna'] o row.nombre_columna\n",
    "row = result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asDict() convierte el objeto Row en un diccionario estándar de Python\n",
    "# Las claves del diccionario son los nombres de las columnas\n",
    "# Los valores son los datos correspondientes de esa fila\n",
    "# Útil para serialización, debugging, o cuando necesitas trabajar con diccionarios\n",
    "row.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteramos sobre todos los valores de la primera fila\n",
    "# Un objeto Row es iterable: recorre los valores en el orden de las columnas\n",
    "# Cada 'item' es el valor de una columna en esa fila específica\n",
    "# Imprimimos cada valor en una línea separada para inspección detallada\n",
    "for item in result[0]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Eso es todo por ahora! ¡Buen trabajo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
