{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "En este notebook vas a ver un ejemplo de los procesos necesarios para realizar un análisis de sentimientos sobre Tweets. Para ello tendremos que seguir los siguientes pasos:\n",
    "1. Conseguir un Corpus: no es más que una base de datos de texto etiquetado\n",
    "2. Limpiar los datos\n",
    "3. Entrenar un modelo con el corpus\n",
    "4. Atacar a la API de Twitter\n",
    "5. Predecir los nuevos Tweets\n",
    "\n",
    "**Estos programas son muy útiles en campañas de marketing, para monitorizar el lanzamiento de un nuevo producto, realizar seguimiento en Twitter de eventos, o simplemente tener monitorizadas ciertas cuentas o hashtags para tener un programa de análisis real time.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Corpus\n",
    "Para conseguir el corpus tendremos que registrarnos en la [página del TASS](http://tass.sepln.org/tass_data/download.php), que se trata de una asociación de análisis semántico que encargada de recopilar texto y mantenerlo etiquetado. \n",
    "\n",
    "Para datasets en ingles lo tenemos más fácil ya que con librerías como [TextBlob](https://textblob.readthedocs.io/en/dev/) podemos predecir directamente la polaridad del Tweet, con modelos ya preentrenados. En el caso del castellano necesitamos acudir a un corpus etiquetado para entrenar nuestro modelo.\n",
    "\n",
    "Registrate en el TASS y accede a sus corpus a través de un link que te llegará al correo tras el registro.\n",
    "\n",
    "![imagen](img/tass_register.png)\n",
    "\n",
    "\n",
    "Una vez estes registrado, descárgate el corpus de tweets en español de entrenamiento. En este punto lo ideal es coger un corpus que se adapte lo máximo posible a los tipos de tweets que intentamos predecir, es decir, si queremos predecir tweets sobre política, procurar elegir un corpus que tenga vocabulario de política.\n",
    "\n",
    "En este notebook se va a elegir un corpus genérico con no demasiados registros para aligerar la limpieza y entrenamiento de los modelos.\n",
    "\n",
    "![imagen](img/download_train_spanish.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos las librerías necesarias para el proyecto\n\n# pandas: para manipulación y análisis de datos en formato tabular (DataFrames)\nimport pandas as pd\n\n# xml.etree.ElementTree: para leer y parsear archivos XML (nuestro corpus viene en este formato)\nimport xml.etree.ElementTree as ET\n\n# seaborn: para crear visualizaciones estadísticas atractivas\nimport seaborn as sns\n\n# sklearn.svm: Support Vector Machines para clasificación\nfrom sklearn.svm import LinearSVC  # Versión lineal (más rápida)\nfrom sklearn.svm import SVC  # Versión con kernel (más flexible)\n\n# Pipeline: para encadenar múltiples pasos del proceso de ML en un solo objeto\nfrom sklearn.pipeline import Pipeline\n\n# GridSearchCV: para búsqueda exhaustiva de los mejores hiperparámetros mediante validación cruzada\nfrom sklearn.model_selection import GridSearchCV"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leemos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parseamos el archivo XML que contiene el corpus de tweets etiquetados\n# El corpus viene descargado del TASS (Workshop on Semantic Analysis at SEPLN)\n\n# parse(): lee el archivo XML y lo convierte en un árbol de elementos\ntree = ET.parse('data/general-train-tagged.xml')\n\n# getroot(): obtiene el elemento raíz del árbol XML (el nodo principal)\nroot = tree.getroot()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creamos un diccionario vacío para almacenar los datos extraídos del XML\n# Cada clave será una columna del DataFrame final\nraw_dict = {\n    'User': [],        # Usuario que escribió el tweet\n    'Content': [],     # Contenido del tweet (el texto)\n    'Date': [],        # Fecha de publicación\n    'Lang': [],        # Idioma del tweet\n    'Polarity': [],    # Polaridad del sentimiento (P+, P, NEU, N, N+, NONE)\n    'Type': []         # Tipo de tweet (AGREEMENT, DISAGREEMENT, etc.)\n}\n\n# Iteramos sobre cada elemento 'tweet' en el XML\nfor i in root.iter('tweet'):\n    # Extraemos cada campo del tweet mediante find() y obtenemos su texto con .text\n    user = i.find('user').text\n    content = i.find('content').text\n    date = i.find('date').text\n    lang = i.find('lang').text\n    \n    # La polaridad está anidada dentro de sentiments > polarity > value\n    polarity = i.find('sentiments').find('polarity').find('value').text\n    \n    # El tipo también está dentro de sentiments > polarity > type\n    tweet_type = i.find('sentiments').find('polarity').find('type').text\n    \n    # Añadimos cada valor a su lista correspondiente en el diccionario\n    raw_dict['User'].append(user)\n    raw_dict['Content'].append(content)\n    raw_dict['Date'].append(date)\n    raw_dict['Lang'].append(lang)\n    raw_dict['Polarity'].append(polarity)\n    raw_dict['Type'].append(tweet_type)\n\n# Convertimos el diccionario en un DataFrame de pandas para facilitar su manipulación\ndf = pd.DataFrame(raw_dict)\n\n# Mostramos la forma del DataFrame (número de filas y columnas)\nprint(df.shape)\n\n# Visualizamos las primeras 5 filas para entender la estructura de los datos\ndf.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuramos pandas para mostrar el contenido completo de las columnas\n# Por defecto, pandas trunca el texto largo con '...'\n# max_colwidth=None permite ver el texto completo de los tweets\npd.set_option('max_colwidth', None)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizamos de nuevo las primeras filas, ahora con el texto completo\n# Esto nos ayuda a entender mejor el contenido real de los tweets\ndf.head()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columna de polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Exploramos los valores únicos de la columna Polarity\n# Esto nos muestra qué categorías de sentimiento tenemos en el dataset\n# Resultado esperado: NONE, NEU, P, P+, N, N+\n# P+ = Muy positivo, P = Positivo, NEU = Neutral, N = Negativo, N+ = Muy negativo, NONE = Sin sentimiento\ndf.Polarity.unique()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creamos un gráfico de barras para visualizar la distribución de polaridades\n# countplot: cuenta cuántos tweets hay de cada categoría y los representa en barras\n# x='Polarity': usamos la columna Polarity en el eje X\n# Esto nos ayuda a ver si hay desbalanceo de clases en nuestro dataset\nsns.countplot(x = 'Polarity', data=df);"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columna de tipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizamos la distribución de la columna Type\n# Esta columna indica si el tweet expresa acuerdo (AGREEMENT) o desacuerdo (DISAGREEMENT)\n# Es útil para entender el contexto de los sentimientos\nsns.countplot(x = 'Type', data=df);"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Limpieza de datos\n",
    "#### Polaridad\n",
    "Vamos a clasificar los Tweets como buenos o malos, por lo que haremos la siguiente agrupación de la polaridad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Función para simplificar la polaridad a un problema binario (positivo vs negativo)\n# En lugar de 6 categorías, vamos a tener solo 2: 0 (positivo) y 1 (negativo)\n\ndef polaridad_fun(x):\n    # Si la polaridad es P (positivo) o P+ (muy positivo), devolvemos 0\n    if x in ('P', 'P+'):\n        return 0\n    # Si la polaridad es N (negativo) o N+ (muy negativo), devolvemos 1\n    elif x in ('N', 'N+'):\n        return 1\n    # NONE y NEU los filtraremos después, por eso no los incluimos aquí"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Eliminamos los tweets neutros y sin polaridad definida\n# ~df['Polarity'].isin([...]) significa \"NO está en la lista\"\n# Nos quedamos solo con tweets claramente positivos o negativos\n\ndf = df[~df['Polarity'].isin(['NONE', 'NEU'])]\n\n# Verificamos que solo quedan las polaridades que nos interesan: P, P+, N, N+\ndf['Polarity'].unique()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aplicamos la función de transformación a toda la columna Polarity\n# apply(): aplica una función a cada elemento de la columna\n# Ahora Polarity contendrá solo 0 (positivo) o 1 (negativo)\n\ndf['Polarity'] = df['Polarity'].apply(polaridad_fun)\n\n# Comprobamos que la transformación fue exitosa\ndf['Polarity'].unique()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idioma\n",
    "Nos quedamos con los tweets en español. Si no tuviésemos esa columna podríamos acudir a librerías como `langid` o `langdetect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filtramos para quedarnos solo con tweets en español\n# Aunque el corpus es español, puede haber tweets en otros idiomas\n\ndf = df[df['Lang'] == 'es']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos cuántos registros nos quedan después de los filtros aplicados\n# Empezamos con 7219 tweets, veamos cuántos tenemos ahora\ndf.shape"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Eliminamos tweets duplicados basándonos en la columna Content\n# subset='Content': solo consideramos el contenido para detectar duplicados\n# inplace=True: modificamos el DataFrame original sin crear una copia\n\ndf.drop_duplicates(subset = 'Content', inplace=True)\n\n# Vemos cuántos tweets eliminamos (de 5066 a ...)\ndf.shape"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signos de puntuación\n",
    "Eliminamos signos de puntuación: puntos, comas, interrogaciones, paréntesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizamos los primeros tweets antes de limpiar signos de puntuación\n# Observa que tienen puntos, comas, interrogaciones, @menciones, etc.\ndf['Content'].head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos el módulo de expresiones regulares (regex)\nimport re\n\n# Compilamos una expresión regular que captura todos los signos de puntuación y números\n# Los paréntesis crean grupos de captura para cada símbolo\n# \\d+ captura uno o más dígitos\n# Incluimos: . ; : ! ? ¿ @ , \" ( ) [ ] y números\n\nsignos = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\¿)|(\\@)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos de nuevo para tener todo junto en esta celda\nimport re\n\n# Compilamos la expresión regular con los signos de puntuación a eliminar\nsignos = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\¿)|(\\@)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n\n# Función que elimina signos de puntuación y convierte a minúsculas\ndef signs_tweets(tweet):\n    # signos.sub('', tweet): sustituye todos los signos encontrados por cadena vacía\n    # .lower(): convierte todo el texto a minúsculas\n    return signos.sub('', tweet.lower())\n\n# Aplicamos la función a toda la columna Content\ndf['Content'] = df['Content'].apply(signs_tweets)\n\n# Visualizamos el resultado: tweets limpios, sin signos y en minúsculas\ndf['Content'].head()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminamos links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Función para eliminar enlaces/URLs de los tweets\n# Los enlaces no aportan información de sentimiento y pueden ser ruido\n\ndef remove_links(df):\n    # Dividimos el tweet en palabras con split()\n    # Si una palabra contiene 'http', la reemplazamos por el token '{link}'\n    # join(): vuelve a unir las palabras en una cadena de texto\n    return \" \".join(['{link}' if ('http') in word else word for word in df.split()])\n\n# Aplicamos la función a toda la columna\ndf['Content'] = df['Content'].apply(remove_links)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Otros\n",
    "Podríamos hacer un preprocesado mucho más fino:\n",
    "1. Hashtags\n",
    "2. Menciones\n",
    "3. Abreviaturas\n",
    "4. Faltas de ortografía\n",
    "5. Risas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo\n",
    "Para montar el modelo tendremos que seguir los siguientes pasos\n",
    "1. Eliminamos las stopwords\n",
    "2. Aplicamos un stemmer, SnowBall por ejemplo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos la lista de stopwords (palabras vacías) en español desde NLTK\n# Stopwords son palabras muy comunes que no aportan significado (el, la, de, en, y, etc.)\n# Al eliminarlas, reducimos el ruido y mejoramos el rendimiento del modelo\n\nfrom nltk.corpus import stopwords\n\n# Cargamos el listado de stopwords en español\nspanish_stopwords = stopwords.words('spanish')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Función para eliminar stopwords de un texto\ndef remove_stopwords(df):\n    # Dividimos el texto en palabras con split()\n    # Filtramos: solo conservamos palabras que NO están en spanish_stopwords\n    # join(): reunimos las palabras filtradas en un texto\n    return \" \".join([word for word in df.split() if word not in spanish_stopwords])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aplicamos la eliminación de stopwords a todos los tweets\ndf['Content'] = df['Content'].apply(remove_stopwords)\n\n# Visualizamos cómo quedan los tweets sin palabras vacías\n# Notarás que desaparecen palabras como \"el\", \"la\", \"de\", \"en\", etc.\ndf.head()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos el stemmer de Snowball para español\n# Stemming: proceso de reducir palabras a su raíz o lexema\n# Ejemplo: \"corriendo\", \"corrió\", \"correr\" -> \"corr\"\n# Esto ayuda al modelo a entender que son variantes de la misma palabra\n\nfrom nltk.stem.snowball import SnowballStemmer\n\ndef spanish_stemmer(x):\n    # Creamos una instancia del stemmer para español\n    stemmer = SnowballStemmer('spanish')\n    \n    # Aplicamos el stemmer a cada palabra del texto\n    # stem(word): reduce la palabra a su raíz\n    return \" \".join([stemmer.stem(word) for word in x.split()])\n\n# Aplicamos stemming a todos los tweets\ndf['Content'] = df['Content'].apply(spanish_stemmer)\n\n# Vemos el resultado: palabras reducidas a su raíz\n# Ejemplo: \"pensando\" -> \"pens\", \"gracias\" -> \"graci\"\ndf['Content'].head()"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seleccionamos columnas\n",
    "Nos quedamos con las columnas que nos interesan para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Nos quedamos solo con las columnas necesarias para el modelo\n# Content: el texto del tweet (ya limpio y procesado)\n# Polarity: la etiqueta (0=positivo, 1=negativo)\n\ndf = df[['Content', 'Polarity']]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Guardamos el dataset procesado en un archivo CSV\n# Este archivo contiene los tweets limpios y listos para entrenar el modelo\n# Es buena práctica guardar los datos procesados para poder reutilizarlos\n\ndf.to_csv('data/output/data_processed.csv')"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos CountVectorizer para convertir texto en vectores numéricos\n# Los modelos de ML no entienden texto, necesitan números\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Creamos el vectorizador con n-gramas de 1 y 2 palabras\n# ngram_range=(1,2): captura palabras individuales (1-grama) y pares de palabras (2-gramas)\n# Ejemplo: \"muy bueno\" se captura como [\"muy\", \"bueno\", \"muy bueno\"]\n# Esto ayuda a captar contexto y expresiones compuestas\n\nvectorizer = CountVectorizer(ngram_range=(1,2))"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Montamos Pipeline\n",
    "Modelos que suelen funcionar bien con pocas observaciones y muchas features son la Regresión logística el LinearSVC o Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos Regresión Logística, un algoritmo de clasificación efectivo para texto\nfrom sklearn.linear_model import LogisticRegression\n\n# Creamos un Pipeline que encadena transformaciones y el modelo\n# Pipeline: ejecuta pasos secuenciales (vectorizar -> clasificar)\n# Ventaja: simplifica el código y evita errores de data leakage\n\npipeline = Pipeline([\n    ('vect', vectorizer),  # Paso 1: Convertir texto a vectores numéricos\n    ('cls', LogisticRegression(max_iter=10000))  # Paso 2: Clasificar con Regresión Logística\n    # max_iter=10000: número máximo de iteraciones para asegurar convergencia\n])\n\n# Definimos la grilla de hiperparámetros para probar diferentes combinaciones\nparameters = {\n    # max_features: número máximo de características (palabras/n-gramas) a considerar\n    'vect__max_features': (5000, 10000),\n    \n    # penalty: tipo de regularización (L2 penaliza coeficientes grandes)\n    \"cls__penalty\": [\"l2\"], \n    \n    # C: inverso de la fuerza de regularización (menor C = más regularización)\n    # Valores más altos = modelo más complejo, valores más bajos = modelo más simple\n    \"cls__C\": [0.1, 0.5, 1.0, 5.0]\n}\n\n# GridSearchCV: prueba todas las combinaciones de hiperparámetros\n# cv=5: validación cruzada con 5 folds (divide datos en 5 partes)\n# n_jobs=-1: usa todos los núcleos del CPU para acelerar el proceso\n# scoring='accuracy': métrica de evaluación (porcentaje de aciertos)\n\ngrid_search = GridSearchCV(pipeline,\n                          parameters,\n                          cv = 5,\n                          n_jobs = -1,\n                          scoring = 'accuracy')"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Entrenamos el modelo con todas las combinaciones de hiperparámetros\n# X: df['Content'] - el texto de los tweets (features)\n# y: df['Polarity'] - las etiquetas (0=positivo, 1=negativo)\n\n# Este proceso puede tardar varios minutos porque:\n# - Prueba 2 valores de max_features × 4 valores de C = 8 combinaciones\n# - Cada combinación se evalúa con validación cruzada de 5 folds\n# - Total: 8 × 5 = 40 entrenamientos\n\ngrid_search.fit(df['Content'], df['Polarity'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mostramos los resultados del entrenamiento\n\n# best_params_: la mejor combinación de hiperparámetros encontrada\nprint(\"Best params:\", grid_search.best_params_)\n\n# best_score_: la mejor precisión (accuracy) obtenida con validación cruzada\n# En este caso: ~76.9% de aciertos\nprint(\"Best acc:\", grid_search.best_score_)\n\n# best_estimator_: el pipeline completo con los mejores hiperparámetros\n# Este es el modelo final que usaremos para hacer predicciones\nprint(\"Best model:\", grid_search.best_estimator_)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Accedemos específicamente al clasificador (segundo paso del pipeline)\n# Esto nos permite ver los parámetros del modelo de Regresión Logística\n# 'cls' es el nombre que le dimos al clasificador en el pipeline\n\ngrid_search.best_estimator_['cls']"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Guardamos el modelo entrenado en un archivo usando pickle\n# pickle: serializa objetos de Python para poder guardarlos y cargarlos después\n\nimport pickle\n\n# Abrimos un archivo en modo escritura binaria ('wb')\nwith open('data/output/finished_model.model', \"wb\") as archivo_salida:\n    # dump(): guarda el mejor modelo (pipeline completo) en el archivo\n    # Incluye tanto el vectorizador como el clasificador entrenado\n    pickle.dump(grid_search.best_estimator_, archivo_salida)\n\n# Ahora podemos cargar este modelo en cualquier momento sin tener que reentrenarlo"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicciones\n",
    "Realizar una predicción con un tweet que escojas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos pickle para cargar el modelo guardado previamente\nimport pickle"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cargamos el modelo entrenado desde el archivo\n# 'rb': modo lectura binaria (read binary)\n\nwith open('data/output/finished_model.model', \"rb\") as archivo_entrada:\n    # load(): deserializa el objeto y lo carga en memoria\n    # pipeline_importada contendrá el pipeline completo (vectorizador + clasificador)\n    pipeline_importada = pickle.load(archivo_entrada)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizamos el pipeline cargado para confirmar que es el correcto\n# Debe mostrar CountVectorizer con max_features=10000 y LogisticRegression\npipeline_importada"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Redefinimos todas las funciones de preprocesamiento\n# Las necesitamos para limpiar nuevos tweets antes de hacer predicciones\n\n# Función para eliminar signos de puntuación y convertir a minúsculas\ndef signs_tweets(tweet):\n    return signos.sub('', tweet.lower())\n\n# Función para eliminar URLs\ndef remove_links(df):\n    return \" \".join(['{link}' if ('http') in word else word for word in df.split()])\n\n# Función para eliminar stopwords (palabras vacías)\ndef remove_stopwords(df):\n    return \" \".join([word for word in df.split() if word not in spanish_stopwords])\n\n# Función para aplicar stemming (reducir palabras a su raíz)\nfrom nltk.stem.snowball import SnowballStemmer\n\ndef spanish_stemmer(x):\n    stemmer = SnowballStemmer('spanish')\n    return \" \".join([stemmer.stem(word) for word in x.split()])"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leemos el pipeline con el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creamos un tweet de ejemplo para probar el modelo\n# Este tweet tiene sentimiento negativo (no estoy aprendiendo, malos profesores, etc.)\ntext = pd.Series('Bua, no estoy aprendiendo en el bootcamp. Qué malos profesores, me sobra tiempo')\n\n# Convertimos a DataFrame\ntest_clean = pd.DataFrame(text, columns=['content'])\n\n# Aplicamos todas las transformaciones en el orden correcto:\n\n# Paso 1: Eliminar signos de puntuación\ntest_clean['content_clean'] = test_clean['content'].apply(signs_tweets)\n\n# Paso 2: Eliminar enlaces (URLs)\ntest_clean['content_clean'] = test_clean['content_clean'].apply(remove_links)\n\n# Paso 3: Eliminar stopwords\ntest_clean['content_clean'] = test_clean['content_clean'].apply(remove_stopwords)\n\n# Paso 4: Aplicar stemming\ntest_clean['content_clean'] = test_clean['content_clean'].apply(spanish_stemmer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualizamos el tweet original y el preprocesado\n# Observa cómo el texto se ha simplificado pero mantiene las palabras clave\n# \"malos profesores\" -> \"mal profesor\" (después del preprocesamiento)\ntest_clean"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicciones de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extraemos el texto limpio que vamos a clasificar\n# Este es el input que pasaremos al modelo\ntest_clean['content_clean']"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hacemos la predicción con el modelo cargado\n# predict(): devuelve la clase predicha (0 o 1)\n# 0 = Sentimiento positivo\n# 1 = Sentimiento negativo\n\npredictions = pipeline_importada.predict(test_clean['content_clean'])\n\n# Añadimos la predicción como una nueva columna\ntest_clean['Polarity'] = pd.Series(predictions)\n\n# Visualizamos el resultado: debería predecir 1 (negativo) para este tweet\ntest_clean"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Obtenemos las probabilidades de cada clase\n# predict_proba(): devuelve un array con [prob_clase_0, prob_clase_1]\n# Esto nos da más información que solo la clase predicha\n\npredictions = pipeline_importada.predict_proba(test_clean['content_clean'])\n\n# Extraemos las probabilidades:\n# predictions[0][0]: probabilidad de ser positivo (clase 0)\n# predictions[0][1]: probabilidad de ser negativo (clase 1)\n\ntest_clean['Polarity_Pos'] = pd.Series(predictions[0][0])  # ~43% positivo\ntest_clean['Polarity_Neg'] = pd.Series(predictions[0][1])  # ~57% negativo\n\n# Visualizamos el resultado completo\n# El modelo está 57% seguro de que es negativo y 43% de que es positivo\n# Como 0.57 > 0.43, clasifica como negativo (clase 1)\ntest_clean"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}