{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5Iw2deeevLb"
   },
   "source": "# Análisis de Sentimientos IMDB\n\nLos datos están divididos equitativamente con 25k reseñas destinadas para entrenamiento y 25k para probar tu clasificador. Además, cada conjunto tiene 12.5k reseñas positivas y 12.5k negativas.\n\nIMDb permite a los usuarios calificar películas en una escala del 1 al 10. Para etiquetar estas reseñas, el curador de los datos etiquetó cualquier cosa con ≤ 4 estrellas como negativa y cualquier cosa con ≥ 7 estrellas como positiva. Las reseñas con 5 o 6 estrellas fueron excluidas.\n\n**Importar las librerías necesarias**"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-jABp8j_evLc"
   },
   "outputs": [],
   "source": [
    "# En Google Colab !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqh7AvdNevLd"
   },
   "outputs": [],
   "source": "# =============================================================================\n# IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n# =============================================================================\n# numpy: para operaciones numéricas y manejo de arrays\n# pandas: para manipulación de datos estructurados (aunque no se usa mucho aquí)\n# os: para operaciones con el sistema operativo (rutas de archivos)\n# re: para expresiones regulares (limpieza de texto)\n# warnings: para suprimir mensajes de advertencia que no son críticos\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")  # Ignoramos warnings para tener output más limpio"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hxr29u8evLd"
   },
   "source": "**Cargar Datos**"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VAkwzdf8evLd"
   },
   "outputs": [],
   "source": [
    "#reviews_train = []\n",
    "#for line in open(os.getcwd() + '/data/imbd_train.txt', 'r', encoding='latin1'):\n",
    "\n",
    "#    reviews_train.append(line.strip())\n",
    "\n",
    "#reviews_test = []\n",
    "#for line in open(os.getcwd() + '/data/imbd_test.txt', 'r', encoding='latin1'):\n",
    "\n",
    "#    reviews_test.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zg6qXrtofJfX"
   },
   "outputs": [],
   "source": "# =============================================================================\n# CARGA DE DATOS - REVIEWS DE PELÍCULAS\n# =============================================================================\n# Cargamos las reviews de entrenamiento desde el archivo de texto\n# Cada línea del archivo es una review completa de una película\n\nreviews_train = []  # Lista para almacenar las reviews de entrenamiento\nfor line in open('imbd_train.txt', 'r', encoding='latin1'):\n    # strip() elimina espacios en blanco y saltos de línea al inicio y final\n    reviews_train.append(line.strip())\n\n# Cargamos las reviews de test (para evaluar el modelo después)\nreviews_test = []  # Lista para almacenar las reviews de test\nfor line in open('imbd_test.txt', 'r', encoding='latin1'):\n    reviews_test.append(line.strip())\n\n# Dataset: 25,000 reviews de entrenamiento y 25,000 de test\n# Train: primeras 12,500 son positivas, últimas 12,500 son negativas\n# Test: misma estructura"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWd7RlalevLe",
    "outputId": "54bd3d88-952d-421a-dad4-8a7101a4f7d1"
   },
   "outputs": [],
   "source": "# =============================================================================\n# EXPLORACIÓN INICIAL - Visualizar algunas reviews\n# =============================================================================\n# Imprimimos las primeras 3 reviews para ver cómo se ven los datos originales\n# Esto nos ayuda a identificar qué tipo de limpieza necesitaremos después\n\nfor i in range(3):\n    print('####################')\n    print(reviews_train[i])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f2owbIgevLe"
   },
   "source": "**Ver uno de los elementos de la lista**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "SX9w-VB6evLe",
    "outputId": "ea009ded-e73c-4053-80b5-e9d1b2c81eb6"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VERIFICAR TAMAÑO DEL DATASET Y VER UN EJEMPLO\n# =============================================================================\n# Comprobamos cuántas reviews tenemos en cada conjunto\n\nprint(len(reviews_train))  # Debería ser 25,000\nprint(len(reviews_test))   # Debería ser 25,000\n\n# Mostramos una review específica (la número 5) para ver su contenido\nreviews_train[5]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKmnrBdcevLe"
   },
   "source": "El texto sin procesar está bastante desordenado para estas reseñas, así que antes de poder hacer cualquier análisis necesitamos limpiar las cosas\n\n\n**Usar expresiones regulares para eliminar los caracteres que no son texto y las etiquetas html**"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzC4vDehevLf"
   },
   "outputs": [],
   "source": "# =============================================================================\n# LIMPIEZA Y PREPROCESAMIENTO DEL TEXTO\n# =============================================================================\n# Las reviews tienen ruido: puntuación, etiquetas HTML, números, etc.\n# Necesitamos limpiar el texto para que el modelo se enfoque en las palabras importantes\n\nimport re\n\n# PASO 1: Definir patrones de expresiones regulares para limpieza\n\n# REPLACE_NO_SPACE: elimina caracteres que NO queremos (los sustituye por cadena vacía)\n# Incluye: puntos, punto y coma, dos puntos, exclamaciones, interrogaciones, comas,\n#          comillas, paréntesis, corchetes y números\nREPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n\n# REPLACE_WITH_SPACE: caracteres que queremos sustituir por espacios\n# Incluye: etiquetas HTML como <br/><br/>, guiones y barras\n# Los sustituimos por espacio para no juntar palabras\nREPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n\n# Constantes para las sustituciones\nNO_SPACE = \"\"\nSPACE = \" \"\n\n# PASO 2: Función de preprocesamiento\ndef preprocess_reviews(reviews):\n    \"\"\"\n    Limpia una lista de reviews de texto:\n    1. Convierte todo a minúsculas (para normalizar)\n    2. Elimina signos de puntuación y números\n    3. Sustituye etiquetas HTML y guiones por espacios\n    \n    Args:\n        reviews: lista de strings con las reviews originales\n    \n    Returns:\n        reviews_clean: lista de strings con las reviews limpias\n    \"\"\"\n    # Primera pasada: eliminar signos de puntuación y números\n    # .lower() convierte todo a minúsculas\n    # .sub() sustituye lo que coincida con el patrón\n    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n    \n    # Segunda pasada: sustituir etiquetas HTML y guiones por espacios\n    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n    \n    return reviews\n\n# PASO 3: Aplicar la limpieza a nuestros datos\nreviews_train_clean = preprocess_reviews(reviews_train)\nreviews_test_clean = preprocess_reviews(reviews_test)\n\n# Ahora las reviews están limpias y listas para vectorización"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "azqKzkZ5evLf",
    "outputId": "bb874673-7bf5-4f30-96af-7a3a8ab7d479"
   },
   "outputs": [],
   "source": "# =============================================================================\n# COMPARAR: Review antes y después de la limpieza\n# =============================================================================\n# Mostramos la misma review (índice 5) después de la limpieza\n# Compara con la versión original que vimos antes\n# Observa: texto en minúsculas, sin puntuación, sin números, sin HTML\n\nreviews_train_clean[5]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXp9MhOAevLf"
   },
   "source": "# Vectorización\nPara que estos datos tengan sentido para nuestro algoritmo de aprendizaje automático necesitaremos convertir cada reseña a una representación numérica, que llamamos vectorización.\n\nLa forma más simple de esto es crear una matriz muy grande con una columna para cada palabra única en tu corpus (donde el corpus son todas las 50k reseñas en nuestro caso). Luego transformamos cada reseña en una fila que contiene 0s y 1s, donde 1 significa que la palabra del corpus correspondiente a esa columna aparece en esa reseña. Dicho esto, cada fila de la matriz será muy dispersa (mayormente ceros). Este proceso también se conoce como codificación one-hot. Usar el método *CountVectorizer*."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUzFlZrbevLf"
   },
   "outputs": [],
   "source": "# =============================================================================\n# IMPORTAR CountVectorizer\n# =============================================================================\n# CountVectorizer es la herramienta de sklearn que convierte texto en vectores numéricos\n# Es necesario porque los modelos de ML solo entienden números, no palabras\n\nfrom sklearn.feature_extraction.text import CountVectorizer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuNDAV6EevLg"
   },
   "outputs": [],
   "source": "# =============================================================================\n# EJEMPLO SIMPLE - Corpus de demostración\n# =============================================================================\n# Creamos un corpus pequeño de 4 documentos para entender cómo funciona la vectorización\n# Este ejemplo nos ayudará a visualizar el proceso antes de aplicarlo a nuestras 25,000 reviews\n\ncorpus = [\n     'This is the first document.',      # Documento 0\n     'This document is the second document.',  # Documento 1\n     'And this is the third one.',       # Documento 2\n     'Is this the first document?',      # Documento 3\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICAK7Sd6evLg",
    "outputId": "142e04bf-0431-4a3c-d838-95b29e879983"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VECTORIZACIÓN - Ejemplo básico\n# =============================================================================\n# Proceso de vectorización:\n# 1. El vectorizador identifica todas las palabras únicas del corpus (vocabulario)\n# 2. Cada palabra única se convierte en una \"feature\" (columna)\n# 3. El vectorizador crea una matriz donde cada fila es un documento\n\nvectorizer = CountVectorizer()  # Creamos el vectorizador\nX = vectorizer.fit_transform(corpus)  # fit: aprende el vocabulario, transform: convierte a matriz\n\n# Veamos qué palabras encontró (el vocabulario, ordenado alfabéticamente)\nvectorizer.get_feature_names_out()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoKfXgpNevLg",
    "outputId": "597c1bfe-1e7a-4010-b5d4-f1e3e93d277e"
   },
   "outputs": [],
   "source": "# =============================================================================\n# TAMAÑO DEL VOCABULARIO\n# =============================================================================\n# Contamos cuántas palabras únicas hay en nuestro pequeño corpus\n# En este caso son 9 palabras diferentes\n\nlen(vectorizer.get_feature_names_out())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "rv2g2_lAevLg",
    "outputId": "69f1bca3-3d0e-46af-c178-ef13adbdc802"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VISUALIZAR LA MATRIZ DE VECTORIZACIÓN\n# =============================================================================\n# Convertimos la matriz dispersa (sparse matrix) a DataFrame para visualizarla mejor\n# Cada fila = un documento\n# Cada columna = una palabra del vocabulario\n# Cada celda = número de veces que la palabra aparece en ese documento\n\n# Ejemplo de lectura:\n# - Fila 0 (documento 0): 'document' aparece 1 vez, 'first' aparece 1 vez, 'is' aparece 1 vez...\n# - Fila 1 (documento 1): 'document' aparece 2 veces (¡fíjate que cuenta!)\n\npd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlWCFx4uevLg"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VECTORIZACIÓN BINARIA - Para nuestras reviews de películas\n# =============================================================================\n# Ahora aplicamos CountVectorizer a nuestras reviews reales con binary=True\n# \n# ¿Por qué binary=True?\n# - En lugar de contar cuántas veces aparece cada palabra (1, 2, 3, ...)\n# - Solo marcamos si la palabra está presente (1) o ausente (0)\n# - Esto suele funcionar mejor para análisis de sentimiento\n# - Evita que palabras muy repetidas dominen el modelo\n\nbaseline_vectorizer = CountVectorizer(binary=True)\n\n# fit(): el vectorizador \"aprende\" todas las palabras únicas de las 25,000 reviews de train\nbaseline_vectorizer.fit(reviews_train_clean)\n\n# transform(): convierte cada review en un vector numérico\n# ¡IMPORTANTE! Aplicamos el MISMO vectorizador a test (mismo vocabulario, mismas columnas)\nX_baseline = baseline_vectorizer.transform(reviews_train_clean)\nX_test_baseline = baseline_vectorizer.transform(reviews_test_clean)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqX_V3DeevLh",
    "outputId": "7dca4698-1dff-49c1-a421-26d3d19bb7f7"
   },
   "outputs": [],
   "source": "# =============================================================================\n# INSPECCIONAR LA MATRIZ RESULTANTE\n# =============================================================================\n# X_baseline es una matriz dispersa (sparse matrix) porque tiene muchos ceros\n# Dimensiones: (25000 filas, 87063 columnas)\n# - 25,000 filas = 25,000 reviews\n# - 87,063 columnas = 87,063 palabras únicas encontradas en el corpus\n# - 3,410,713 elementos almacenados = celdas con valor 1 (el resto son 0)\n#\n# ¿Por qué sparse matrix?\n# - Si guardáramos todos los valores (incluyendo ceros) ocuparía muchísima memoria\n# - Sparse solo guarda las posiciones con 1, ahorrando espacio\n\nX_baseline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uNXCZeyRevLh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7098c18f-b38f-4118-c79f-3a4df552507b"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VOCABULARIO DEL VECTORIZADOR\n# =============================================================================\n# Mostramos el tamaño de la matriz: (25000 reviews, 87063 palabras únicas)\nprint(X_baseline.shape)\n\n# El atributo vocabulary_ es un diccionario donde:\n# - Clave: la palabra\n# - Valor: el índice/columna que le corresponde en la matriz\n# Ejemplo: {'good': 35421, 'bad': 8792, ...}\n# Esto significa que la columna 35421 representa la palabra 'good'\n\nbaseline_vectorizer.vocabulary_"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GXBlhGHevLh"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VECTORIZACIÓN CON CONTEO (no binaria)\n# =============================================================================\n# Ahora probamos SIN binary=True para ver la diferencia\n# En este caso, la matriz contendrá el NÚMERO DE VECES que aparece cada palabra\n# (no solo 0 o 1, sino 0, 1, 2, 3, 4, ...)\n\nvectorizer_c = CountVectorizer()  # Sin binary=True\nvectorizer_c.fit(reviews_train_clean)\n\n# Esta matriz contendrá conteos reales\nX_baseline_c = vectorizer_c.transform(reviews_train_clean)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzA1CNzAevLh",
    "outputId": "d0eb0ee8-23ab-462e-fc25-b7c37710c6f3"
   },
   "outputs": [],
   "source": "# =============================================================================\n# COMPARACIÓN: Conteo vs Binario\n# =============================================================================\n# Dimensiones: (25000, 87063) - ¡igual que antes!\n# El número de palabras únicas (columnas) es el mismo\n# Lo que cambia son los VALORES dentro de la matriz\n\nprint(X_baseline_c.shape)\nprint(len(vectorizer_c.get_feature_names_out()))  # Las mismas 87,063 palabras\n\n# Si descomentamos la siguiente línea, veríamos la matriz completa\n# pero es ENORME (25000 x 87063 = 2,176,575,000 celdas!)\n# X_baseline_c.toarray()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TNlncpIevLh",
    "outputId": "f0de4e2f-041a-411a-8994-6ba2b4240553"
   },
   "outputs": [],
   "source": "# =============================================================================\n# REPRESENTACIÓN DE LA MATRIZ SPARSE\n# =============================================================================\n# Matriz demasiado grande como para que numpy la imprima completa por pantalla\n# Python nos muestra el formato comprimido (Compressed Sparse Row)\n# 3,410,713 elementos almacenados de 2+ mil millones de posiciones posibles\n\nX_baseline_c"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xykEdi7AevLi"
   },
   "source": "# Entrenar un Modelo Base\n\nEntrenar un modelo de Regresión Logística después de transformar los datos con CountVectorizer\n\n* Son fáciles de interpretar\n* Los modelos lineales tienden a funcionar bien en conjuntos de datos dispersos como este\n* Aprenden muy rápido en comparación con otros algoritmos.\n\nProbar modelos con valores de C de [0.01, 0.05, 0.25, 0.5, 1] y ver cuál es el mejor valor para C, y calcular la precisión"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHIRZAegevLi"
   },
   "outputs": [],
   "source": "# =============================================================================\n# MODELO BASELINE - Regresión Logística con Grid Search\n# =============================================================================\n# Vamos a entrenar nuestro primer modelo de clasificación para predecir el sentimiento\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# PASO 1: Crear las etiquetas (targets)\n# Las reviews están ordenadas: primeras 12,500 son positivas (1), últimas 12,500 negativas (0)\ntarget = [1 if i < 12500 else 0 for i in range(25000)]\n\n# Equivalente más explícito (comentado):\n# target = []\n# for i in range(25000):\n#     if i < 12500:\n#         target.append(1)  # Positivo\n#     else:\n#         target.append(0)  # Negativo\n\n# PASO 2: Función para entrenar el modelo con validación cruzada\ndef train_model(X_TRAIN, X_TEST):\n    \"\"\"\n    Entrena un modelo de Regresión Logística con Grid Search\n    \n    Grid Search:\n    - Prueba diferentes valores del hiperparámetro C\n    - C controla la regularización (penalización por complejidad)\n    - C pequeño = más regularización = modelo más simple\n    - C grande = menos regularización = modelo más complejo\n    \n    Cross-Validation (cv=5):\n    - Divide los datos de entrenamiento en 5 partes\n    - Entrena 5 veces, cada vez usando 4 partes para entrenar y 1 para validar\n    - Esto nos da una estimación más robusta del rendimiento\n    \n    Args:\n        X_TRAIN: matriz de features de entrenamiento\n        X_TEST: matriz de features de test\n    \"\"\"\n    \n    lr = LogisticRegression()  # Creamos el modelo\n    \n    # Hiperparámetros a probar\n    params = {\n        'C': [0.01, 0.05, 0.25, 0.5, 1]\n    }\n    \n    # GridSearchCV prueba todas las combinaciones y elige la mejor\n    grid = GridSearchCV(lr, params, cv=5)\n    grid.fit(X_TRAIN, target)\n    \n    # Evaluamos el mejor modelo encontrado en el conjunto de test\n    print(\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIzphl1VevLi",
    "outputId": "e1739366-946d-4d8a-c619-0ef05de257af"
   },
   "outputs": [],
   "source": "# =============================================================================\n# ENTRENAR Y EVALUAR EL MODELO BASELINE\n# =============================================================================\n# Entrenamos el modelo con las matrices vectorizadas (binarias)\n# Este es nuestro BASELINE - el modelo más simple contra el que compararemos mejoras\n\n# X_baseline: reviews de train vectorizadas\n# X_test_baseline: reviews de test vectorizadas\n\ntrain_model(X_baseline, X_test_baseline)\n\n# Resultado: ~88.18% de accuracy\n# Esto significa que el modelo clasifica correctamente el 88.18% de las reviews\n# ¡No está mal para un primer intento!"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1ekCzxqevLi"
   },
   "source": "# Eliminar Stop Words\n\nLas stop words son palabras muy comunes como 'if', 'but', 'we', 'he', 'she' y 'they'. Normalmente podemos eliminar estas palabras sin cambiar la semántica de un texto y hacerlo a menudo (pero no siempre) mejora el rendimiento de un modelo. Eliminar estas stop words se vuelve mucho más útil cuando comenzamos a usar secuencias de palabras más largas como características del modelo (ver n-gramas más adelante).\n\nAntes de aplicar el CountVectorizer, eliminemos las stopwords incluidas en nltk.corpus\n\nLuego aplicar el CountVectorizer, entrenar el modelo de Regresión Logística y obtener la precisión."
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "TL4DICY8evLi"
   },
   "outputs": [],
   "source": [
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ypZ7jZL5evLi",
    "outputId": "14312258-8198-46e7-adfd-fc44cf066c3b"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VERIFICAR VERSIÓN DE NUMPY\n# =============================================================================\n# Comprobamos la versión de numpy instalada\n# Nota: versiones antiguas pueden tener compatibilidad diferente con otras librerías\n\nnp.__version__\n# Versión antigua era 1.26.4, ahora está actualizada a 2.2.6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVNGDMgmevLj",
    "outputId": "c4db9c34-ab9b-4da0-b9e3-fd589bb5dbf3"
   },
   "outputs": [],
   "source": "# =============================================================================\n# DESCARGAR STOPWORDS DE NLTK\n# =============================================================================\n# NLTK (Natural Language Toolkit) es una librería muy popular para NLP\n# Necesitamos descargar el dataset de stopwords la primera vez que lo usamos\n\nimport nltk\nnltk.download('stopwords')  # Descarga las stopwords en múltiples idiomas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eZY5oiUevLj",
    "outputId": "2dbf6a67-5c1d-4267-efcb-95b9a53cb8c5"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VISUALIZAR STOPWORDS EN INGLÉS\n# =============================================================================\n# ¿Qué son las stopwords?\n# Son palabras muy comunes que aparecen en casi todos los textos\n# Ejemplos: 'the', 'a', 'is', 'in', 'of', 'and', etc.\n# \n# ¿Por qué eliminarlas?\n# - No aportan mucho significado al sentimiento\n# - Reducen el ruido en el modelo\n# - Disminuyen el tamaño del vocabulario (menos features)\n\nfrom nltk.corpus import stopwords\n\n# Mostramos las primeras 20 stopwords en inglés\nstopwords.words('english')[:20]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qi-otvE8evLk",
    "outputId": "3d2ece59-afd9-476e-e8e9-91f1a41ba6da"
   },
   "outputs": [],
   "source": "# =============================================================================\n# CONTAR STOPWORDS EN INGLÉS\n# =============================================================================\n# NLTK tiene 198 stopwords predefinidas para inglés\n\nlen(stopwords.words('english'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C35F9bnZevLk",
    "outputId": "ae0255a4-f8d0-4878-ac1c-8600a4cf773c"
   },
   "outputs": [],
   "source": "# =============================================================================\n# STOPWORDS EN OTROS IDIOMAS - Ejemplo en español\n# =============================================================================\n# NLTK incluye stopwords para múltiples idiomas\n# Aquí vemos las primeras 20 en español\n\nstopwords.words('spanish')[:20]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNCdFEbpevLl",
    "outputId": "44c163a0-2323-4bcd-aa84-44016a530835"
   },
   "outputs": [],
   "source": "# =============================================================================\n# CANTIDAD DE STOPWORDS EN ESPAÑOL\n# =============================================================================\n# El español tiene 313 stopwords en NLTK (más que inglés)\n# Esto depende de la complejidad morfológica de cada idioma\n\nlen(stopwords.words('spanish'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2r-9RpaevLl",
    "outputId": "b4a3ba5e-a249-4f3d-ad1e-4862f94c2b5d"
   },
   "outputs": [],
   "source": "# =============================================================================\n# STOPWORDS EN CHINO\n# =============================================================================\n# Ejemplo de stopwords en chino\n# Los idiomas asiáticos tienen sistemas de escritura muy diferentes\n\nstopwords.words('chinese')[:10]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TV9MALqXevLl",
    "outputId": "3d72da20-3f5c-4915-de68-b4e4aba1c9d9"
   },
   "outputs": [],
   "source": "# =============================================================================\n# CANTIDAD DE STOPWORDS EN CHINO\n# =============================================================================\n# El chino tiene 841 stopwords - ¡mucho más que inglés o español!\n\nlen(stopwords.words('chinese'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQdlB-iqevLl",
    "outputId": "b6efe901-2c2a-4d7f-c8d5-1b631f3f8325"
   },
   "outputs": [],
   "source": "# =============================================================================\n# STOPWORDS EN EUSKERA (VASCO)\n# =============================================================================\n# NLTK incluye hasta idiomas menos comunes como el euskera\n# Primeras 10 stopwords en vasco\n\nstopwords.words('basque')[:10]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKy0QojJevLm",
    "outputId": "3bca5fab-406e-4014-f7d0-db887d851029"
   },
   "outputs": [],
   "source": "# =============================================================================\n# CANTIDAD DE STOPWORDS EN EUSKERA\n# =============================================================================\n# El euskera tiene 326 stopwords en NLTK\n\nlen(stopwords.words('basque'))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xdbuIRHevLw"
   },
   "outputs": [],
   "source": "# =============================================================================\n# MÉTODO 1: ELIMINAR STOPWORDS MANUALMENTE (antes de vectorizar)\n# =============================================================================\n# Este método elimina stopwords aplicando un filtro sobre las reviews\n# Es el enfoque \"manual\" antes de usar CountVectorizer\n\nfrom nltk.corpus import stopwords\n\n# Cargamos la lista de stopwords en inglés\nenglish_stop_words = stopwords.words('english')\n\ndef remove_stop_words(corpus):\n    \"\"\"\n    Elimina stopwords de cada review en el corpus\n    \n    Proceso:\n    1. Para cada review, la dividimos en palabras (split())\n    2. Filtramos las palabras que NO están en la lista de stopwords\n    3. Unimos las palabras que quedan de nuevo en un string\n    \n    Args:\n        corpus: lista de reviews (strings)\n    \n    Returns:\n        removed_stop_words: lista de reviews sin stopwords\n    \"\"\"\n    removed_stop_words = []\n    for review in corpus:\n        # List comprehension que:\n        # - Divide la review en palabras (review.split())\n        # - Convierte cada palabra a minúsculas\n        # - Solo incluye palabras que NO están en english_stop_words\n        # - Une todo con espacios (' '.join())\n        removed_stop_words.append(\n            ' '.join([word.lower() for word in review.split() \n                      if word.lower() not in english_stop_words])\n        )\n    \n    return removed_stop_words\n\n# Aplicamos la eliminación de stopwords ANTES de vectorizar\nno_stop_words_train = remove_stop_words(reviews_train_clean)\nno_stop_words_test = remove_stop_words(reviews_test_clean)\n\n# NOTA: Este método es más \"manual\" y menos eficiente que usar el parámetro\n# stop_words del CountVectorizer (ver más abajo)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmryB0ZKevLw"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VECTORIZAR DESPUÉS DE ELIMINAR STOPWORDS (método manual)\n# =============================================================================\n# Ahora vectorizamos las reviews que ya tienen las stopwords eliminadas\n# \n# Documentación de CountVectorizer:\n# - lowercase=True: convierte todo a minúsculas antes de vectorizar (por defecto)\n# - stop_words: permite pasar una lista de stopwords (¡mejor método, ver abajo!)\n\ncv = CountVectorizer(binary=True)\ncv.fit(no_stop_words_train)\n\nX = cv.transform(no_stop_words_train)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mv6LdF7VevLw",
    "outputId": "0808841a-0fcb-4056-9bf0-3f958184f929"
   },
   "outputs": [],
   "source": "# =============================================================================\n# COMPROBAR DIMENSIONES\n# =============================================================================\n# Verificamos el tamaño de la matriz después de eliminar stopwords\n# Esperamos menos columnas (palabras) que antes\n\nprint(X.shape)  # Debería ser (25000, algo menos que 87063)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-r-4k07evLw",
    "outputId": "e7569d70-4b97-4807-bc3b-35548d019781"
   },
   "outputs": [],
   "source": "# =============================================================================\n# ENTRENAR MODELO CON STOPWORDS ELIMINADAS (método manual)\n# =============================================================================\n# Aplicamos la misma transformación a test y entrenamos el modelo\n\nX_test = cv.transform(no_stop_words_test)\n\n# Evaluamos: ¿mejora o empeora el accuracy?\ntrain_model(X, X_test)\n\n# Resultado: ~87.9% - ¡ligeramente peor que el baseline!\n# Esto puede pasar: no siempre eliminar stopwords mejora el modelo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf-LODZ5evLw",
    "outputId": "cca91271-145f-4d2b-c303-9c97b451ae34"
   },
   "outputs": [],
   "source": "# =============================================================================\n# COMPARACIÓN: ¿Cuántas palabras eliminamos?\n# =============================================================================\n# Comparamos el número de features (columnas) antes y después\n\nprint(X_baseline.shape)  # Baseline: 87,063 palabras\nprint(X.shape)           # Con stopwords eliminadas: 87,046 palabras\nprint(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])\n\n# ¡Solo eliminó 17 palabras! ¿Por qué tan pocas?\n# Porque nuestra función remove_stop_words() hace un split() simple\n# que no tokeniza tan bien como CountVectorizer\n# Por ejemplo: \"it's\" no se separa correctamente en \"it\" y \"'s\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7L0mbQwMevLx"
   },
   "outputs": [],
   "source": "# =============================================================================\n# MÉTODO 2: ELIMINAR STOPWORDS CON CountVectorizer (MEJOR MÉTODO)\n# =============================================================================\n# Este es el método RECOMENDADO: pasar stop_words directamente al vectorizador\n# \n# Ventajas:\n# - Más simple (menos código)\n# - Más eficiente\n# - Mejor tokenización (CountVectorizer es más inteligente)\n# - Elimina más stopwords correctamente\n\ncv = CountVectorizer(binary=True,\n                     stop_words=english_stop_words)  # ¡Solo añadimos este parámetro!\n\n# Aplicamos sobre las reviews originales (limpias, pero sin eliminar stopwords manualmente)\ncv.fit(reviews_train_clean)\n\nX = cv.transform(reviews_train_clean)\nX_test = cv.transform(reviews_test_clean)\n\n# train_model(X, X_test)  # Comentado para no ejecutar ahora"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1jSkT8XevLx",
    "outputId": "14e2a606-3965-4870-832c-c88e701080e0"
   },
   "outputs": [],
   "source": "# =============================================================================\n# COMPARACIÓN: Método manual vs CountVectorizer\n# =============================================================================\n# Comparamos cuántas stopwords eliminó cada método\n\nprint(X_baseline.shape)  # Baseline: 87,063 palabras\nprint(X.shape)           # Con CountVectorizer stop_words: 86,918 palabras\nprint(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])\n\n# ¡Ahora sí! Eliminó 145 palabras\n# Mucho mejor que las 17 del método manual\n# \n# ¿Por qué?\n# CountVectorizer tokeniza mejor (separa \"it's\" en \"it\" y \"'s\")\n# Luego elimina todas las stopwords correctamente"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgRiJVWDevLx"
   },
   "source": "**Nota:** En la práctica, una forma más fácil de eliminar stop words es simplemente usar el argumento stop_words con cualquiera de las clases 'Vectorizer' de scikit-learn. Si quieres usar la lista completa de stop words de NLTK puedes hacer stop_words='english'. En la práctica he encontrado que usar la lista de NLTK en realidad disminuye mi rendimiento porque es demasiado expansiva, así que normalmente proporciono mi propia lista de palabras. Por ejemplo, stop_words=['in','of','at','a','the']."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L54gvI6evLx"
   },
   "source": "Un siguiente paso común en el preprocesamiento de texto es normalizar las palabras en tu corpus intentando convertir todas las diferentes formas de una palabra dada en una. Dos métodos que existen para esto son Stemming y Lemmatization.\n\n# Stemming\n\nStemming se considera el enfoque más crudo/de fuerza bruta para la normalización (aunque esto no necesariamente significa que tendrá peor rendimiento). Hay varios algoritmos, pero en general todos usan reglas básicas para cortar los finales de las palabras.\n\nNLTK tiene varias implementaciones de algoritmos de stemming. Usaremos el Porter stemmer. Los más usados:\n* PorterStemmer\n* SnowballStemmer\n\nAplicar un PorterStemmer, vectorizar y entrenar el modelo nuevamente"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSG7ZITVevLx",
    "outputId": "e23a555b-dbac-4a4c-b82f-c51d20813046"
   },
   "outputs": [],
   "source": "# =============================================================================\n# STEMMING - Reducción de palabras a su raíz (método \"bruto\")\n# =============================================================================\n# ¿Qué es stemming?\n# Es un proceso que recorta las palabras eliminando sufijos para obtener su \"raíz\"\n# Es un método rápido pero \"bruto\" (no siempre linguísticamente correcto)\n#\n# Ejemplos:\n# - \"running\", \"runs\", \"ran\" → \"run\"\n# - \"cats\", \"catty\" → \"cat\"\n# - \"flies\", \"flying\" → \"fli\"\n#\n# Algoritmos de stemming:\n# - PorterStemmer: el más común, desarrollado por Martin Porter en 1980\n# - SnowballStemmer: una mejora del PorterStemmer\n# - LancasterStemmer: más agresivo (recorta más)\n\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\n\n# Lista de palabras de ejemplo en diferentes formas\nplurals = ['caresses', 'flies', 'fly','flight', 'flown', 'dies', 'die', 'mules', 'denied', 'deny',\n            'died', 'agreed','agree', 'owned', 'humbled', 'sized',\n            'meeting', 'stating', 'siezing', 'itemization',\n            'sensational', 'traditional', 'reference', 'colonizer', 'colonizing',\n            'plotted']\n\n# Aplicamos stemming a cada palabra\nsingles = [stemmer.stem(plural) for plural in plurals]\n\n# Observa cómo se recortan:\n# - 'flies' y 'fly' → 'fli' (pierde sentido)\n# - 'agreed' y 'agree' → 'agre'\n# - 'sensational' → 'sensat'\nprint(' '.join(singles))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYt6sEfqevLx",
    "outputId": "91accf71-55e6-4b07-fb72-df332115cdb5"
   },
   "outputs": [],
   "source": "# =============================================================================\n# SNOWBALL STEMMER - Versión mejorada del Porter Stemmer\n# =============================================================================\n# SnowballStemmer es una evolución del PorterStemmer\n# Ventaja: soporta múltiples idiomas (15 idiomas diferentes)\n# Sintaxis: SnowballStemmer('idioma')\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')  # Especificamos el idioma\n\nplurals = ['caresses', 'flies', 'fly','flight', 'dies', 'mules', 'denied', 'deny',\n            'died', 'agreed', 'owned', 'humbled', 'sized',\n            'meeting', 'stating', 'siezing', 'itemization',\n            'sensational', 'traditional', 'reference', 'colonizer',\n            'plotted']\n\nsingles = [stemmer.stem(plural) for plural in plurals]\n\n# Los resultados son muy similares al PorterStemmer\n# Pero Snowball suele ser ligeramente más preciso\nprint(' '.join(singles))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VibI7GZMevLy",
    "outputId": "5b25a4eb-be01-4a2f-f28d-8e924051dc4c"
   },
   "outputs": [],
   "source": "# =============================================================================\n# SNOWBALL STEMMER EN ESPAÑOL\n# =============================================================================\n# Ejemplo de stemming en español\n# Es importante porque cada idioma tiene reglas morfológicas diferentes\n\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('spanish')  # Cambiamos el idioma\n\nplurals = ['recorrer', 'corriendo', 'correlación', 'correré', \n           'casas', 'casero', 'caso', 'playa', \n           'volando', 'volar', 'volveré']\n\nsingles = [stemmer.stem(plural) for plural in plurals]\n\n# Observa los resultados:\n# - 'recorrer', 'corriendo', 'correré' → 'recorr', 'corr', 'corr'\n# - 'casas', 'casero', 'caso' → 'cas', 'caser', 'cas'\n# - 'volando', 'volar', 'volveré' → 'vol', 'vol', 'volv'\nprint(' '.join(singles))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhCpJ9O3evLy"
   },
   "outputs": [],
   "source": "# =============================================================================\n# APLICAR STEMMING A NUESTRAS REVIEWS\n# =============================================================================\n# Ahora aplicamos stemming a todas las reviews de películas\n# Objetivo: reducir el vocabulario agrupando palabras similares\n\nfrom nltk.stem.porter import PorterStemmer\n\ndef get_stemmed_text(corpus):\n    \"\"\"\n    Aplica stemming a cada palabra de cada review\n    \n    Proceso:\n    1. Para cada review, dividirla en palabras\n    2. Aplicar stemming a cada palabra\n    3. Unir las palabras procesadas de nuevo en un string\n    \n    Args:\n        corpus: lista de reviews\n    \n    Returns:\n        lista de reviews con palabras \"stemmed\"\n    \"\"\"\n    stemmer = PorterStemmer()\n    \n    # Para cada review:\n    # - Dividir en palabras (.split())\n    # - Aplicar stemming a cada palabra (stemmer.stem())\n    # - Unir con espacios (' '.join())\n    return [' '.join([stemmer.stem(word) for word in review.split()]) \n            for review in corpus]\n\n# Aplicamos stemming a train y test\nstemmed_reviews_train = get_stemmed_text(reviews_train_clean)\nstemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n\n# Ahora vectorizamos las reviews \"stemmed\"\ncv = CountVectorizer(binary=True, stop_words=english_stop_words)\ncv.fit(stemmed_reviews_train)\n\nX_stem = cv.transform(stemmed_reviews_train)\nX_test = cv.transform(stemmed_reviews_test)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVc3cfv1evLy",
    "outputId": "01b20c5e-d6f5-482a-8892-9ce6585c9493"
   },
   "outputs": [],
   "source": "# =============================================================================\n# EVALUAR MODELO CON STEMMING\n# =============================================================================\n# Entrenamos el modelo con las reviews procesadas con stemming\n# ¿Mejora el accuracy?\n\ntrain_model(X_stem, X_test)\n\n# Resultado: ~87.68%\n# Similar al modelo anterior (sin stemming con stopwords eliminadas)\n# Stemming no siempre mejora, pero reduce significativamente el vocabulario"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyT5mSLhevLy",
    "outputId": "cc58a9c4-a3fd-4509-99d3-c76e4041021d"
   },
   "outputs": [],
   "source": "# =============================================================================\n# IMPACTO DEL STEMMING EN EL VOCABULARIO\n# =============================================================================\n# Comparamos el tamaño del vocabulario antes y después del stemming\n\nprint(X_baseline.shape)      # Sin stemming: 87,063 palabras\nprint(X_stem.shape)          # Con stemming: 66,715 palabras\nprint(\"Diff X normal y X tras stemmer y vectorización:\", \n      X_baseline.shape[1] - X_stem.shape[1])\n\n# ¡Reducción de 20,348 palabras!\n# Esto es una reducción del ~23% del vocabulario\n# \n# ¿Por qué?\n# Porque stemming agrupa formas de la misma palabra:\n# - \"amazing\", \"amazed\", \"amazement\" → \"amaz\"\n# - \"loving\", \"loved\", \"loves\" → \"love\"\n# \n# Ventajas:\n# - Menos features = modelo más rápido de entrenar\n# - Agrupa conceptos similares\n# - Reduce overfitting\n#\n# Desventajas:\n# - Pierde matices del lenguaje\n# - Puede crear \"palabras\" sin sentido (\"fli\" en lugar de \"fly\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDXm5YoFevLy"
   },
   "source": "# Lemmatization\n\nLa lemmatization funciona identificando la parte del discurso de una palabra dada y luego aplicando reglas más complejas para transformar la palabra en su raíz verdadera."
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "fJT8m90NevLz"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FppH1l9wevLz",
    "outputId": "12e11fb9-102c-4583-eb2f-3454e10cfd2d"
   },
   "outputs": [],
   "source": "# =============================================================================\n# DESCARGAR WORDNET - Base de datos léxica para lemmatization\n# =============================================================================\n# WordNet es una base de datos léxica del inglés\n# Contiene información sobre relaciones entre palabras, sinónimos, etc.\n# Es necesaria para hacer lemmatization correctamente\n\nnltk.download('wordnet')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJwmAwFRevLz",
    "outputId": "48571f6d-2ce8-448f-af8a-575a6db74ea3"
   },
   "outputs": [],
   "source": "# =============================================================================\n# LEMMATIZATION - Reducción de palabras a su forma base (método \"inteligente\")\n# =============================================================================\n# ¿Qué es lemmatization?\n# Es un proceso más sofisticado que stemming\n# Usa un diccionario (WordNet) para encontrar la forma base real de cada palabra\n#\n# Diferencias clave con stemming:\n# - Stemming: recorta sufijos (método bruto) → \"flies\" → \"fli\"\n# - Lemmatization: busca la raíz real → \"flies\" → \"fly\"\n#\n# Ventajas de lemmatization:\n# - Produce palabras reales (no \"raíces inventadas\")\n# - Más preciso linguísticamente\n# - Considera el contexto (parte del discurso: verbo, sustantivo, etc.)\n#\n# Desventajas:\n# - Más lento que stemming (requiere búsqueda en diccionario)\n# - Necesita WordNet u otro diccionario\n# - No disponible fácilmente en todos los idiomas\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Mismas palabras de ejemplo que con stemming\nplurals = ['caresses', 'flies','fly','flight', 'dies', 'mules', 'studies',\n            'died', 'agreed', 'owned', 'humbled', 'sized',\n            'meeting', 'stating', 'siezing', 'itemization',\n            'sensational', 'traditional', 'reference', 'colonizer',\n            'plotted']\n\n# Aplicamos lemmatization\nsingles = [lemmatizer.lemmatize(plural) for plural in plurals]\n\n# Compara con stemming:\n# - Stemming: 'flies' → 'fli' (sin sentido)\n# - Lemmatization: 'flies' → 'fly' (palabra real)\n#\n# - Stemming: 'agreed' → 'agre'\n# - Lemmatization: 'agreed' → 'agreed' (no detecta que es verbo sin contexto)\n#\n# Nota: por defecto, lemmatize() asume que la palabra es un sustantivo\n# Para mejor precisión se puede especificar: lemmatize(word, pos='v') para verbos\nprint(' '.join(singles))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvFDXfv2evLz",
    "outputId": "2029d781-eafe-4d9a-8920-009062c37ba0"
   },
   "outputs": [],
   "source": "# =============================================================================\n# APLICAR LEMMATIZATION A NUESTRAS REVIEWS\n# =============================================================================\n# Aplicamos lemmatization a todas las reviews de películas\n# El proceso es similar a stemming pero con resultados más precisos\n\ndef get_lemmatized_text(corpus):\n    \"\"\"\n    Aplica lemmatization a cada palabra de cada review\n    \n    Proceso similar a get_stemmed_text() pero usando WordNetLemmatizer\n    \n    Args:\n        corpus: lista de reviews\n    \n    Returns:\n        lista de reviews con palabras lemmatizadas\n    \"\"\"\n    from nltk.stem import WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n    \n    # Para cada review:\n    # - Dividir en palabras\n    # - Aplicar lemmatization a cada palabra\n    # - Unir de nuevo\n    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) \n            for review in corpus]\n\n# Lemmatizamos las reviews\nlemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\nlemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n\n# Vectorizamos con conteo tras lematizar\ncv = CountVectorizer(binary=True, stop_words=english_stop_words)\ncv.fit(lemmatized_reviews_train)\n\nX = cv.transform(lemmatized_reviews_train)\nX_test = cv.transform(lemmatized_reviews_test)\n\n# Entrenamos y evaluamos\ntrain_model(X, X_test)\n\n# Resultado: ~87.82%\n# Similar a stemming, ligeramente mejor"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYk6uutsevL0",
    "outputId": "43fef4a6-b2db-4b42-a113-b0887aa1fdfc"
   },
   "outputs": [],
   "source": "# =============================================================================\n# IMPACTO DE LEMMATIZATION EN EL VOCABULARIO\n# =============================================================================\n# Comparamos el tamaño del vocabulario con lemmatization vs baseline\n\nprint(X_baseline.shape)  # Baseline: 87,063 palabras\nprint(X.shape)           # Con lemmatization: 80,215 palabras\nprint(\"Diff X normal y X tras lematizador y vectorización:\", \n      X_baseline.shape[1] - X.shape[1])\n\n# Reducción de 6,848 palabras (7.9%)\n# Mucho MENOS reducción que stemming (que eliminó 20,348 palabras)\n#\n# ¿Por qué?\n# Lemmatization es menos agresivo:\n# - Stemming: \"running\", \"runs\", \"ran\" → \"run\" (todas iguales)\n# - Lemmatization: \"running\" → \"running\", \"runs\" → \"run\", \"ran\" → \"ran\"\n#   (preserva algunas diferencias si no tiene contexto)\n#\n# Conclusión:\n# - Lemmatization: más preciso, preserva más información\n# - Stemming: más agresivo, reduce más el vocabulario\n# - En este caso, ninguno mejora significativamente el accuracy del baseline"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjD2rQxuevL0"
   },
   "source": "# n-gramas\n\nPotencialmente podemos agregar más poder predictivo a nuestro modelo agregando secuencias de dos o tres palabras (bigramas o trigramas) también. Por ejemplo, si una reseña tiene la secuencia de tres palabras \"didn't love movie\" solo consideraríamos estas palabras individualmente con un modelo de solo unigramas y probablemente no capturaríamos que esto es en realidad un sentimiento negativo porque la palabra 'love' por sí misma estará altamente correlacionada con una reseña positiva.\n\nLa librería scikit-learn hace esto muy fácil de probar. Solo usa el argumento ngram_range con cualquiera de las clases 'Vectorizer'."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lW6kUowYevL0",
    "outputId": "dfc2c752-2b72-41fc-fd3a-d3e01b804ebd"
   },
   "outputs": [],
   "source": "# =============================================================================\n# N-GRAMAS - Capturando secuencias de palabras\n# =============================================================================\n# ¿Qué son los n-gramas?\n# Son secuencias de N palabras consecutivas en un texto\n#\n# Tipos:\n# - Unigrama (n=1): palabras individuales → \"didn't\", \"love\", \"music\"\n# - Bigrama (n=2): pares de palabras → \"didn't love\", \"love music\"\n# - Trigrama (n=3): tríos de palabras → \"didn't love music\", \"love music at\"\n#\n# ¿Por qué usar n-gramas?\n# - Capturan contexto: \"not good\" tiene significado negativo\n# - Solo con unigramas: \"not\" (negativo?) + \"good\" (positivo?) = confuso\n# - Con bigramas: \"not good\" es claramente negativo\n#\n# Ejemplos útiles en análisis de sentimiento:\n# - \"didn't love\" (negativo, aunque \"love\" solo sea positivo)\n# - \"not bad\" (positivo, aunque \"not\" y \"bad\" sean negativos)\n# - \"very good\" (muy positivo)\n\nfrom nltk import ngrams\n\nsentence = \"didn't love music at all my love\"\n\n# Creamos unigramas, bigramas y trigramas\none = ngrams(sentence.split(), 1)    # Palabras individuales\ntwo = ngrams(sentence.split(), 2)    # Pares de palabras\nthree = ngrams(sentence.split(), 3)  # Tríos de palabras\n\n# Unigramas\nprint(\"UNIGRAMAS (palabras individuales):\")\nfor grams in one:\n    print(grams)\nprint('###############')\n\n# Bigramas\nprint(\"BIGRAMAS (pares de palabras):\")\nfor grams in two:\n    print(grams)\nprint('###############')\n\n# Trigramas\nprint(\"TRIGRAMAS (tríos de palabras):\")\nfor grams in three:\n    print(grams)\n\n# Observa cómo los bigramas capturan mejor el contexto:\n# - (\"didn't\", \"love\") transmite negación del amor\n# - Solo \"love\" podría parecer positivo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VfYGKmDevL1",
    "outputId": "88983c1e-76cd-44b1-efeb-2b28ea86b16b"
   },
   "outputs": [],
   "source": "# =============================================================================\n# N-GRAMAS CON CountVectorizer - Ejemplo simple\n# =============================================================================\n# CountVectorizer puede generar n-gramas automáticamente con ngram_range\n# \n# ngram_range es una tupla (min_n, max_n):\n# - (1, 1): solo unigramas\n# - (2, 2): solo bigramas\n# - (1, 2): unigramas Y bigramas\n# - (1, 3): unigramas, bigramas Y trigramas\n\nngram_vectorizer = CountVectorizer(binary=True,\n                                   ngram_range=(1, 2))  # Unigramas + Bigramas\n\n# Aplicamos al ejemplo simple\nvector = ngram_vectorizer.fit_transform([sentence]).toarray()\nprint(vector)\nprint(len(vector[0]))\n\n# El vector tiene 12 elementos:\n# - 7 unigramas únicos\n# - 5 bigramas únicos (6 bigramas totales, pero \"love\" se repite)\n# \n# NOTA: algunas palabras pueden ser eliminadas automáticamente por CountVectorizer\n# (por ejemplo, palabras de una sola letra como 'a')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXexWbebevL1",
    "outputId": "9df978bf-b2a8-4c6b-8d36-18e2727d0ddd"
   },
   "outputs": [],
   "source": "# =============================================================================\n# APLICAR N-GRAMAS A NUESTRAS REVIEWS - Con límite de features\n# =============================================================================\n# Ahora aplicamos n-gramas a todas nuestras reviews de películas\n# \n# IMPORTANTE: ngram_range=(1, 2) crea MUCHAS features\n# - Unigramas: ~87,000\n# - Bigramas: cientos de miles o millones\n# - Total: puede explotar en tamaño\n#\n# Solución: max_features limita el número de features\n# Selecciona solo las N palabras/n-gramas más frecuentes\n\nngram_vectorizer = CountVectorizer(binary=True, \n                                   stop_words=english_stop_words,\n                                   ngram_range=(1, 2),      # Unigramas + bigramas\n                                   max_features=30000)      # Solo las 30,000 más frecuentes\n\nngram_vectorizer.fit(reviews_train_clean)\n\nX = ngram_vectorizer.transform(reviews_train_clean)\nX_test = ngram_vectorizer.transform(reviews_test_clean)\n\n# Entrenamos el modelo\ntrain_model(X, X_test)\n\n# Resultado: ~88.74% - ¡MEJORA respecto al baseline!\n# Los bigramas ayudan al modelo a entender mejor el contexto"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTvaVdSbevL1",
    "outputId": "16021f12-1a49-4dcb-bbb4-ad41979a0386"
   },
   "outputs": [],
   "source": "# =============================================================================\n# IMPACTO DE max_features\n# =============================================================================\n# Comparamos el tamaño de features con y sin límite\n\nprint(X_baseline.shape)  # Baseline: 87,063 palabras (solo unigramas)\nprint(X.shape)           # Con n-gramas: 30,000 features (limitado por max_features)\nprint(\"Diff X normal y X tras n-gramas:\", X_baseline.shape[1] - X.shape[1])\n\n# Sin max_features, ¡tendríamos 1,448,047 features!\n# \n# ¿Por qué tantas?\n# Número de bigramas posibles = vocabulario × vocabulario\n# Si tenemos ~87,000 palabras, hay millones de combinaciones posibles\n#\n# max_features=30000 nos da un buen balance:\n# - Suficientes features para capturar patrones importantes\n# - No tantas que el modelo sea inmanejable\n# - Selecciona las más frecuentes/informativas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qN6ZyKxcevL2",
    "outputId": "b15a8165-012a-46c2-e121-98231bc003d8"
   },
   "outputs": [],
   "source": "# =============================================================================\n# EXPERIMENTO: Solo bigramas (sin unigramas)\n# =============================================================================\n# ¿Qué pasa si usamos SOLO bigramas?\n# ngram_range=(2, 2) → solo pares de palabras, no palabras individuales\n\nngram_vectorizer = CountVectorizer(binary=True, \n                                   stop_words=english_stop_words,\n                                   ngram_range=(2, 2),      # SOLO bigramas\n                                   max_features=10000)      # 10,000 bigramas más frecuentes\n\nngram_vectorizer.fit(reviews_train_clean)\n\nX = ngram_vectorizer.transform(reviews_train_clean)\nX_test = ngram_vectorizer.transform(reviews_test_clean)\n\nprint(X_baseline.shape)  # Baseline: 87,063 unigramas\nprint(X.shape)           # Solo bigramas: 10,000 features\nprint(\"Diff X normal y X tras n-gramas:\", X_baseline.shape[1] - X.shape[1])\n\n# Entrenamos el modelo\ntrain_model(X, X_test)\n\n# Resultado: ~81.54% - PEOR que el baseline\n# \n# Conclusión:\n# - Los bigramas solos NO son suficientes\n# - Necesitamos COMBINAR unigramas + bigramas para mejor rendimiento\n# - Los unigramas capturan palabras individuales importantes\n# - Los bigramas capturan contexto y relaciones entre palabras"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYjtZdpIevL2"
   },
   "source": "# TF-IDF\n\nOtra forma común de representar cada documento en un corpus es usar la estadística tf-idf (term frequency-inverse document frequency) para cada palabra, que es un factor de ponderación que podemos usar en lugar de representaciones binarias o de conteo de palabras.\n\nHay varias formas de hacer la transformación tf-idf pero en resumen, **tf-idf busca representar el número de veces que una palabra dada aparece en un documento (una reseña de película en nuestro caso) en relación con el número de documentos en el corpus en los que aparece la palabra**.\n\n**Nota:** Ahora que hemos visto los n-gramas, cuando me refiero a 'palabras' realmente quiero decir cualquier n-grama (secuencia de palabras) si el modelo está usando una n mayor que uno."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBp_rX83evL2",
    "outputId": "a0980269-82ac-4dcb-b1f8-76821231789a"
   },
   "outputs": [],
   "source": "# =============================================================================\n# CÁLCULO MANUAL DE IDF (Inverse Document Frequency)\n# =============================================================================\n# Antes de usar TfidfVectorizer, entendamos cómo funciona el cálculo IDF\n# \n# IDF (Inverse Document Frequency):\n# Penaliza palabras muy comunes y realza palabras raras\n# Fórmula: 1 + ln((N + 1) / (count + 1))\n# \n# Donde:\n# - N = número total de documentos en el corpus\n# - count = número de documentos que contienen la palabra\n#\n# Ejemplo: si una palabra aparece en muchos documentos, su IDF será bajo\n\n# Número de documentos\nN = 3\n\n# Número de veces que aparece una palabra específica (en cuántos documentos)\ncount = 2\n\n# Calculamos el IDF\n# Si la palabra aparece en 2 de 3 documentos:\n# IDF = 1 + ln((3 + 1) / (2 + 1)) = 1 + ln(4/3) = 1.288\n1 + np.log((N + 1)/(count + 1))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "io0cL4HAevL2",
    "outputId": "346a6f87-dbbb-4f07-a0ba-e7bd85c087b0"
   },
   "outputs": [],
   "source": "# =============================================================================\n# TF-IDF - Term Frequency - Inverse Document Frequency\n# =============================================================================\n# TF-IDF combina dos métricas:\n# \n# 1. TF (Term Frequency): \n#    ¿Cuántas veces aparece la palabra en un documento específico?\n# \n# 2. IDF (Inverse Document Frequency):\n#    ¿Qué tan rara/común es la palabra en todo el corpus?\n#    - Palabras muy comunes (ej: \"the\", \"is\") → IDF bajo\n#    - Palabras raras pero informativas → IDF alto\n#\n# TF-IDF = TF × IDF\n#\n# ¿Por qué usar TF-IDF en lugar de conteos simples?\n# - Reduce el peso de palabras muy comunes\n# - Aumenta el peso de palabras distintivas\n# - Mejor representación del \"contenido\" real del documento\n#\n# Ejemplo práctico:\n# - Palabra \"ralph\" aparece en 3 documentos → IDF = 1.0 (muy común en este corpus)\n# - Palabra \"nice\" aparece en 1 documento → IDF = 1.69 (más distintiva)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Corpus de ejemplo\nsent1 = 'My name is Ralph'\nsent2 = 'Ralph is nice'\nsent3 = 'Ralph'\n\n# Creamos el TfidfVectorizer\ntest = TfidfVectorizer()\ntest.fit_transform([sent1, sent2, sent3])\n\n# Valores IDF para cada palabra\n# Cuanto más común es la palabra, más bajo es su IDF\nprint(test.idf_)\nprint(test.get_feature_names_out())\n\n# Resultados:\n# 'ralph': 1.0 (aparece en los 3 documentos, muy común)\n# 'my', 'name', 'nice': 1.69 (aparecen en 1 solo documento, distintivas)\n# 'is': 1.29 (aparece en 2 documentos, medianamente común)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylcN8G3yevL3",
    "outputId": "92d668b6-5d39-4bc3-eed6-377c0980e481"
   },
   "outputs": [],
   "source": "# =============================================================================\n# VERIFICAR EL CÁLCULO IDF MANUALMENTE\n# =============================================================================\n# Verificamos que el cálculo coincide con lo que hace TfidfVectorizer\n\n# 'ralph' aparece en 3 documentos de un total de 3\n# Fórmula: 1 + ln((N + 1) / (count + 1))\n# 1 + ln((3 + 1) / (3 + 1)) = 1 + ln(4/4) = 1 + ln(1) = 1 + 0 = 1.0\n\n1 + np.log((3 + 1)/(3 + 1))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grzH7PgAevL3",
    "outputId": "6c0afa12-f902-4230-953e-ef4ff08615b4"
   },
   "outputs": [],
   "source": "# =============================================================================\n# APLICAR TF-IDF A NUESTRAS REVIEWS\n# =============================================================================\n# Ahora aplicamos TfidfVectorizer en lugar de CountVectorizer\n# Esto convierte cada review en un vector de valores TF-IDF\n# en lugar de simples conteos binarios o frecuencias\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Creamos el vectorizador TF-IDF\ntfidf_vectorizer = TfidfVectorizer()\n\n# Ajustamos y transformamos las reviews\ntfidf_vectorizer.fit(reviews_train_clean)\nX = tfidf_vectorizer.transform(reviews_train_clean)\nprint(X.shape)\n\n# Transformamos test con el mismo vectorizador\nX_test = tfidf_vectorizer.transform(reviews_test_clean)\n\n# Ahora cada celda de la matriz NO contiene 0 o 1\n# Contiene valores continuos (floats) que representan TF-IDF\n# Valores más altos = palabras más importantes para ese documento específico"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smppaV2hevL3",
    "outputId": "7b4ca924-2f43-49cb-d760-36371292a517"
   },
   "outputs": [],
   "source": "# =============================================================================\n# ENTRENAR MODELO CON TF-IDF\n# =============================================================================\n# Entrenamos Regresión Logística con las features TF-IDF\n# ¿Mejora respecto al baseline con CountVectorizer?\n\ntrain_model(X, X_test)\n\n# Resultado: ~88.18% \n# Prácticamente IGUAL que el baseline con CountVectorizer binario\n# \n# Conclusión:\n# - TF-IDF no siempre es mejor que conteos binarios\n# - Para análisis de sentimiento, la presencia/ausencia (binario) suele funcionar bien\n# - TF-IDF es más útil en tareas como búsqueda de información o clasificación de tópicos"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvK1aSuKevL3",
    "outputId": "918dc1f5-145d-4a67-b2d4-15cf0133e09b"
   },
   "outputs": [],
   "source": "# =============================================================================\n# COMPARACIÓN DE DIMENSIONES: TF-IDF vs Baseline\n# =============================================================================\n# Comparamos el tamaño de las matrices\n\nprint(X_baseline.shape)  # CountVectorizer binario: 87,063 palabras\nprint(X.shape)           # TfidfVectorizer: 87,063 palabras\nprint(\"Diff X normal y X tras TF-IDF:\", X_baseline.shape[1] - X.shape[1])\n\n# ¡Mismo número de features! (diferencia = 0)\n# Lo que cambia son los VALORES dentro de la matriz:\n# - CountVectorizer binario: solo 0 y 1\n# - TfidfVectorizer: valores continuos (floats) entre 0 y ~1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tonxn9kevL4"
   },
   "source": "# Máquinas de Vectores de Soporte (SVM)\n\nRecordemos que los clasificadores lineales tienden a funcionar bien en conjuntos de datos muy dispersos (como el que tenemos). Otro algoritmo que puede producir excelentes resultados con un tiempo de entrenamiento rápido son las Máquinas de Vectores de Soporte con un kernel lineal.\n\nConstruir un modelo con un rango de n-gramas de 1 a 2:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdwj1YkTevL4",
    "outputId": "ffa66563-213a-4b24-fd6f-fe8562d427cd"
   },
   "outputs": [],
   "source": "# =============================================================================\n# SVM (Support Vector Machine) CON N-GRAMAS\n# =============================================================================\n# Ahora probamos un algoritmo diferente: SVM con kernel lineal\n# \n# ¿Por qué SVM?\n# - Funciona muy bien con datos de alta dimensionalidad (muchas features)\n# - Especialmente bueno con datos sparse (matrices con muchos ceros)\n# - A menudo más rápido que Regresión Logística en estos casos\n# - Encuentra el hiperplano óptimo que separa las clases\n#\n# LinearSVC = Linear Support Vector Classifier\n# \"Linear\" porque usa un kernel lineal (decisiones lineales)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Configuramos CountVectorizer con n-gramas (1-3)\n# Incluimos unigramas, bigramas Y trigramas\nngram_vectorizer = CountVectorizer(binary=True, \n                                   ngram_range=(1, 3),      # Hasta trigramas\n                                   max_features=20000)      # Limitamos a 20,000 features\n\nngram_vectorizer.fit(reviews_train_clean)\nX = ngram_vectorizer.transform(reviews_train_clean)\nX_test = ngram_vectorizer.transform(reviews_test_clean)\n\n# Función para entrenar SVM con Grid Search\ndef train_model_svm(X_TRAIN, X_TEST):\n    \"\"\"\n    Entrena un modelo SVM con Grid Search\n    \n    Hiperparámetro C:\n    - Similar a Regresión Logística\n    - C alto: modelo más complejo, puede hacer overfitting\n    - C bajo: modelo más simple, puede hacer underfitting\n    \n    Args:\n        X_TRAIN: matriz de features de entrenamiento\n        X_TEST: matriz de features de test\n    \"\"\"\n    \n    svm = LinearSVC()  # Creamos el clasificador SVM\n    \n    # Valores de C a probar\n    params = {\n        'C': [0.01, 0.05, 0.25, 0.5, 1]\n    }\n    \n    # Grid Search con validación cruzada\n    grid = GridSearchCV(svm, params, cv=5)\n    grid.fit(X_TRAIN, target)\n    \n    # Evaluamos en test\n    print(\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))\n\n# Entrenamos el SVM\ntrain_model_svm(X, X_test)\n\n# Resultado: ~88.65%\n# ¡Mejor que el baseline! Los trigramas + SVM ayudan un poco más"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWUMedCkevL4"
   },
   "source": "# Modelo Final\n\nEliminar un pequeño conjunto de stop words junto con un rango de n-gramas de 1 a 3 y un clasificador de vectores de soporte lineal muestra los mejores resultados."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CnJ4kblevL4",
    "outputId": "3a3059de-cc14-4230-8f27-134a1f8fab42"
   },
   "outputs": [],
   "source": "# =============================================================================\n# MODELO FINAL - Mejor configuración encontrada\n# =============================================================================\n# Después de probar diferentes técnicas, el mejor modelo combina:\n# - N-gramas (1-3): unigramas, bigramas y trigramas\n# - Lista pequeña de stopwords (no la lista completa de NLTK)\n# - Regresión Logística (funciona casi tan bien como SVM)\n#\n# ¿Por qué esta configuración?\n# - Los trigramas capturan frases completas como \"not very good\"\n# - Eliminar TODAS las stopwords puede ser contraproducente\n# - Solo eliminamos las palabras más comunes que claramente no aportan\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC\n\n# Lista REDUCIDA de stopwords\n# Solo incluimos las más comunes y menos informativas\nstop_words = ['in', 'of', 'at', 'a', 'the']\n\n# CountVectorizer con la mejor configuración\nngram_vectorizer = CountVectorizer(binary=True,\n                                   ngram_range=(1, 3),         # Hasta trigramas\n                                   stop_words=stop_words)      # Pocas stopwords\n\n# Ajustamos y transformamos\nngram_vectorizer.fit(reviews_train_clean)\nX = ngram_vectorizer.transform(reviews_train_clean)\nX_test = ngram_vectorizer.transform(reviews_test_clean)\n\n# Entrenamos con Regresión Logística\ntrain_model(X, X_test)\n\n# Resultado: ~90.01% - ¡EL MEJOR HASTA AHORA!\n# \n# Mejoras respecto al baseline (88.18%):\n# - +1.83% de accuracy\n# - Mejor captura de contexto con n-gramas\n# - Balance entre complejidad y generalización"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MeRFH8KevL5"
   },
   "source": "# Características Positivas y Negativas Principales\n\nObtener las características más importantes del modelo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZF_AZ1WevL6",
    "outputId": "9c6324d6-5d38-4d33-daf4-1dca30e237a5"
   },
   "outputs": [],
   "source": "# =============================================================================\n# ANÁLISIS DE FEATURES - Palabras más importantes\n# =============================================================================\n# Ahora vamos a investigar qué palabras son más importantes para el modelo\n# Esto nos ayuda a entender QUÉ está aprendiendo el modelo\n#\n# Entrenamos un modelo simple (sin n-gramas) para análisis\n\ncv = CountVectorizer(binary=True)\ncv.fit(reviews_train_clean)\nX = cv.transform(reviews_train_clean)\n\n# Entrenamos Regresión Logística\nlog_reg = LogisticRegression(C=0.5)\nlog_reg.fit(X, target)\n\n# Importancia de los coeficientes\n# log_reg.coef_ contiene los pesos aprendidos por el modelo\n# - Coeficiente positivo alto → palabra fuertemente asociada con clase positiva\n# - Coeficiente negativo alto → palabra fuertemente asociada con clase negativa\nprint(len(log_reg.coef_[0]))  # Total: 87,063 coeficientes (uno por palabra)\n\n# Cada palabra tiene su coeficiente asociado\ncv.get_feature_names_out()\n\n# Creamos un diccionario: palabra → coeficiente\n# Esto nos permite ordenar y encontrar las palabras más importantes\nfeature_to_coef = {\n    word: coef for word, coef in zip(\n        cv.get_feature_names_out(), log_reg.coef_[0]\n    )\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAdTW3R0evL6",
    "outputId": "1075c7c2-9da7-4e9c-a268-d375d3df8740"
   },
   "outputs": [],
   "source": "# =============================================================================\n# PRUEBA DEL MODELO - Review negativa\n# =============================================================================\n# Probamos el modelo con una review claramente negativa\n# predict() devuelve 0 (negativo) o 1 (positivo)\n\nlog_reg.predict(cv.transform(['This movie is horrible']))\n\n# Resultado esperado: [0] (negativo)\n# ¡El modelo identifica correctamente el sentimiento negativo!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMdRuQNpevL6",
    "outputId": "08f1ad64-28b3-4dcb-dd83-3ac7bba1e1f1"
   },
   "outputs": [],
   "source": "# =============================================================================\n# PRUEBA DEL MODELO - Review positiva\n# =============================================================================\n# Probamos con una review claramente positiva\n\nlog_reg.predict(cv.transform(['This movie is incredible']))\n\n# Resultado esperado: [1] (positivo)\n# ¡El modelo identifica correctamente el sentimiento positivo!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5F-4y4_evL6",
    "outputId": "99c348bd-402f-4cba-c98b-58175f3d5263"
   },
   "outputs": [],
   "source": "# =============================================================================\n# TOP 5 PALABRAS MÁS POSITIVAS Y NEGATIVAS\n# =============================================================================\n# Identificamos las palabras con mayor peso en cada sentimiento\n# Esto nos ayuda a interpretar qué aprendió el modelo\n\n# TOP 5 PALABRAS MÁS POSITIVAS\n# Ordenamos por coeficiente de mayor a menor (reverse=True)\nprint(\"TOP 5 PALABRAS MÁS POSITIVAS:\")\nprint(\"=\" * 50)\nfor best_positive in sorted(\n    feature_to_coef.items(),\n    key=lambda x: x[1],      # Ordenar por coeficiente\n    reverse=True)[:5]:        # Top 5\n    print(f\"{best_positive[0]}: {best_positive[1]:.4f}\")\n\nprint('\\n' + '=' * 50)\nprint(\"TOP 5 PALABRAS MÁS NEGATIVAS:\")\nprint(\"=\" * 50)\n\n# TOP 5 PALABRAS MÁS NEGATIVAS\n# Ordenamos por coeficiente de menor a mayor (sin reverse)\nfor best_negative in sorted(\n    feature_to_coef.items(),\n    key=lambda x: x[1])[:5]:  # Top 5 más negativas\n    print(f\"{best_negative[0]}: {best_negative[1]:.4f}\")\n\n# Resultados típicos:\n# POSITIVAS: 'excellent', 'perfect', 'superb', 'wonderful', 'brilliant'\n# NEGATIVAS: 'worst', 'waste', 'awful', 'boring', 'terrible'\n#\n# ¡Tiene sentido! El modelo aprendió palabras que realmente indican sentimiento\n#\n# Interpretación de coeficientes:\n# - 'excellent': +1.38 → fuerte indicador de review positiva\n# - 'worst': -2.08 → fuerte indicador de review negativa\n# - Cuanto mayor el valor absoluto, más importante es la palabra para clasificar"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}