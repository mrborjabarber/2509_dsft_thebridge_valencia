{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerías esenciales para análisis de datos\n",
    "import pandas as pd  # Para manipulación de datos estructurados\n",
    "import numpy as np   # Para operaciones numéricas\n",
    "import matplotlib as mpl  # Para configuración de gráficos\n",
    "import seaborn as sns     # Para visualizaciones estadísticas\n",
    "\n",
    "# Cargamos el dataset de Titanic modificado para clasificación\n",
    "df = pd.read_csv('data/titanic_modified.csv')\n",
    "# Mostramos información básica del dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos train_test_split para dividir los datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Obtenemos los nombres de las columnas que usaremos como características\n",
    "classes = list(df.loc[:, 'Pclass':].columns)\n",
    "\n",
    "# Separamos características (X) y variable objetivo (y)\n",
    "X = df.loc[:, 'Pclass':].values  # Todas las columnas desde 'Pclass' hasta el final\n",
    "y = df['Survived'].values        # Variable objetivo: supervivencia\n",
    "\n",
    "# Dividimos los datos en conjuntos de entrenamiento y prueba\n",
    "# test_size=0.20: 20% para prueba, 80% para entrenamiento\n",
    "# random_state=55: semilla para reproducibilidad\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.20,\n",
    "                                                    random_state=55)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting classifier\n",
    "1. Primero declaramos todos los clasificadores que participarán en el voting classifier.\n",
    "2. A continuación agrupamos los estimadores en una lista de tuplas, con sus identificadores.\n",
    "3. Declaramos el VotingClassifier\n",
    "\n",
    "En primer lugar, probaremos un `hard` VotingClassifier, es decir, tendrá en cuenta las predicciones de los clasificadores, no sus probabilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos la generación de números aleatorios con semilla\n",
    "# Esto demuestra la reproducibilidad cuando fijamos random_state\n",
    "np.random.seed(42)\n",
    "print(np.random.randint(10))  # Primer número aleatorio\n",
    "print(np.random.randint(10))  # Segundo número aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los algoritmos necesarios para crear el ensemble de votación\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "'''\n",
    "Creamos tres clasificadores diferentes para el ensemble:\n",
    "1. LogisticRegression: algoritmo lineal rápido y interpretable\n",
    "2. RandomForestClassifier: ensemble de árboles, robusto y eficaz\n",
    "3. SVC: Support Vector Classifier, potente para clasificación no-lineal\n",
    "\n",
    "Nota: Por defecto VotingClassifier usa 'soft' voting si todos los algoritmos\n",
    "soportan predict_proba(). SVM requiere probability=True para esto.\n",
    "'''\n",
    "\n",
    "# Configuramos los tres clasificadores base\n",
    "log_clf = LogisticRegression(random_state=42) # Regresión logística\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42) # Random Forest con 100 árboles\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=42)  # SVM con parámetros por defecto\n",
    "\n",
    "# Creamos una lista de tuplas con identificador y modelo\n",
    "estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)]\n",
    "\n",
    "# Creamos el VotingClassifier con votación 'hard' (por mayoría simple)\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos el VotingClassifier con votación hard (por mayoría simple)\n",
    "# Cada algoritmo vota por una clase y se elige la clase más votada\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos los aciertos (accuracy score) de todos los clasificadores + el voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos accuracy_score para evaluar el rendimiento\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Comparamos el rendimiento individual de cada clasificador vs el ensemble\n",
    "# Entrenamos y evaluamos cada modelo por separado\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    # Entrenamos el modelo actual\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Hacemos predicciones en el conjunto de test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Mostramos el nombre del algoritmo y su accuracy\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probaremos ahora con el `soft` VotingClassifier, es decir, teniendo e cuenta las probabilidades, no los votos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos los clasificadores para votación 'soft' (basada en probabilidades)\n",
    "# En soft voting, se promedian las probabilidades de cada clase para la decisión final\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42) # probability=True necesario para soft voting\n",
    "\n",
    "estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)]\n",
    "\n",
    "# Creamos VotingClassifier con votación 'soft' (promedia probabilidades)\n",
    "soft_voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "\n",
    "# Entrenamos el modelo\n",
    "soft_voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos el rendimiento de cada algoritmo individual vs el ensemble con soft voting\n",
    "# El soft voting suele ser superior al hard voting cuando los algoritmos estiman bien las probabilidades\n",
    "for clf in (log_clf, rnd_clf, svm_clf, soft_voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Classifier\n",
    "Sistema de clasificación por votación de algoritmos. En este caso siempre es el mismo tipo de algoritmo, habitualmente árboles de decisión.\n",
    "\n",
    "El siguiente ejemplo sería un algoritmo muy similar a un RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos un BaggingClassifier (Bootstrap Aggregating)\n",
    "# Este es la base del RandomForest: múltiples modelos del mismo tipo con muestreo aleatorio\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Definimos el estimador base: un árbol de decisión con profundidad máxima 3\n",
    "estimator = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# Configuramos el BaggingClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    estimator = estimator,        # Tipo de modelo base\n",
    "    n_estimators=300,             # Número de modelos en el ensemble\n",
    "    max_samples=100,              # Cantidad de muestras para cada modelo (bootstrap)\n",
    "    bootstrap=True,               # Usamos bootstrap (muestreo con reemplazo)\n",
    "    # max_features = 3            # Features aleatorias por modelo (comentado)\n",
    "    random_state=42)\n",
    "\n",
    "# Entrenamos el ensemble\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor que un DecisionTree por separado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos el rendimiento del BaggingClassifier con un árbol individual\n",
    "# Esto demuestra el poder del ensemble: reduce la varianza y mejora la generalización\n",
    "tree_clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Los RandomForest cuentan con todos los hiperparámetros de un DecissionTree y los de un algoritmo de Bagging.\n",
    "\n",
    "**RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos RandomForestClassifier\n",
    "# Es una versión optimizada de BaggingClassifier que añade aleatoriedad en las características\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500,    # 500 árboles en el bosque\n",
    "                                 max_leaf_nodes=16,   # Máximo 16 nodos hoja por árbol\n",
    "                                 random_state=42)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "# Calculamos la precisión del Random Forest\n",
    "accuracy_score(y_pred_rf, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mejor que el Bagging Classifier, al llevar parámetros por defecto aplicados al DecisionTree que funcionan mejor para estos datos.\n",
    "\n",
    "Veamos el feature importance. En cada split de los árboles se calcula el IG (Information Gained) teniendo en cuenta la entropía antes y después del split. Se realiza una ponderación del IG en cada spllit, teniendo en cuenta la feature del split y con ello sklearn consigue el feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la importancia de las características según el Random Forest\n",
    "# El feature importance se calcula basado en cuánto reduce cada característica la impureza\n",
    "sns.barplot(x=classes, y=rnd_clf.feature_importances_);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué hiperparámetros debería tocar en el RandomForest?**\n",
    "1. `n_estimators`: número de árboles que participarán en las votaciones. Cuantos más mejor. NO producen overfitting. Cuanto más complejo es el dataset, mejor vendrá que haya muchos árboles. Más de 200 suele ser redundante.\n",
    "2. `max_depth`: profundida de los árboles. Cuanto más profundos, más complejo es el modelo, pero menos generaliza. De  nuevo, cuanto más complejo es el problema, mayor profundidad necesitaremos. No más de 20/30 es lo normal.\n",
    "3. `max_features`: features a tener en cuenta en los splits del árbol. Cuanto más bajo, mejor generalizará y menos overfitting. Numero menor a la cantidad de features del dataset, sino dará error.\n",
    "4. `min_samples_split`: mínima cantidad de muestras en un nodo antes de ser spliteado. 2 por defecto. Números bajos suelen dar buenos resultados (<50). Cuanto más alto, mejor generaliza, pero más baja la precisión.\n",
    "5. `min_samples_leaf`: mínima cantidad de puntos permitidos en un `leaf node`, es decir, un nodo que no va a volver a ser spliteado. Valores bajos funcionan bien (<50).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForestRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos RandomForestRegressor para un problema de regresión\n",
    "# Cargamos dataset de precios de casas en Estados Unidos\n",
    "df_reg = pd.read_csv('data/USA_Housing.csv')\n",
    "\n",
    "# Seleccionamos las características (variables independientes)\n",
    "X_reg = df_reg.loc[:, 'Avg. Area Income': 'Area Population'].values\n",
    "Y_reg = df_reg['Price'].values  # Variable objetivo: precio de la casa\n",
    "df_reg.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos y entrenamos RandomForestRegressor\n",
    "# Para regresión, cada árbol predice un valor numérico y se promedia la predicción\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "rnd_reg = RandomForestRegressor(n_estimators=200,      # 200 árboles\n",
    "                                max_leaf_nodes=32,     # Máximo 32 nodos hoja\n",
    "                                random_state=42)\n",
    "rnd_reg.fit(X_reg, Y_reg)\n",
    "\n",
    "# Evaluamos con Mean Absolute Error (diferencia promedio entre predicción y valor real)\n",
    "y_pred_reg = rnd_reg.predict(X_reg)\n",
    "mean_absolute_error(Y_reg, y_pred_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "**AdaBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos AdaBoostClassifier (Adaptive Boosting)\n",
    "# AdaBoost entrena modelos secuencialmente, cada uno corrigiendo los errores del anterior\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Usamos árboles de decisión muy simples (stumps) como estimadores base\n",
    "estimator = DecisionTreeClassifier(max_depth=1)  # max_depth=1 = decision stump\n",
    "\n",
    "ada_clf = AdaBoostClassifier(estimator = estimator,\n",
    "                             n_estimators=200,      # 200 iteraciones\n",
    "                             learning_rate=0.5,     # Tasa de aprendizaje (contribución de cada modelo)\n",
    "                             random_state=42)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También tenemos feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos las importancias de características calculadas por AdaBoost\n",
    "# Estas se basan en cómo cada característica contribuye a reducir el error\n",
    "ada_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la importancia de características del AdaBoost\n",
    "# Comparamos con RandomForest para ver diferencias en la selección de características\n",
    "sns.barplot(x=classes, y=ada_clf.feature_importances_);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué hiperparámetros debería tocar en el AdaBoostClassifier?**\n",
    "1. `n_estimators`: número de árboles que participarán en la corrección secuencial del error del modelo. Si corregimos el error a la perfección el algoritmo termina de entrenar. Cuantos más estimadores, mejor corregiremos el error pero mayor probabilidad de caer en overfitting. Valores superiores a 100 suelen sobreajustar el modelo aunque dependerá de la complejidad y volumen de los datos.\n",
    "2. `learning_rate`: no suele tener valores superiores a 1. Cuanto más alto, más aporta cada nuevo árbol, más preciso, pero caemos en overfitting. **Importante**: un learning rate bajo y alto número de estimadores no necesariamente tiene por qué aumentar la precisión y si va a inducir en altos costes computacionales.\n",
    "3. `algorithm`: 'SAME' o 'SAME.R'. 'SAME.R' utiliza la probabilidad para actualizar los modelos aditivos, mientras que 'SAME' usa los valores de clasificación. Similar a soft vs hard voting. 'SAMME.R' converge antes que 'SAMME'\n",
    "4. `estimator`: se suele dejar por defecto, aunque podría encajar un SVM o una RegresiónLogística\n",
    "5. `max_depth`: **OJO**, no es un hiperparámetro del AdaBoostClassifier, sino del DecisionTreeClassifier. Habrá que probar varios árboles con diferentes `max_depth` y después ponerlos como `base_estimator` en el AdaBoost. Cuanto mayor es este hiperparámetro, más preciso, pero también más overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoostRegressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos AdaBoostRegressor para problemas de regresión\n",
    "# Funciona igual que el clasificador pero para valores continuos\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada_reg = AdaBoostRegressor(n_estimators=200,    # 200 estimadores\n",
    "                            random_state=42)\n",
    "ada_reg.fit(X_reg, Y_reg)\n",
    "\n",
    "# Evaluamos el rendimiento con Mean Absolute Error\n",
    "y_pred_ada_reg = ada_reg.predict(X_reg)\n",
    "mean_absolute_error(Y_reg, y_pred_ada_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoosting\n",
    "\n",
    "**GradientBoostingRegressor**\n",
    "\n",
    "El GradientBoosting funciona sólo con árboles, por eso no es posible cambiar el estimador. Directamente los hiperparámetros a configurar en en GradientBoosting son los del DecissionTree.\n",
    "\n",
    "En este caso, vamos a probar primero con un regresor, que se entiende mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zh1dlOnAmuFL"
   },
   "outputs": [],
   "source": [
    "# Demostramos Gradient Boosting manualmente paso a paso\n",
    "# Creamos datos sintéticos para entender mejor el proceso\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5  # 100 puntos entre -0.5 y 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)  # Función cuadrática + ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUaOw9_AmuFN",
    "outputId": "fa442317-ac92-4c4f-9afb-e470c974a653"
   },
   "outputs": [],
   "source": [
    "# Paso 1: Entrenamos el primer árbol con los datos originales\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oV8o2rM0muFP",
    "outputId": "edb7c45b-a6df-4809-fd2d-f8391da6ac53"
   },
   "outputs": [],
   "source": [
    "# Paso 2: Calculamos los residuos (errores) y entrenamos el segundo árbol\n",
    "y2 = y - tree_reg1.predict(X)  # Residuos = valores reales - predicciones del árbol 1\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2.fit(X, y2)  # Entrenamos el segundo árbol para predecir los residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cislkt7HmuFS",
    "outputId": "4c65f876-16c2-4b8b-85f3-8918b777af58"
   },
   "outputs": [],
   "source": [
    "# Paso 3: Calculamos los residuos del segundo paso y entrenamos el tercer árbol\n",
    "y3 = y2 - tree_reg2.predict(X)  # Nuevos residuos = residuos anteriores - predicciones árbol 2\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg3.fit(X, y3)  # El tercer árbol intenta capturar lo que queda por explicar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGp-DSIumuFU"
   },
   "outputs": [],
   "source": [
    "# Definimos un punto nuevo para hacer predicción\n",
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrgoEcG7muFW"
   },
   "outputs": [],
   "source": [
    "# Predicción final del ensemble: suma de todas las predicciones\n",
    "# GradientBoosting = árbol1 + árbol2 + árbol3 + ... (cada uno corrige errores del anterior)\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaO527VxmuFY",
    "outputId": "6d1388c2-792c-4c9a-cc75-47fa5ecfcd5f"
   },
   "outputs": [],
   "source": [
    "# Mostramos el resultado de la predicción del ensemble manual\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos matplotlib para visualización\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wgls3RIpmuFb"
   },
   "outputs": [],
   "source": [
    "# Función auxiliar para visualizar las predicciones de los regresores\n",
    "def plot_predictions(regressors, X, y, axes, label=None, style=\"r-\", data_style=\"b.\", data_label=None):\n",
    "    \"\"\"\n",
    "    Visualiza las predicciones de uno o varios regresores\n",
    "    \"\"\"\n",
    "    x1 = np.linspace(axes[0], axes[1], 500)  # Puntos para graficar la curva\n",
    "    # Suma las predicciones de todos los regresores (para ensemble)\n",
    "    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)\n",
    "    plt.plot(X[:, 0], y, data_style, label=data_label)    # Datos originales\n",
    "    plt.plot(x1, y_pred, style, linewidth=2, label=label) # Predicciones del modelo\n",
    "    if label or data_label:\n",
    "        plt.legend(loc=\"upper center\", fontsize=16)\n",
    "    plt.axis(axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlBxhrBlmuFe",
    "outputId": "ab322637-71b1-4961-e431-2dc269ed4061"
   },
   "outputs": [],
   "source": [
    "# Visualizamos paso a paso el proceso de Gradient Boosting\n",
    "# Mostramos cómo cada árbol mejora la predicción del anterior\n",
    "plt.figure(figsize=(11,11))\n",
    "\n",
    "plt.subplot(321)\n",
    "# Primer árbol: predicción inicial\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h_1(x_1)$\", style=\"g-\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Residuals and tree predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(322)\n",
    "# Ensemble con solo el primer árbol\n",
    "plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1)$\", data_label=\"Training set\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "plt.title(\"Ensemble predictions\", fontsize=16)\n",
    "\n",
    "plt.subplot(323)\n",
    "# Segundo árbol: entrena en los residuos del primero\n",
    "plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_2(x_1)$\", style=\"g-\", data_style=\"k+\", data_label=\"Residuals\")\n",
    "plt.ylabel(\"$y - h_1(x_1)$\", fontsize=16)\n",
    "\n",
    "plt.subplot(324)\n",
    "# Ensemble con primer + segundo árbol\n",
    "plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1)$\")\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.subplot(325)\n",
    "# Tercer árbol: entrena en los residuos restantes\n",
    "plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label=\"$h_3(x_1)$\", style=\"g-\", data_style=\"k+\")\n",
    "plt.ylabel(\"$y - h_1(x_1) - h_2(x_1)$\", fontsize=16)\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "\n",
    "plt.subplot(326)\n",
    "# Ensemble final: suma de los tres árboles\n",
    "plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label=\"$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=16)\n",
    "plt.ylabel(\"$y$\", fontsize=16, rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos GradientBoostingRegressor usando sklearn\n",
    "# Esto automatiza el proceso manual que acabamos de ver\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2,         # Profundidad máxima de cada árbol\n",
    "                                 n_estimators=100,    # 100 iteraciones (árboles)\n",
    "                                 learning_rate=1.0,   # Tasa de aprendizaje (contribución de cada árbol)\n",
    "                                 random_state=42)\n",
    "gbrt.fit(X_reg, Y_reg)\n",
    "\n",
    "# Evaluamos el rendimiento\n",
    "y_pred_gbrt = gbrt.predict(X_reg)\n",
    "mean_absolute_error(Y_reg, y_pred_gbrt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GradientBoostingClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos GradientBoostingClassifier para problemas de clasificación\n",
    "# Funciona similar al regresor pero con función de pérdida para clasificación\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbct = GradientBoostingClassifier(max_depth=2,         # Profundidad máxima de cada árbol\n",
    "                                 n_estimators=100,     # 100 iteraciones\n",
    "                                 learning_rate=1.0,    # Tasa de aprendizaje\n",
    "                                 random_state=42)\n",
    "gbct.fit(X_train, y_train)\n",
    "\n",
    "# Evaluamos con accuracy\n",
    "y_pred_gbct = gbct.predict(X_test)\n",
    "accuracy_score(y_test, y_pred_gbct)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué hiperparámetros debería tocar en el GradientBoosting?**\n",
    "1. `n_estimators`: número de árboles que participarán en la corrección secuencial del error del modelo. Si corregimos el error a la perfección el algoritmo termina de entrenar. Cuantos más estimadores, mejor corregiremos el error pero mayor probabilidad de caer en overfitting. Valores superiores a 100 suelen sobreajustar el modelo aunque dependerá de la complejidad y volumen de los datos.\n",
    "2. `learning_rate`: no suele tener valores superiores a 1. Cuanto más alto, más aporta cada nuevo árbol, más preciso, pero caemos en overfitting. **Importante**: un learning rate bajo y alto número de estimadores no necesariamente tiene por qué aumentar la precisión y si va a inducir en altos costes computacionales.\n",
    "3. `max_depth`: Cuanto mayor es este hyperparámetro, más preciso, pero también más overfitting.\n",
    "\n",
    "Se puede iterar sobre todos los hiperparámetros recorridos en el RandomForest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "**XGBRegressor**\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "\n",
    "XGBoost permite regularizar el modelo, puede manejar missings, por lo que no es necesario tener el dataset perfectamente limpio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Líneas para instalar las librerías necesarias (ejecutar si no están instaladas)\n",
    "#!pip install xgboost\n",
    "#!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos XGBoostRegressor - versión optimizada de Gradient Boosting\n",
    "# XGBoost incluye regularización, manejo de valores faltantes y mejor rendimiento\n",
    "import xgboost\n",
    "\n",
    "# Creamos y entrenamos el regresor XGBoost con parámetros por defecto\n",
    "xgb_reg = xgboost.XGBRegressor(random_state=42)\n",
    "\n",
    "xgb_reg.fit(X_reg, Y_reg)\n",
    "y_pred = xgb_reg.predict(X_reg)\n",
    "# Evaluamos: XGBoost suele dar mejor rendimiento que sklearn GradientBoosting\n",
    "mean_absolute_error(Y_reg, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementamos XGBoostClassifier para problemas de clasificación\n",
    "# Mantiene las ventajas del regresor: regularización, eficiencia y robustez\n",
    "xgb_clas = xgboost.XGBClassifier(random_state=42)\n",
    "\n",
    "xgb_clas.fit(X_train, y_train)\n",
    "y_pred = xgb_clas.predict(X_test)\n",
    "# XGBoost suele superar a otros algoritmos de boosting en muchos datasets\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué hiperparámetros debería tocar en el XGB?**\n",
    "1. `n_estimators`: igual que para el GradientBoosting.\n",
    "2. `booster`: tipo de modelo que correrá en cada iteración. Arboles o regresiones. `gbtree` or `gblinear`. Los árboles suelen ir bien.\n",
    "3. `learning_rate`: o también llamado `eta`. Como el learning rate del GradientBoosting.\n",
    "4. `max_depth`: nada nuevo\n",
    "\n",
    "Si quieres afinar más todavía el XGBoost consulta [esta completa guía](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lightgbm.readthedocs.io/en/v3.3.2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
