{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cnIgSHlncJe"
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLI6MqjEncJe"
   },
   "source": [
    "Primero vamos a ver cómo funciona un word embedding de una forma simulada al tiempo que vemos como emplear la capa de embeddings de Keras, luego veremos como hacer sentence embedding utilizando un modulo o modelo preentrenado de Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vd0hPlPcncJe"
   },
   "outputs": [],
   "source": "# Importamos las librerías necesarias para trabajar con embeddings\n\n# NumPy: Biblioteca fundamental para computación científica y manejo de arrays\nimport numpy as np\n\n# TensorFlow: Framework de deep learning que usaremos para crear y entrenar modelos\n# También incluye Keras, una API de alto nivel para redes neuronales\nimport tensorflow as tf"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrGRy_IzncJf"
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu5Gw66CncJf"
   },
   "source": [
    "Recuerda que un word embedding transforma las palabras de un texto en un vector de n dimensiones. Veamos como hacerlo con una capa de embeddings, sin entrenar y así podrás ver como instanciarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPYFTiPxncJf"
   },
   "outputs": [],
   "source": "# ============================================================================\n# EJEMPLO DE WORD EMBEDDING CON KERAS\n# ============================================================================\n# Vamos a crear un vocabulario de ejemplo basado en una frase conocida\n# Cada palabra será convertida en un vector de números (embedding)\n\n# Vocabulario de ejemplo: frase de \"La princesa prometida\"\ncategorias_ejemplo = [\"Me\",\"llamo\",\"Iñigo\",\"Montoya\",\"soy\",\"tú\",\"mataste\",\"a\",\"mi\",\"padre\"]\n\n# PASO 1: CONVERTIR PALABRAS A ÍNDICES NUMÉRICOS\n# -----------------------------------------------\n# StringLookup es una capa de preprocesamiento que asigna un número único a cada palabra\n# Esto es necesario porque las redes neuronales trabajan con números, no con texto\npre_conversion = tf.keras.layers.StringLookup()\n\n# adapt() es como hacer un \"fit\" en sklearn: analiza el vocabulario y crea el mapeo\n# Después de esto, cada palabra tendrá asignado un número único\npre_conversion.adapt(categorias_ejemplo)\n\n# PASO 2: CREAR EL MODELO DE LOOKUP + EMBEDDING\n# ----------------------------------------------\n# Sequential permite encadenar capas de forma secuencial\nlookup_y_embedding = tf.keras.Sequential([\n    # Capa de entrada: recibe strings (palabras)\n    tf.keras.layers.InputLayer(shape=[], dtype=tf.string),\n    \n    # Capa 1: Convierte palabras en índices numéricos\n    pre_conversion,\n    \n    # Capa 2: Embedding - convierte índices en vectores densos\n    tf.keras.layers.Embedding(\n        input_dim = pre_conversion.vocabulary_size(),  # Tamaño del vocabulario (cuántas palabras únicas tenemos)\n        output_dim = 2  # Dimensión del embedding (convertiremos cada palabra en un vector de 2 números)\n    )\n])\n\n# NOTA IMPORTANTE: input_dim es el tamaño del vocabulario\n#                  output_dim es la dimensión del vector embedding resultante\n# En este caso, cada palabra se convertirá en un vector de 2 dimensiones [x, y]"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsQWwySOncJg"
   },
   "source": [
    "Este \"modelo\" no resuelve ningún tipo de problema solo pasa las palabras a traves de la capa de codificación y luego de la embeddings y genera por cadda palabra un vector de 2 dimensiones (output_dim). Pero además como no está entrenada funcionará porque tiene pesos inicializados de forma aleatoria. Es decir que si le pasamos como entrada la variable con la frase de ejemplo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FBaUhY9xncJg",
    "outputId": "3800053f-80b8-412f-b33b-02195b89cdd7"
   },
   "outputs": [],
   "source": "# VISUALIZACIÓN DEL PROCESO DE CODIFICACIÓN\n# ==========================================\n# Veamos cómo la capa StringLookup convierte nuestras palabras en índices numéricos\n# Esto es el primer paso antes de crear los embeddings\n\npre_conversion(categorias_ejemplo)\n\n# SALIDA ESPERADA: Un tensor con 10 números (uno por cada palabra)\n# Cada número es el índice único asignado a esa palabra en el vocabulario\n# Por ejemplo: \"Me\" -> 9, \"llamo\" -> 6, \"Iñigo\" -> 10, etc."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydP_R2yVncJg",
    "outputId": "8d207485-9005-4491-d7cd-7038bbb12cc5"
   },
   "outputs": [],
   "source": "# APLICANDO EL EMBEDDING COMPLETO (LOOKUP + EMBEDDING)\n# =====================================================\n# Ahora pasamos nuestras palabras por todo el modelo secuencial:\n# 1. InputLayer recibe las palabras como strings\n# 2. StringLookup las convierte a índices\n# 3. Embedding convierte los índices en vectores de 2 dimensiones\n\nlookup_y_embedding(np.array(categorias_ejemplo))\n\n# SALIDA ESPERADA: Una matriz de forma (10, 2)\n# - 10 filas: una por cada palabra\n# - 2 columnas: las dos dimensiones del embedding\n# \n# Cada palabra ahora está representada por 2 números (ej: [-0.044, 0.034])\n# IMPORTANTE: Como la capa Embedding no está entrenada, los valores son ALEATORIOS\n# En un modelo real, estos valores se aprenderían durante el entrenamiento\n# para capturar el significado semántico de las palabras"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZM5s_vrncJh"
   },
   "source": [
    "Nos convierte cada palabra en un embedding (sin sentido)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K-cg9EEncJh"
   },
   "source": [
    "Otra forma de hacerlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBOo7GSuncJh",
    "outputId": "087eeede-e9ba-4310-cfcb-fc5c52c07739"
   },
   "outputs": [],
   "source": "# FORMA ALTERNATIVA: PROCESANDO UNA FRASE COMPLETA\n# =================================================\n# En lugar de pasar una lista de palabras, podemos procesar una frase directamente\n# usando split() para separar las palabras\n\nfrase = \"Me llamo Iñigo Montoya\"\n\n# split() divide la frase en palabras usando espacios como separador\n# [\"Me\", \"llamo\", \"Iñigo\", \"Montoya\"]\nlookup_y_embedding(np.array(frase.split()))\n\n# SALIDA: Matriz de (4, 2) - 4 palabras convertidas en vectores de 2 dimensiones\n# Observa que las palabras que aparecían en el vocabulario original\n# tendrán los mismos embeddings que antes (porque son las mismas palabras)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab_RKvewncJh"
   },
   "source": [
    "Para poder darle valor tendríamos que incluir nuestras dos capas (la codificadora y la de embedding) en un modelo con un objetivo determinado y la capa de embeddings se entrenaría para generar los embeddings que mejor se adapten al problema a solucionar con ese modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9m3u2BpncJh"
   },
   "source": [
    "### Sentences embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8x2B-KU7ncJi"
   },
   "source": [
    "Vamos a convertir una serie de frases en embeddings. En concreto de 50 dimensiones. Lo haremos utilizando un modelo preentrenado el nnlem-en-dim50 de Google. Internamente es un modelo word embeddings que convierte cada palabra en un embedding de 50 dimensiones y luego calcular el centroide de todos los vectores obtenidos para una frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1CmdGjwbncJi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QigAcHyUncJi",
    "outputId": "077595e0-2dcc-4719-b8a3-dbc00aebe123"
   },
   "outputs": [],
   "source": "# ============================================================================\n# SENTENCE EMBEDDINGS CON TENSORFLOW HUB\n# ============================================================================\n# Ahora vamos a usar un modelo PREENTRENADO para convertir frases completas en embeddings\n# A diferencia del ejemplo anterior, este modelo ya ha sido entrenado con millones de textos\n\n# Importamos TensorFlow Hub: repositorio de modelos preentrenados\nimport tensorflow_hub as hub\n\n# CARGANDO EL MODELO PREENTRENADO\n# --------------------------------\n# Modelo: NNLM (Neural Network Language Model) de Google\n# - \"en\": entrenado en inglés\n# - \"dim50\": genera embeddings de 50 dimensiones\n# - \"2\": versión 2 del modelo\n# \n# Este modelo funciona así:\n# 1. Toma cada palabra de la frase y la convierte en un embedding de 50 dimensiones\n# 2. Calcula el CENTROIDE (promedio) de todos los vectores de las palabras\n# 3. El resultado es un único vector de 50 dimensiones que representa toda la frase\nhub_layer = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n\n# PROBANDO EL MODELO CON DOS FRASES FAMOSAS DE SHAKESPEARE\n# ---------------------------------------------------------\n# \"To be\" y \"Not to be\" son frases del famoso soliloquio de Hamlet\nsentence_embeddings = hub_layer(tf.constant([\"To be\", \"Not to be\"]))\n\n# Mostramos los embeddings redondeados a 2 decimales para mejor legibilidad\nsentence_embeddings.numpy().round(2)\n\n# SALIDA ESPERADA: Matriz de (2, 50)\n# - 2 filas: una por cada frase\n# - 50 columnas: las 50 dimensiones del embedding\n# \n# VENTAJA: Estos embeddings SÍ tienen significado semántico porque el modelo\n# fue entrenado con grandes cantidades de texto. Frases similares tendrán\n# embeddings cercanos en el espacio vectorial de 50 dimensiones."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KX--_k_2ncJi"
   },
   "source": [
    "Probemos ahora algunas cosas como por ejemplo obtener la similitud entre sentencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjAIAW4LncJi"
   },
   "outputs": [],
   "source": "# CORPUS DE EJEMPLO: FRASES EN ESPAÑOL SOBRE DIFERENTES TEMAS\n# =============================================================\n# Vamos a probar el modelo con frases en español (aunque fue entrenado en inglés)\n# Tenemos dos categorías de frases:\n# - Frases 1 y 2: Sobre fútbol (Real Madrid y Barcelona)\n# - Frases 3 y 4: Sobre la guerra Rusia-Ucrania\n\nsentences = ['El Real Madrid lo tiene difícil para ganar al Manchester City.',\n             'El Barcelona puede clasificar frente al PSG, si se esfuerza.',\n             'Las tropas rusas han tomado Dubroknic.',\n             'El ejercito ucraniano se ha replegado']\n\n# OBJETIVO: Verificar si el modelo puede capturar la similitud semántica\n# incluso en español (idioma diferente al de entrenamiento)\n# - Las frases 1-2 deberían ser similares entre sí (ambas de fútbol)\n# - Las frases 3-4 deberían ser similares entre sí (ambas de guerra)\n# - Las frases de diferentes temas deberían ser menos similares"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1J1ND0q3ncJi"
   },
   "outputs": [],
   "source": "# GENERANDO EMBEDDINGS PARA NUESTRAS 4 FRASES EN ESPAÑOL\n# ========================================================\n# Convertimos cada frase en un vector de 50 dimensiones\n\nsentence_embeddings = hub_layer(tf.constant(sentences))\n\n# RESULTADO: Matriz de (4, 50)\n# - 4 frases → 4 vectores\n# - Cada vector tiene 50 dimensiones\n# \n# Ahora cada frase está representada en un espacio vectorial donde:\n# - Frases con significados similares estarán CERCA\n# - Frases con significados diferentes estarán LEJOS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYZ7uzsxncJi",
    "outputId": "a23f5c04-88a6-419c-b986-df6e961db9b6",
    "scrolled": true
   },
   "outputs": [],
   "source": "# ============================================================================\n# CALCULANDO SIMILITUD ENTRE TODAS LAS FRASES\n# ============================================================================\n# Vamos a comparar cada frase con todas las demás usando dos métricas:\n# 1. SIMILITUD DE COSENO: mide el ángulo entre vectores (valores entre -1 y 1)\n# 2. DISTANCIA EUCLIDIANA: mide la distancia \"directa\" entre vectores\n\n# Importamos las herramientas necesarias\nfrom sklearn.metrics.pairwise import cosine_similarity  # Para calcular similitud de coseno\nfrom itertools import combinations  # Para generar todas las combinaciones posibles\n\n# COMPARACIÓN DE TODAS LAS COMBINACIONES POSIBLES\n# -----------------------------------------------\n# combinations(lista, r=2) genera todos los pares posibles sin repetir\n# Por ejemplo: (1,2), (1,3), (1,4), (2,3), (2,4), (3,4)\n\nfor (frase1, vec1), (frase2, vec2) in combinations(zip(sentences, sentence_embeddings.numpy()), r=2):\n    # Calculamos dos métricas de similitud:\n    \n    # 1. SIMILITUD DE COSENO (valores cercanos a 1 = muy similares)\n    similitud_coseno = cosine_similarity([vec1], [vec2])\n    \n    # 2. DISTANCIA EUCLIDIANA (valores pequeños = muy similares)\n    distancia_euclidiana = np.linalg.norm(vec1 - vec2)\n    \n    # Mostramos los resultados\n    print(frase1, \"vs\", frase2, similitud_coseno, distancia_euclidiana)\n\n# INTERPRETACIÓN DE RESULTADOS:\n# -----------------------------\n# SIMILITUD DE COSENO:\n# - Valores cercanos a 1: frases muy similares\n# - Valores cercanos a 0: frases no relacionadas\n# - Valores cercanos a -1: frases opuestas\n#\n# DISTANCIA EUCLIDIANA:\n# - Valores pequeños: frases similares\n# - Valores grandes: frases diferentes\n#\n# ESPERADO: Las frases de fútbol (1-2) y las de guerra (3-4) deberían\n# tener mayor similitud entre sí que frases de temas diferentes"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP1EhY7GncJj"
   },
   "source": [
    "Con el coseno tendríamos algún problema con la distancia quedan mejor emparejadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39zhUJ1yncJj",
    "outputId": "821c2177-626e-4931-f674-def05a3f2003"
   },
   "outputs": [],
   "source": "# ============================================================================\n# SISTEMA DE PREGUNTAS Y RESPUESTAS CON EMBEDDINGS\n# ============================================================================\n# Vamos a crear un sistema básico de Q&A que encuentra la respuesta más relevante\n# para una pregunta dada, basándose en la similitud de embeddings\n\n# PREGUNTA 1: Sobre el Barcelona\n# --------------------------------\nquestion = \"¿Contra quién juega el Barcelona?\"\n\n# Paso 1: Convertir la pregunta en un embedding de 50 dimensiones\npregunta = hub_layer(tf.constant([question]))\nvec_q = pregunta.numpy()  # Convertimos a numpy array para facilitar cálculos\n\n# Paso 2: Calcular distancia entre la pregunta y cada frase candidata\ndistancias = []  # Almacenará las distancias\nrespuestas = []  # Almacenará las frases candidatas\n\nfor answer, vec_a in zip(sentences, sentence_embeddings.numpy()):\n    respuestas.append(answer)\n    \n    # Calculamos distancia euclidiana entre embedding de pregunta y respuesta\n    # Menor distancia = mayor similitud semántica\n    distancias.append(np.linalg.norm(vec_q - vec_a))\n\n# Paso 3: Encontrar la respuesta con menor distancia (más similar)\n# np.argmin() devuelve el índice del valor mínimo\nindice_mejor_respuesta = np.argmin(distancias)\n\n# Mostramos pregunta y respuesta\nprint(f\"P:{question}\")\nprint(f\"R:{respuestas[indice_mejor_respuesta]}\")\n\n# FUNCIONAMIENTO:\n# ---------------\n# El modelo encuentra que \"El Barcelona puede clasificar frente al PSG...\"\n# es la frase más similar a la pregunta porque:\n# - Ambas contienen la palabra \"Barcelona\"\n# - Ambas tienen contexto de fútbol\n# - Los embeddings capturan esta similitud semántica"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxznFLCgncJj",
    "outputId": "871422b6-8b93-4b35-8164-1e532bcb006f"
   },
   "outputs": [],
   "source": "# PREGUNTA 2: Sobre Ucrania\n# --------------------------\n# Probamos el mismo sistema con una pregunta sobre tema militar\n\nquestion = \"¿Qué hacen los ucranianos?\"\n\n# Convertimos la pregunta en embedding\npregunta = hub_layer(tf.constant([question]))\nvec_q = pregunta.numpy()\n\n# Calculamos distancias con todas las frases candidatas\ndistancias = []\nrespuestas = []\n\nfor answer, vec_a in zip(sentences, sentence_embeddings.numpy()):\n    respuestas.append(answer)\n    distancias.append(np.linalg.norm(vec_q - vec_a))\n\n# Encontramos la frase más similar\nprint(f\"P:{question}\")\nprint(f\"R:{respuestas[np.argmin(distancias)]}\")\n\n# RESULTADO ESPERADO:\n# -------------------\n# Debería devolver \"El ejercito ucraniano se ha replegado\"\n# porque es la única frase que menciona acciones de los ucranianos\n# \n# NOTA: El embedding captura que \"ucranianos\" y \"ucraniano\" son similares\n# incluso con diferente forma (plural vs singular)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yr390r2qncJj",
    "outputId": "ed4c9599-5fe9-4539-96ca-33f3c7fa9af7"
   },
   "outputs": [],
   "source": "# PREGUNTA 3: Sobre una ubicación específica\n# -------------------------------------------\n# Probamos con una pregunta sobre un lugar específico (Dubrovnik)\n\nquestion = \"¿Qué han pasado en Dubrocnick?\"\n\n# Convertimos la pregunta en embedding\npregunta = hub_layer(tf.constant([question]))\nvec_q = pregunta.numpy()\n\n# Calculamos distancias con todas las frases candidatas\ndistancias = []\nrespuestas = []\n\nfor answer, vec_a in zip(sentences, sentence_embeddings.numpy()):\n    respuestas.append(answer)\n    distancias.append(np.linalg.norm(vec_q - vec_a))\n\n# Encontramos la frase más similar\nprint(f\"P:{question}\")\nprint(f\"R:{respuestas[np.argmin(distancias)]}\")\n\n# RESULTADO ESPERADO:\n# -------------------\n# Debería devolver \"Las tropas rusas han tomado Dubroknic.\"\n# \n# ASPECTO INTERESANTE:\n# --------------------\n# Observa que en la pregunta escribimos \"Dubrocnick\" (con error ortográfico)\n# pero el sistema encuentra la respuesta correcta con \"Dubroknic\"\n# Esto demuestra la ROBUSTEZ de los embeddings ante pequeñas variaciones\n# en la escritura, porque capturan el significado semántico general\n# \n# APLICACIONES PRÁCTICAS:\n# -----------------------\n# Este tipo de sistema se usa en:\n# - Chatbots que buscan respuestas en una base de conocimiento\n# - Buscadores semánticos que encuentran documentos relevantes\n# - Sistemas de recomendación de contenido similar\n# - FAQ automáticos que encuentran la pregunta frecuente más parecida"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bdYv5T6fncJj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}