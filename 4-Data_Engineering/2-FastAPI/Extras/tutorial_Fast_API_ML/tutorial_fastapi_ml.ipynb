{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Completo: FastAPI para Modelos de Machine Learning\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este tutorial aprenderás a crear una API REST completa usando **FastAPI** para servir modelos de Machine Learning. FastAPI es un framework moderno y rápido para construir APIs con Python 3.7+.\n",
    "\n",
    "### ¿Qué vamos a construir?\n",
    "\n",
    "Crearemos una aplicación que incluye:\n",
    "1. **Endpoint básico** - Para verificar que la API funciona\n",
    "2. **Endpoint de predicción** - Para hacer inferencias con nuestro modelo\n",
    "3. **Endpoint de monitorización** - Para ver métricas y estado del modelo\n",
    "4. **Endpoint de reentrenamiento** - Para actualizar el modelo con nuevos datos\n",
    "\n",
    "### Requisitos previos\n",
    "\n",
    "- Python 3.7+\n",
    "- Conocimientos básicos de Machine Learning\n",
    "- Familiaridad con APIs REST (recomendado)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n",
    "\n",
    "Primero necesitamos instalar todas las bibliotecas necesarias:\n",
    "\n",
    "- **fastapi**: Framework para crear la API\n",
    "- **uvicorn**: Servidor ASGI para ejecutar FastAPI\n",
    "- **tensorflow**: Para crear y usar modelos de deep learning\n",
    "- **numpy**: Para manipulación de arrays\n",
    "- **pandas**: Para manejo de datos\n",
    "- **scikit-learn**: Para métricas y preprocesamiento\n",
    "- **pydantic**: Para validación de datos (incluido con FastAPI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de todas las dependencias necesarias\n",
    "# Ejecuta esta celda una sola vez\n",
    "!pip install fastapi uvicorn tensorflow numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importación de Bibliotecas\n",
    "\n",
    "Importamos todas las bibliotecas que usaremos en el tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Todas las bibliotecas importadas correctamente\n",
      "Versión de TensorFlow: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Importaciones para FastAPI y servidor\n",
    "from fastapi import FastAPI, HTTPException, File, UploadFile\n",
    "from pydantic import BaseModel, Field\n",
    "import uvicorn\n",
    "\n",
    "# Importaciones para Machine Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Importaciones para utilidades\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import pickle\n",
    "\n",
    "print(\"✓ Todas las bibliotecas importadas correctamente\")\n",
    "print(f\"Versión de TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creación y Guardado de un Modelo de Ejemplo\n",
    "\n",
    "### 3.1 ¿Por qué necesitamos guardar modelos?\n",
    "\n",
    "Cuando entrenamos un modelo de Machine Learning, queremos poder reutilizarlo sin tener que reentrenarlo cada vez. Keras ofrece dos formatos principales:\n",
    "\n",
    "- **.keras** (recomendado): Formato nativo de Keras 3.0+, guarda todo en un solo archivo\n",
    "- **.h5**: Formato HDF5, compatible con versiones anteriores\n",
    "\n",
    "### 3.2 Creación de un dataset sintético\n",
    "\n",
    "Para este tutorial, crearemos un problema de regresión simple: predecir el precio de una casa basándose en características como tamaño, número de habitaciones, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset creado:\n",
      "  - Número de muestras: 1000\n",
      "  - Número de características: 4\n",
      "  - Rango de precios: 32381€ - 449305€\n",
      "\n",
      "Primeras 5 muestras:\n",
      "Tamaño     Habit.   Baños    Antigüedad   Precio    \n",
      "143.6      4        3        31.2         277667    €\n",
      "287.7      3        2        28.5         397144    €\n",
      "233.0      5        3        31.0         373740    €\n",
      "199.7      1        2        10.1         227036    €\n",
      "89.0       5        1        19.8         233867    €\n"
     ]
    }
   ],
   "source": [
    "# Configuración de semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Generación de datos sintéticos para predicción de precios de casas\n",
    "# Características: tamaño (m2), habitaciones, baños, antigüedad (años)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generamos características aleatorias\n",
    "tamano = np.random.uniform(50, 300, n_samples)  # Tamaño entre 50 y 300 m2\n",
    "habitaciones = np.random.randint(1, 6, n_samples)  # Entre 1 y 5 habitaciones\n",
    "banos = np.random.randint(1, 4, n_samples)  # Entre 1 y 3 baños\n",
    "antiguedad = np.random.uniform(0, 50, n_samples)  # Antigüedad entre 0 y 50 años\n",
    "\n",
    "# Creamos la variable objetivo (precio) con una fórmula lógica + ruido\n",
    "# Precio base: 1000€/m2, +20000€ por habitación, +15000€ por baño, -500€ por año de antigüedad\n",
    "precio = (\n",
    "    tamano * 1000 + \n",
    "    habitaciones * 20000 + \n",
    "    banos * 15000 - \n",
    "    antiguedad * 500 +\n",
    "    np.random.normal(0, 20000, n_samples)  # Añadimos ruido\n",
    ")\n",
    "\n",
    "# Combinamos todas las características en una matriz\n",
    "X = np.column_stack([tamano, habitaciones, banos, antiguedad])\n",
    "y = precio\n",
    "\n",
    "print(f\"Dataset creado:\")\n",
    "print(f\"  - Número de muestras: {n_samples}\")\n",
    "print(f\"  - Número de características: {X.shape[1]}\")\n",
    "print(f\"  - Rango de precios: {y.min():.0f}€ - {y.max():.0f}€\")\n",
    "print(f\"\\nPrimeras 5 muestras:\")\n",
    "print(f\"{'Tamaño':<10} {'Habit.':<8} {'Baños':<8} {'Antigüedad':<12} {'Precio':<10}\")\n",
    "for i in range(5):\n",
    "    print(f\"{X[i,0]:<10.1f} {X[i,1]:<8.0f} {X[i,2]:<8.0f} {X[i,3]:<12.1f} {y[i]:<10.0f}€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Preparación de los datos\n",
    "\n",
    "Antes de entrenar, debemos:\n",
    "1. Dividir los datos en entrenamiento y prueba\n",
    "2. Normalizar las características (importante para redes neuronales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Datos divididos:\n",
      "  - Training set: 800 muestras\n",
      "  - Test set: 200 muestras\n",
      "\n",
      "✓ Scaler guardado en 'scaler.pkl'\n"
     ]
    }
   ],
   "source": [
    "# División de datos en entrenamiento (80%) y prueba (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  # 20% para prueba\n",
    "    random_state=42  # Para reproducibilidad\n",
    ")\n",
    "\n",
    "# Normalización de características usando StandardScaler\n",
    "# Esto convierte los datos para que tengan media=0 y desviación estándar=1\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Ajustamos y transformamos training\n",
    "X_test_scaled = scaler.transform(X_test)  # Solo transformamos test (sin fit)\n",
    "\n",
    "# Guardamos el scaler para usarlo después en las predicciones\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(f\"✓ Datos divididos:\")\n",
    "print(f\"  - Training set: {X_train.shape[0]} muestras\")\n",
    "print(f\"  - Test set: {X_test.shape[0]} muestras\")\n",
    "print(f\"\\n✓ Scaler guardado en 'scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Creación del Modelo\n",
    "\n",
    "Crearemos una red neuronal simple con:\n",
    "- Capa de entrada: 4 características\n",
    "- 2 capas ocultas con activación ReLU\n",
    "- Capa de salida: 1 neurona (predicción del precio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquitectura del modelo:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ capa_oculta_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa_oculta_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ salida (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ capa_oculta_1 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ capa_oculta_2 (\u001b[38;5;33mDense\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ salida (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,433</span> (9.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,433\u001b[0m (9.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,433</span> (9.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,433\u001b[0m (9.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construcción del modelo usando la API Sequential de Keras\n",
    "modelo = models.Sequential([\n",
    "    # Capa de entrada: especificamos el número de características\n",
    "    layers.Input(shape=(4,)),  # 4 características\n",
    "    \n",
    "    # Primera capa oculta: 64 neuronas con activación ReLU\n",
    "    layers.Dense(64, activation='relu', name='capa_oculta_1'),\n",
    "    \n",
    "    # Segunda capa oculta: 32 neuronas con activación ReLU\n",
    "    layers.Dense(32, activation='relu', name='capa_oculta_2'),\n",
    "    \n",
    "    # Capa de salida: 1 neurona (predicción del precio)\n",
    "    # Sin activación porque es un problema de regresión\n",
    "    layers.Dense(1, name='salida')\n",
    "])\n",
    "\n",
    "# Compilación del modelo\n",
    "# - Optimizer: Adam (buen optimizer por defecto)\n",
    "# - Loss: MSE (Mean Squared Error) para regresión\n",
    "# - Metrics: MAE (Mean Absolute Error) para seguimiento\n",
    "modelo.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mean_absolute_error']\n",
    ")\n",
    "\n",
    "# Mostramos la arquitectura del modelo\n",
    "print(\"Arquitectura del modelo:\\n\")\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Entrenamiento del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando el modelo...\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 69756403712.0000 - mean_absolute_error: 251284.2500 - val_loss: 71279558656.0000 - val_mean_absolute_error: 253447.0938\n",
      "Epoch 2/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69755838464.0000 - mean_absolute_error: 251283.1250 - val_loss: 71278862336.0000 - val_mean_absolute_error: 253445.7500\n",
      "Epoch 3/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69754945536.0000 - mean_absolute_error: 251281.4062 - val_loss: 71277740032.0000 - val_mean_absolute_error: 253443.5469\n",
      "Epoch 4/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69753520128.0000 - mean_absolute_error: 251278.6562 - val_loss: 71275970560.0000 - val_mean_absolute_error: 253440.1562\n",
      "Epoch 5/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69751316480.0000 - mean_absolute_error: 251274.3438 - val_loss: 71273250816.0000 - val_mean_absolute_error: 253434.9531\n",
      "Epoch 6/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69747965952.0000 - mean_absolute_error: 251267.9688 - val_loss: 71269195776.0000 - val_mean_absolute_error: 253427.3281\n",
      "Epoch 7/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69743157248.0000 - mean_absolute_error: 251258.8438 - val_loss: 71263535104.0000 - val_mean_absolute_error: 253416.7812\n",
      "Epoch 8/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69736554496.0000 - mean_absolute_error: 251246.5000 - val_loss: 71255932928.0000 - val_mean_absolute_error: 253402.7500\n",
      "Epoch 9/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69727846400.0000 - mean_absolute_error: 251230.2500 - val_loss: 71246036992.0000 - val_mean_absolute_error: 253384.5938\n",
      "Epoch 10/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69716606976.0000 - mean_absolute_error: 251209.4688 - val_loss: 71233429504.0000 - val_mean_absolute_error: 253361.7031\n",
      "Epoch 11/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69702483968.0000 - mean_absolute_error: 251183.5781 - val_loss: 71217782784.0000 - val_mean_absolute_error: 253333.5000\n",
      "Epoch 12/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69685157888.0000 - mean_absolute_error: 251151.9062 - val_loss: 71198728192.0000 - val_mean_absolute_error: 253299.3438\n",
      "Epoch 13/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69664260096.0000 - mean_absolute_error: 251113.9531 - val_loss: 71175921664.0000 - val_mean_absolute_error: 253258.7500\n",
      "Epoch 14/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69639430144.0000 - mean_absolute_error: 251069.0938 - val_loss: 71149051904.0000 - val_mean_absolute_error: 253211.1562\n",
      "Epoch 15/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69610389504.0000 - mean_absolute_error: 251016.8438 - val_loss: 71117783040.0000 - val_mean_absolute_error: 253156.0781\n",
      "Epoch 16/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69576785920.0000 - mean_absolute_error: 250956.6562 - val_loss: 71081771008.0000 - val_mean_absolute_error: 253092.9062\n",
      "Epoch 17/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69538299904.0000 - mean_absolute_error: 250888.0000 - val_loss: 71040729088.0000 - val_mean_absolute_error: 253021.2188\n",
      "Epoch 18/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69494628352.0000 - mean_absolute_error: 250810.3438 - val_loss: 70994337792.0000 - val_mean_absolute_error: 252940.5000\n",
      "Epoch 19/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69445459968.0000 - mean_absolute_error: 250723.2031 - val_loss: 70942310400.0000 - val_mean_absolute_error: 252850.3438\n",
      "Epoch 20/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69390499840.0000 - mean_absolute_error: 250626.1250 - val_loss: 70884343808.0000 - val_mean_absolute_error: 252750.2031\n",
      "Epoch 21/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69329469440.0000 - mean_absolute_error: 250518.5469 - val_loss: 70820118528.0000 - val_mean_absolute_error: 252639.5938\n",
      "Epoch 22/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 69262065664.0000 - mean_absolute_error: 250400.0469 - val_loss: 70749405184.0000 - val_mean_absolute_error: 252518.0938\n",
      "Epoch 23/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69188050944.0000 - mean_absolute_error: 250270.2500 - val_loss: 70671925248.0000 - val_mean_absolute_error: 252385.3281\n",
      "Epoch 24/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69107171328.0000 - mean_absolute_error: 250128.5781 - val_loss: 70587383808.0000 - val_mean_absolute_error: 252240.7031\n",
      "Epoch 25/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69019181056.0000 - mean_absolute_error: 249974.7188 - val_loss: 70495526912.0000 - val_mean_absolute_error: 252083.9062\n",
      "Epoch 26/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68923842560.0000 - mean_absolute_error: 249808.2969 - val_loss: 70396174336.0000 - val_mean_absolute_error: 251914.5312\n",
      "Epoch 27/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68820942848.0000 - mean_absolute_error: 249628.8438 - val_loss: 70289063936.0000 - val_mean_absolute_error: 251732.2031\n",
      "Epoch 28/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68710223872.0000 - mean_absolute_error: 249435.9688 - val_loss: 70174007296.0000 - val_mean_absolute_error: 251536.6250\n",
      "Epoch 29/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68591484928.0000 - mean_absolute_error: 249229.3281 - val_loss: 70050807808.0000 - val_mean_absolute_error: 251327.4062\n",
      "Epoch 30/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68464533504.0000 - mean_absolute_error: 249008.5000 - val_loss: 69919219712.0000 - val_mean_absolute_error: 251104.1562\n",
      "Epoch 31/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68329177088.0000 - mean_absolute_error: 248773.2500 - val_loss: 69779087360.0000 - val_mean_absolute_error: 250866.5469\n",
      "Epoch 32/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68185235456.0000 - mean_absolute_error: 248523.0938 - val_loss: 69630222336.0000 - val_mean_absolute_error: 250614.2969\n",
      "Epoch 33/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 68032487424.0000 - mean_absolute_error: 248257.7812 - val_loss: 69472452608.0000 - val_mean_absolute_error: 250347.0469\n",
      "Epoch 34/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67870818304.0000 - mean_absolute_error: 247977.0000 - val_loss: 69305647104.0000 - val_mean_absolute_error: 250064.5469\n",
      "Epoch 35/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67700088832.0000 - mean_absolute_error: 247680.4062 - val_loss: 69129682944.0000 - val_mean_absolute_error: 249766.5000\n",
      "Epoch 36/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67520176128.0000 - mean_absolute_error: 247367.7031 - val_loss: 68944404480.0000 - val_mean_absolute_error: 249452.6562\n",
      "Epoch 37/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67330920448.0000 - mean_absolute_error: 247038.6562 - val_loss: 68749697024.0000 - val_mean_absolute_error: 249122.6719\n",
      "Epoch 38/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 67132186624.0000 - mean_absolute_error: 246692.9688 - val_loss: 68545388544.0000 - val_mean_absolute_error: 248776.2969\n",
      "Epoch 39/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66923847680.0000 - mean_absolute_error: 246330.3438 - val_loss: 68331372544.0000 - val_mean_absolute_error: 248413.2969\n",
      "Epoch 40/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66705829888.0000 - mean_absolute_error: 245950.5469 - val_loss: 68107579392.0000 - val_mean_absolute_error: 248033.4062\n",
      "Epoch 41/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66478026752.0000 - mean_absolute_error: 245553.2969 - val_loss: 67873890304.0000 - val_mean_absolute_error: 247636.3438\n",
      "Epoch 42/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66240356352.0000 - mean_absolute_error: 245138.4531 - val_loss: 67630276608.0000 - val_mean_absolute_error: 247222.0469\n",
      "Epoch 43/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65992732672.0000 - mean_absolute_error: 244705.7812 - val_loss: 67376652288.0000 - val_mean_absolute_error: 246790.2500\n",
      "Epoch 44/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65735143424.0000 - mean_absolute_error: 244254.9688 - val_loss: 67112939520.0000 - val_mean_absolute_error: 246340.7031\n",
      "Epoch 45/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65467502592.0000 - mean_absolute_error: 243785.9688 - val_loss: 66839064576.0000 - val_mean_absolute_error: 245873.1562\n",
      "Epoch 46/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65189748736.0000 - mean_absolute_error: 243298.5000 - val_loss: 66555031552.0000 - val_mean_absolute_error: 245387.5469\n",
      "Epoch 47/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64901861376.0000 - mean_absolute_error: 242792.3750 - val_loss: 66260819968.0000 - val_mean_absolute_error: 244883.7500\n",
      "Epoch 48/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64603815936.0000 - mean_absolute_error: 242267.4062 - val_loss: 65956401152.0000 - val_mean_absolute_error: 244361.5312\n",
      "Epoch 49/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64295567360.0000 - mean_absolute_error: 241723.5000 - val_loss: 65641762816.0000 - val_mean_absolute_error: 243820.7500\n",
      "Epoch 50/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63977160704.0000 - mean_absolute_error: 241160.5469 - val_loss: 65316933632.0000 - val_mean_absolute_error: 243261.2188\n",
      "Epoch 51/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63648628736.0000 - mean_absolute_error: 240578.2812 - val_loss: 64981893120.0000 - val_mean_absolute_error: 242682.7969\n",
      "Epoch 52/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63309946880.0000 - mean_absolute_error: 239976.7031 - val_loss: 64636661760.0000 - val_mean_absolute_error: 242085.4062\n",
      "Epoch 53/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62961156096.0000 - mean_absolute_error: 239355.6250 - val_loss: 64281321472.0000 - val_mean_absolute_error: 241468.9531\n",
      "Epoch 54/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62602305536.0000 - mean_absolute_error: 238714.9219 - val_loss: 63915896832.0000 - val_mean_absolute_error: 240833.3438\n",
      "Epoch 55/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62233423872.0000 - mean_absolute_error: 238054.5312 - val_loss: 63540436992.0000 - val_mean_absolute_error: 240178.4688\n",
      "Epoch 56/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61854535680.0000 - mean_absolute_error: 237374.3281 - val_loss: 63154987008.0000 - val_mean_absolute_error: 239504.2969\n",
      "Epoch 57/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61465722880.0000 - mean_absolute_error: 236674.2500 - val_loss: 62759620608.0000 - val_mean_absolute_error: 238810.7031\n",
      "Epoch 58/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61067079680.0000 - mean_absolute_error: 235954.2188 - val_loss: 62354415616.0000 - val_mean_absolute_error: 238097.5781\n",
      "Epoch 59/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 60658692096.0000 - mean_absolute_error: 235214.1562 - val_loss: 61939437568.0000 - val_mean_absolute_error: 237364.8750\n",
      "Epoch 60/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60240601088.0000 - mean_absolute_error: 234454.0000 - val_loss: 61514764288.0000 - val_mean_absolute_error: 236612.5312\n",
      "Epoch 61/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 59812917248.0000 - mean_absolute_error: 233673.7500 - val_loss: 61080526848.0000 - val_mean_absolute_error: 235840.5781\n",
      "Epoch 62/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 59375771648.0000 - mean_absolute_error: 232873.2969 - val_loss: 60636856320.0000 - val_mean_absolute_error: 235048.9688\n",
      "Epoch 63/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58929274880.0000 - mean_absolute_error: 232052.6562 - val_loss: 60183883776.0000 - val_mean_absolute_error: 234237.7031\n",
      "Epoch 64/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 58473558016.0000 - mean_absolute_error: 231211.7812 - val_loss: 59721732096.0000 - val_mean_absolute_error: 233406.6719\n",
      "Epoch 65/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58008764416.0000 - mean_absolute_error: 230350.6562 - val_loss: 59250532352.0000 - val_mean_absolute_error: 232555.8750\n",
      "Epoch 66/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57535037440.0000 - mean_absolute_error: 229469.3750 - val_loss: 58770456576.0000 - val_mean_absolute_error: 231685.2969\n",
      "Epoch 67/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57052504064.0000 - mean_absolute_error: 228567.7969 - val_loss: 58281656320.0000 - val_mean_absolute_error: 230795.0469\n",
      "Epoch 68/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56561315840.0000 - mean_absolute_error: 227645.9531 - val_loss: 57784324096.0000 - val_mean_absolute_error: 229885.0938\n",
      "Epoch 69/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56061698048.0000 - mean_absolute_error: 226703.9531 - val_loss: 57278619648.0000 - val_mean_absolute_error: 228955.4688\n",
      "Epoch 70/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55553794048.0000 - mean_absolute_error: 225741.7188 - val_loss: 56764674048.0000 - val_mean_absolute_error: 228006.1562\n",
      "Epoch 71/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 55037775872.0000 - mean_absolute_error: 224759.3438 - val_loss: 56242700288.0000 - val_mean_absolute_error: 227037.1719\n",
      "Epoch 72/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 54513893376.0000 - mean_absolute_error: 223756.9531 - val_loss: 55712915456.0000 - val_mean_absolute_error: 226048.5938\n",
      "Epoch 73/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53982318592.0000 - mean_absolute_error: 222734.5000 - val_loss: 55175512064.0000 - val_mean_absolute_error: 225040.5469\n",
      "Epoch 74/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53443252224.0000 - mean_absolute_error: 221692.0781 - val_loss: 54630719488.0000 - val_mean_absolute_error: 224013.0000\n",
      "Epoch 75/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52896915456.0000 - mean_absolute_error: 220629.8281 - val_loss: 54078758912.0000 - val_mean_absolute_error: 222966.0781\n",
      "Epoch 76/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52343545856.0000 - mean_absolute_error: 219547.7500 - val_loss: 53519855616.0000 - val_mean_absolute_error: 221899.8438\n",
      "Epoch 77/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51783352320.0000 - mean_absolute_error: 218445.9062 - val_loss: 52954234880.0000 - val_mean_absolute_error: 220814.4219\n",
      "Epoch 78/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 51216576512.0000 - mean_absolute_error: 217324.4688 - val_loss: 52382150656.0000 - val_mean_absolute_error: 219709.8438\n",
      "Epoch 79/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 50643460096.0000 - mean_absolute_error: 216183.5469 - val_loss: 51803840512.0000 - val_mean_absolute_error: 218586.2812\n",
      "Epoch 80/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 50064244736.0000 - mean_absolute_error: 215023.2188 - val_loss: 51219587072.0000 - val_mean_absolute_error: 217443.9062\n",
      "Epoch 81/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 49479196672.0000 - mean_absolute_error: 213843.6562 - val_loss: 50629632000.0000 - val_mean_absolute_error: 216282.7812\n",
      "Epoch 82/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48888578048.0000 - mean_absolute_error: 212644.9688 - val_loss: 50034233344.0000 - val_mean_absolute_error: 215103.0000\n",
      "Epoch 83/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48292659200.0000 - mean_absolute_error: 211427.2500 - val_loss: 49433661440.0000 - val_mean_absolute_error: 213904.7812\n",
      "Epoch 84/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 47691702272.0000 - mean_absolute_error: 210190.7500 - val_loss: 48828166144.0000 - val_mean_absolute_error: 212688.1719\n",
      "Epoch 85/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 47085936640.0000 - mean_absolute_error: 208935.5312 - val_loss: 48218058752.0000 - val_mean_absolute_error: 211453.4688\n",
      "Epoch 86/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 46475694080.0000 - mean_absolute_error: 207661.8125 - val_loss: 47603646464.0000 - val_mean_absolute_error: 210200.7969\n",
      "Epoch 87/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 45861269504.0000 - mean_absolute_error: 206369.7969 - val_loss: 46985183232.0000 - val_mean_absolute_error: 208930.2812\n",
      "Epoch 88/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 45242933248.0000 - mean_absolute_error: 205059.6562 - val_loss: 46362963968.0000 - val_mean_absolute_error: 207642.1094\n",
      "Epoch 89/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 44621012992.0000 - mean_absolute_error: 203731.6094 - val_loss: 45737287680.0000 - val_mean_absolute_error: 206336.4531\n",
      "Epoch 90/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 43995779072.0000 - mean_absolute_error: 202385.7812 - val_loss: 45108473856.0000 - val_mean_absolute_error: 205013.5781\n",
      "Epoch 91/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 43367546880.0000 - mean_absolute_error: 201022.4688 - val_loss: 44476825600.0000 - val_mean_absolute_error: 203673.6562\n",
      "Epoch 92/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 42736615424.0000 - mean_absolute_error: 199641.8438 - val_loss: 43842650112.0000 - val_mean_absolute_error: 202316.9062\n",
      "Epoch 93/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 42103271424.0000 - mean_absolute_error: 198244.0938 - val_loss: 43206258688.0000 - val_mean_absolute_error: 200943.5938\n",
      "Epoch 94/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 41467817984.0000 - mean_absolute_error: 196829.5156 - val_loss: 42567950336.0000 - val_mean_absolute_error: 199553.9531\n",
      "Epoch 95/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 40830562304.0000 - mean_absolute_error: 195398.2969 - val_loss: 41928019968.0000 - val_mean_absolute_error: 198148.2188\n",
      "Epoch 96/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 40191823872.0000 - mean_absolute_error: 193950.7188 - val_loss: 41286803456.0000 - val_mean_absolute_error: 196726.5938\n",
      "Epoch 97/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 39551930368.0000 - mean_absolute_error: 192487.0938 - val_loss: 40644644864.0000 - val_mean_absolute_error: 195289.4531\n",
      "Epoch 98/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 38911172608.0000 - mean_absolute_error: 191007.6562 - val_loss: 40001818624.0000 - val_mean_absolute_error: 193836.9688\n",
      "Epoch 99/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 38269870080.0000 - mean_absolute_error: 189512.7188 - val_loss: 39358668800.0000 - val_mean_absolute_error: 192369.5312\n",
      "Epoch 100/100\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 37628354560.0000 - mean_absolute_error: 188002.5156 - val_loss: 38715518976.0000 - val_mean_absolute_error: 190887.4062\n",
      "\n",
      "✓ Entrenamiento completado\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo\n",
    "print(\"Entrenando el modelo...\\n\")\n",
    "\n",
    "history = modelo.fit(\n",
    "    X_train_scaled,  # Datos de entrada normalizados\n",
    "    y_train,  # Precios objetivo\n",
    "    epochs=100,  # Número de épocas\n",
    "    batch_size=32,  # Tamaño del batch\n",
    "    validation_split=0.2,  # 20% de training para validación\n",
    "    verbose=1  # Mostrar progreso\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas en el conjunto de prueba:\n",
      "  - MSE (Mean Squared Error): 37,153,426,918\n",
      "  - RMSE (Root Mean Squared Error): 192,752€\n",
      "  - MAE (Mean Absolute Error): 187,355€\n",
      "  - R² Score: -5.2438\n",
      "\n",
      "Interpretación: En promedio, nuestras predicciones se desvían ±187,355€ del precio real\n"
     ]
    }
   ],
   "source": [
    "# Evaluación en el conjunto de prueba\n",
    "test_loss, test_mae = modelo.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "# Hacemos predicciones para calcular más métricas\n",
    "y_pred = modelo.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "# Calculamos métricas adicionales\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Métricas en el conjunto de prueba:\")\n",
    "print(f\"  - MSE (Mean Squared Error): {mse:,.0f}\")\n",
    "print(f\"  - RMSE (Root Mean Squared Error): {rmse:,.0f}€\")\n",
    "print(f\"  - MAE (Mean Absolute Error): {mae:,.0f}€\")\n",
    "print(f\"  - R² Score: {r2:.4f}\")\n",
    "print(f\"\\nInterpretación: En promedio, nuestras predicciones se desvían ±{mae:,.0f}€ del precio real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Guardado del Modelo\n",
    "\n",
    "Ahora guardaremos el modelo en ambos formatos para que puedas ver cómo funciona cada uno:\n",
    "\n",
    "#### Formato .keras (Recomendado)\n",
    "- Formato nativo y moderno\n",
    "- Guarda todo: arquitectura, pesos, optimizer, etc.\n",
    "- Más eficiente y rápido\n",
    "\n",
    "#### Formato .h5 (Legado)\n",
    "- Formato HDF5\n",
    "- Compatible con versiones anteriores de Keras\n",
    "- Más común en proyectos antiguos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo guardado en formato .keras\n",
      "✓ Modelo guardado en formato .h5\n",
      "✓ Metadatos guardados en 'modelos/metadata.json'\n",
      "\n",
      "==================================================\n",
      "ARCHIVOS CREADOS:\n",
      "  📁 modelos/\n",
      "    📄 modelo_precios.keras  (formato recomendado)\n",
      "    📄 modelo_precios.h5     (formato legado)\n",
      "    📄 metadata.json         (información del modelo)\n",
      "  📄 scaler.pkl              (normalizador de datos)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Crear carpeta para modelos si no existe\n",
    "os.makedirs('modelos', exist_ok=True)\n",
    "\n",
    "# 1. Guardar en formato .keras (RECOMENDADO)\n",
    "modelo.save('modelos/modelo_precios.keras')\n",
    "print(\"✓ Modelo guardado en formato .keras\")\n",
    "\n",
    "# 2. Guardar en formato .h5 (compatibilidad)\n",
    "modelo.save('modelos/modelo_precios.h5')\n",
    "print(\"✓ Modelo guardado en formato .h5\")\n",
    "\n",
    "# 3. Guardar metadatos del modelo (útil para monitorización)\n",
    "metadata = {\n",
    "    'fecha_entrenamiento': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'metricas': {\n",
    "        'mse': float(mse),\n",
    "        'rmse': float(rmse),\n",
    "        'mae': float(mae),\n",
    "        'r2': float(r2)\n",
    "    },\n",
    "    'arquitectura': {\n",
    "        'capas': len(modelo.layers),\n",
    "        'parametros_totales': modelo.count_params()\n",
    "    },\n",
    "    'datos_entrenamiento': {\n",
    "        'n_muestras_train': len(X_train),\n",
    "        'n_muestras_test': len(X_test),\n",
    "        'n_caracteristicas': X_train.shape[1]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('modelos/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"✓ Metadatos guardados en 'modelos/metadata.json'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ARCHIVOS CREADOS:\")\n",
    "print(\"  📁 modelos/\")\n",
    "print(\"    📄 modelo_precios.keras  (formato recomendado)\")\n",
    "print(\"    📄 modelo_precios.h5     (formato legado)\")\n",
    "print(\"    📄 metadata.json         (información del modelo)\")\n",
    "print(\"  📄 scaler.pkl              (normalizador de datos)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Verificación: Carga del Modelo\n",
    "\n",
    "Vamos a verificar que podemos cargar correctamente el modelo guardado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo .keras cargado correctamente\n",
      "✓ Modelo .h5 cargado correctamente\n",
      "\n",
      "Comparación de predicciones (primeras 5 muestras):\n",
      "Original        Cargado .keras  Cargado .h5     Real           \n",
      "19420           19420           19420           200758         \n",
      "78198           78198           78198           301970         \n",
      "75496           75496           75496           273995         \n",
      "70698           70698           70698           220921         \n",
      "151371          151371          151371          359485         \n",
      "\n",
      "✓ Verificación completada: Los modelos cargados funcionan correctamente\n"
     ]
    }
   ],
   "source": [
    "# Cargar modelo desde formato .keras\n",
    "modelo_cargado_keras = keras.models.load_model('modelos/modelo_precios.keras')\n",
    "print(\"✓ Modelo .keras cargado correctamente\")\n",
    "\n",
    "# Cargar modelo desde formato .h5\n",
    "modelo_cargado_h5 = keras.models.load_model('modelos/modelo_precios.h5')\n",
    "print(\"✓ Modelo .h5 cargado correctamente\")\n",
    "\n",
    "# Verificar que las predicciones son idénticas\n",
    "pred_original = modelo.predict(X_test_scaled[:5], verbose=0)\n",
    "pred_keras = modelo_cargado_keras.predict(X_test_scaled[:5], verbose=0)\n",
    "pred_h5 = modelo_cargado_h5.predict(X_test_scaled[:5], verbose=0)\n",
    "\n",
    "print(\"\\nComparación de predicciones (primeras 5 muestras):\")\n",
    "print(f\"{'Original':<15} {'Cargado .keras':<15} {'Cargado .h5':<15} {'Real':<15}\")\n",
    "for i in range(5):\n",
    "    print(f\"{pred_original[i][0]:<15.0f} {pred_keras[i][0]:<15.0f} {pred_h5[i][0]:<15.0f} {y_test.iloc[i] if isinstance(y_test, pd.Series) else y_test[i]:<15.0f}\")\n",
    "\n",
    "print(\"\\n✓ Verificación completada: Los modelos cargados funcionan correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Creación de la API con FastAPI\n",
    "\n",
    "Ahora que tenemos nuestro modelo guardado, vamos a crear la API REST.\n",
    "\n",
    "### 4.1 Modelos de Datos con Pydantic\n",
    "\n",
    "Pydantic nos permite definir esquemas de datos con validación automática. Esto asegura que los datos que recibe nuestra API sean correctos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelos de datos definidos correctamente\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\borja\\AppData\\Local\\Temp\\ipykernel_27396\\3911377819.py:6: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  tamano: float = Field(..., description=\"Tamaño en m2\", gt=0, example=150.0)\n",
      "C:\\Users\\borja\\AppData\\Local\\Temp\\ipykernel_27396\\3911377819.py:7: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  habitaciones: int = Field(..., description=\"Número de habitaciones\", ge=1, le=10, example=3)\n",
      "C:\\Users\\borja\\AppData\\Local\\Temp\\ipykernel_27396\\3911377819.py:8: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  banos: int = Field(..., description=\"Número de baños\", ge=1, le=5, example=2)\n",
      "C:\\Users\\borja\\AppData\\Local\\Temp\\ipykernel_27396\\3911377819.py:9: PydanticDeprecatedSince20: Using extra keyword arguments on `Field` is deprecated and will be removed. Use `json_schema_extra` instead. (Extra keys: 'example'). Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  antiguedad: float = Field(..., description=\"Antigüedad en años\", ge=0, le=100, example=10.0)\n",
      "C:\\Users\\borja\\AppData\\Local\\Temp\\ipykernel_27396\\3911377819.py:4: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class DatosCasa(BaseModel):\n"
     ]
    }
   ],
   "source": [
    "# Modelos de datos para validación de requests/responses\n",
    "\n",
    "# Modelo para recibir datos de predicción\n",
    "class DatosCasa(BaseModel):\n",
    "    \"\"\"Datos de entrada para predecir el precio de una casa\"\"\"\n",
    "    tamano: float = Field(..., description=\"Tamaño en m2\", gt=0, example=150.0)\n",
    "    habitaciones: int = Field(..., description=\"Número de habitaciones\", ge=1, le=10, example=3)\n",
    "    banos: int = Field(..., description=\"Número de baños\", ge=1, le=5, example=2)\n",
    "    antiguedad: float = Field(..., description=\"Antigüedad en años\", ge=0, le=100, example=10.0)\n",
    "    \n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"tamano\": 150.0,\n",
    "                \"habitaciones\": 3,\n",
    "                \"banos\": 2,\n",
    "                \"antiguedad\": 10.0\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Modelo para respuesta de predicción\n",
    "class PrediccionRespuesta(BaseModel):\n",
    "    \"\"\"Respuesta con la predicción del precio\"\"\"\n",
    "    precio_predicho: float = Field(..., description=\"Precio predicho en euros\")\n",
    "    confianza: str = Field(..., description=\"Nivel de confianza de la predicción\")\n",
    "    timestamp: str = Field(..., description=\"Fecha y hora de la predicción\")\n",
    "\n",
    "# Modelo para datos de reentrenamiento\n",
    "class DatosReentrenamiento(BaseModel):\n",
    "    \"\"\"Datos para reentrenar el modelo\"\"\"\n",
    "    datos: List[DatosCasa] = Field(..., description=\"Lista de datos de casas\")\n",
    "    precios: List[float] = Field(..., description=\"Precios reales correspondientes\")\n",
    "    epochs: int = Field(default=50, description=\"Número de épocas para reentrenamiento\", ge=1, le=200)\n",
    "\n",
    "print(\"✓ Modelos de datos definidos correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Variables Globales y Carga Inicial\n",
    "\n",
    "Definimos variables globales para mantener el estado del modelo, métricas y scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Modelo .keras cargado\n",
      "✓ Scaler cargado\n",
      "✓ Metadata cargada\n",
      "\n",
      "✓ Sistema listo para usar\n"
     ]
    }
   ],
   "source": [
    "# Variables globales para la aplicación\n",
    "modelo_actual = None  # Modelo cargado en memoria\n",
    "scaler_actual = None  # Scaler para normalización\n",
    "metadata_actual = {}  # Metadatos del modelo\n",
    "historial_predicciones = []  # Historial para monitorización\n",
    "\n",
    "# Función para cargar el modelo y sus componentes\n",
    "def cargar_modelo():\n",
    "    \"\"\"\n",
    "    Carga el modelo, scaler y metadata desde disco.\n",
    "    Esta función se ejecuta al iniciar la aplicación.\n",
    "    \"\"\"\n",
    "    global modelo_actual, scaler_actual, metadata_actual\n",
    "    \n",
    "    try:\n",
    "        # Cargar el modelo (intentamos .keras primero, luego .h5)\n",
    "        if os.path.exists('modelos/modelo_precios.keras'):\n",
    "            modelo_actual = keras.models.load_model('modelos/modelo_precios.keras')\n",
    "            print(\"✓ Modelo .keras cargado\")\n",
    "        elif os.path.exists('modelos/modelo_precios.h5'):\n",
    "            modelo_actual = keras.models.load_model('modelos/modelo_precios.h5')\n",
    "            print(\"✓ Modelo .h5 cargado\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No se encontró ningún modelo guardado\")\n",
    "        \n",
    "        # Cargar el scaler\n",
    "        with open('scaler.pkl', 'rb') as f:\n",
    "            scaler_actual = pickle.load(f)\n",
    "        print(\"✓ Scaler cargado\")\n",
    "        \n",
    "        # Cargar metadata si existe\n",
    "        if os.path.exists('modelos/metadata.json'):\n",
    "            with open('modelos/metadata.json', 'r') as f:\n",
    "                metadata_actual = json.load(f)\n",
    "            print(\"✓ Metadata cargada\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al cargar el modelo: {e}\")\n",
    "        return False\n",
    "\n",
    "# Cargar el modelo al inicio\n",
    "if cargar_modelo():\n",
    "    print(\"\\n✓ Sistema listo para usar\")\n",
    "else:\n",
    "    print(\"\\n⚠️ El sistema no está completamente inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ENDPOINT 1: Endpoint Básico de Saludo\n",
    "\n",
    "Este es el endpoint más simple. Sirve para:\n",
    "- Verificar que la API está funcionando\n",
    "- Health check básico\n",
    "- Prueba de conectividad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Endpoints básicos creados\n",
      "  - GET /\n",
      "  - GET /health\n"
     ]
    }
   ],
   "source": [
    "# Creamos la aplicación FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"API de Predicción de Precios de Casas\",\n",
    "    description=\"API REST para servir modelos de ML con FastAPI\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# ENDPOINT 1: Ruta raíz - Saludo básico\n",
    "@app.get(\"/\", tags=[\"General\"])\n",
    "async def raiz():\n",
    "    \"\"\"\n",
    "    Endpoint básico de bienvenida.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mensaje de bienvenida y estado del sistema\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"mensaje\": \"¡Hola! Bienvenido a la API de predicción de precios de casas\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"status\": \"operativo\",\n",
    "        \"modelo_cargado\": modelo_actual is not None,\n",
    "        \"endpoints_disponibles\": [\n",
    "            \"/docs - Documentación interactiva\",\n",
    "            \"/predecir - Hacer predicciones\",\n",
    "            \"/monitorizar - Ver métricas del modelo\",\n",
    "            \"/reentrenar - Reentrenar el modelo\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# ENDPOINT 1.1: Health check\n",
    "@app.get(\"/health\", tags=[\"General\"])\n",
    "async def health_check():\n",
    "    \"\"\"\n",
    "    Verifica el estado de salud de la API.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Estado detallado del sistema\n",
    "    \"\"\"\n",
    "    # Verificamos todos los componentes\n",
    "    estado = {\n",
    "        \"status\": \"healthy\",\n",
    "        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"componentes\": {\n",
    "            \"modelo\": \"OK\" if modelo_actual is not None else \"ERROR\",\n",
    "            \"scaler\": \"OK\" if scaler_actual is not None else \"ERROR\",\n",
    "            \"metadata\": \"OK\" if metadata_actual else \"WARNING\"\n",
    "        },\n",
    "        \"estadisticas\": {\n",
    "            \"predicciones_realizadas\": len(historial_predicciones),\n",
    "            \"fecha_ultimo_entrenamiento\": metadata_actual.get('fecha_entrenamiento', 'Desconocida')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Si algún componente crítico falla, cambiamos el estado\n",
    "    if modelo_actual is None or scaler_actual is None:\n",
    "        estado[\"status\"] = \"unhealthy\"\n",
    "        raise HTTPException(status_code=503, detail=\"Servicio no disponible\")\n",
    "    \n",
    "    return estado\n",
    "\n",
    "print(\"✓ Endpoints básicos creados\")\n",
    "print(\"  - GET /\")\n",
    "print(\"  - GET /health\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ENDPOINT 2: Predicción con el Modelo\n",
    "\n",
    "Este es el endpoint principal de la API. Permite hacer predicciones usando el modelo cargado.\n",
    "\n",
    "### Flujo de predicción:\n",
    "1. Recibir datos de entrada (validados por Pydantic)\n",
    "2. Normalizar los datos con el scaler\n",
    "3. Hacer la predicción con el modelo\n",
    "4. Guardar en el historial para monitorización\n",
    "5. Retornar el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Endpoints de predicción creados\n",
      "  - POST /predecir\n",
      "  - POST /predecir/lote\n"
     ]
    }
   ],
   "source": [
    "# ENDPOINT 2: Predicción\n",
    "@app.post(\"/predecir\", response_model=PrediccionRespuesta, tags=[\"Predicción\"])\n",
    "async def predecir_precio(datos: DatosCasa):\n",
    "    \"\"\"\n",
    "    Predice el precio de una casa basándose en sus características.\n",
    "    \n",
    "    Args:\n",
    "        datos (DatosCasa): Características de la casa\n",
    "        \n",
    "    Returns:\n",
    "        PrediccionRespuesta: Precio predicho y metadatos\n",
    "        \n",
    "    Raises:\n",
    "        HTTPException: Si el modelo no está cargado o hay un error en la predicción\n",
    "    \"\"\"\n",
    "    # Verificar que el modelo esté cargado\n",
    "    if modelo_actual is None or scaler_actual is None:\n",
    "        raise HTTPException(\n",
    "            status_code=503,\n",
    "            detail=\"El modelo no está cargado. Por favor, reinicia el servidor.\"\n",
    "        )\n",
    "    \n",
    "    try:\n",
    "        # Paso 1: Preparar los datos de entrada\n",
    "        # Convertimos los datos de Pydantic a un array numpy\n",
    "        entrada = np.array([[\n",
    "            datos.tamano,\n",
    "            datos.habitaciones,\n",
    "            datos.banos,\n",
    "            datos.antiguedad\n",
    "        ]])\n",
    "        \n",
    "        # Paso 2: Normalizar los datos usando el scaler\n",
    "        # Es CRÍTICO usar el mismo scaler que se usó en el entrenamiento\n",
    "        entrada_normalizada = scaler_actual.transform(entrada)\n",
    "        \n",
    "        # Paso 3: Hacer la predicción\n",
    "        prediccion = modelo_actual.predict(entrada_normalizada, verbose=0)\n",
    "        precio_predicho = float(prediccion[0][0])\n",
    "        \n",
    "        # Paso 4: Calcular nivel de confianza (simplificado)\n",
    "        # En un sistema real, usaríamos intervalos de confianza o predicción\n",
    "        # Aquí usamos una heurística basada en el MAE del modelo\n",
    "        mae_modelo = metadata_actual.get('metricas', {}).get('mae', 20000)\n",
    "        error_relativo = (mae_modelo / precio_predicho) * 100\n",
    "        \n",
    "        if error_relativo < 10:\n",
    "            confianza = \"alta\"\n",
    "        elif error_relativo < 20:\n",
    "            confianza = \"media\"\n",
    "        else:\n",
    "            confianza = \"baja\"\n",
    "        \n",
    "        # Paso 5: Guardar en historial para monitorización\n",
    "        historial_predicciones.append({\n",
    "            \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"entrada\": {\n",
    "                \"tamano\": datos.tamano,\n",
    "                \"habitaciones\": datos.habitaciones,\n",
    "                \"banos\": datos.banos,\n",
    "                \"antiguedad\": datos.antiguedad\n",
    "            },\n",
    "            \"prediccion\": precio_predicho,\n",
    "            \"confianza\": confianza\n",
    "        })\n",
    "        \n",
    "        # Limitar el historial a las últimas 1000 predicciones\n",
    "        if len(historial_predicciones) > 1000:\n",
    "            historial_predicciones.pop(0)\n",
    "        \n",
    "        # Paso 6: Retornar la respuesta\n",
    "        return PrediccionRespuesta(\n",
    "            precio_predicho=round(precio_predicho, 2),\n",
    "            confianza=confianza,\n",
    "            timestamp=datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Manejo de errores\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error al realizar la predicción: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# ENDPOINT 2.1: Predicción por lotes (batch prediction)\n",
    "@app.post(\"/predecir/lote\", tags=[\"Predicción\"])\n",
    "async def predecir_lote(datos_lista: List[DatosCasa]):\n",
    "    \"\"\"\n",
    "    Predice precios para múltiples casas a la vez.\n",
    "    Más eficiente que hacer múltiples llamadas individuales.\n",
    "    \n",
    "    Args:\n",
    "        datos_lista (List[DatosCasa]): Lista de casas\n",
    "        \n",
    "    Returns:\n",
    "        dict: Lista de predicciones\n",
    "    \"\"\"\n",
    "    if modelo_actual is None or scaler_actual is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "    \n",
    "    try:\n",
    "        # Convertir todos los datos a un array numpy\n",
    "        entrada = np.array([\n",
    "            [d.tamano, d.habitaciones, d.banos, d.antiguedad]\n",
    "            for d in datos_lista\n",
    "        ])\n",
    "        \n",
    "        # Normalizar y predecir\n",
    "        entrada_normalizada = scaler_actual.transform(entrada)\n",
    "        predicciones = modelo_actual.predict(entrada_normalizada, verbose=0)\n",
    "        \n",
    "        # Formatear resultados\n",
    "        resultados = [\n",
    "            {\n",
    "                \"indice\": i,\n",
    "                \"entrada\": {\n",
    "                    \"tamano\": datos_lista[i].tamano,\n",
    "                    \"habitaciones\": datos_lista[i].habitaciones,\n",
    "                    \"banos\": datos_lista[i].banos,\n",
    "                    \"antiguedad\": datos_lista[i].antiguedad\n",
    "                },\n",
    "                \"precio_predicho\": round(float(predicciones[i][0]), 2)\n",
    "            }\n",
    "            for i in range(len(predicciones))\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"n_predicciones\": len(resultados),\n",
    "            \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"predicciones\": resultados\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error: {str(e)}\")\n",
    "\n",
    "print(\"✓ Endpoints de predicción creados\")\n",
    "print(\"  - POST /predecir\")\n",
    "print(\"  - POST /predecir/lote\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. ENDPOINT 3: Monitorización del Modelo\n",
    "\n",
    "La monitorización es crucial en producción. Este endpoint permite:\n",
    "- Ver métricas del modelo actual\n",
    "- Analizar el historial de predicciones\n",
    "- Detectar posibles problemas o degradación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Endpoints de monitorización creados\n",
      "  - GET /monitorizar\n",
      "  - GET /monitorizar/historial\n"
     ]
    }
   ],
   "source": [
    "# ENDPOINT 3: Monitorización\n",
    "@app.get(\"/monitorizar\", tags=[\"Monitorización\"])\n",
    "async def monitorizar_modelo():\n",
    "    \"\"\"\n",
    "    Devuelve métricas y estadísticas sobre el modelo en producción.\n",
    "    \n",
    "    Incluye:\n",
    "    - Métricas de entrenamiento\n",
    "    - Estadísticas de uso\n",
    "    - Información de la arquitectura\n",
    "    - Análisis del historial de predicciones\n",
    "    \n",
    "    Returns:\n",
    "        dict: Métricas completas del modelo\n",
    "    \"\"\"\n",
    "    if modelo_actual is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "    \n",
    "    # 1. Información básica del modelo\n",
    "    info_basica = {\n",
    "        \"nombre\": \"Modelo de Predicción de Precios de Casas\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"estado\": \"activo\",\n",
    "        \"fecha_carga\": metadata_actual.get('fecha_entrenamiento', 'Desconocida')\n",
    "    }\n",
    "    \n",
    "    # 2. Métricas de entrenamiento (del archivo metadata.json)\n",
    "    metricas_entrenamiento = metadata_actual.get('metricas', {\n",
    "        \"mse\": \"No disponible\",\n",
    "        \"rmse\": \"No disponible\",\n",
    "        \"mae\": \"No disponible\",\n",
    "        \"r2\": \"No disponible\"\n",
    "    })\n",
    "    \n",
    "    # 3. Información de la arquitectura\n",
    "    arquitectura = {\n",
    "        \"n_capas\": len(modelo_actual.layers),\n",
    "        \"parametros_totales\": modelo_actual.count_params(),\n",
    "        \"parametros_entrenables\": sum([tf.size(w).numpy() for w in modelo_actual.trainable_weights]),\n",
    "        \"capas_detalle\": [\n",
    "            {\n",
    "                \"nombre\": layer.name,\n",
    "                \"tipo\": layer.__class__.__name__,\n",
    "                \"forma_salida\": str(layer.output_shape)\n",
    "            }\n",
    "            for layer in modelo_actual.layers\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # 4. Estadísticas de uso (desde el inicio del servidor)\n",
    "    estadisticas_uso = {\n",
    "        \"total_predicciones\": len(historial_predicciones),\n",
    "        \"predicciones_ultima_hora\": sum(\n",
    "            1 for p in historial_predicciones\n",
    "            if (datetime.now() - datetime.strptime(p['timestamp'], '%Y-%m-%d %H:%M:%S')).seconds < 3600\n",
    "        ) if historial_predicciones else 0\n",
    "    }\n",
    "    \n",
    "    # 5. Análisis del historial de predicciones\n",
    "    if historial_predicciones:\n",
    "        precios_predichos = [p['prediccion'] for p in historial_predicciones]\n",
    "        distribucion_confianza = {\n",
    "            \"alta\": sum(1 for p in historial_predicciones if p['confianza'] == 'alta'),\n",
    "            \"media\": sum(1 for p in historial_predicciones if p['confianza'] == 'media'),\n",
    "            \"baja\": sum(1 for p in historial_predicciones if p['confianza'] == 'baja')\n",
    "        }\n",
    "        \n",
    "        analisis_predicciones = {\n",
    "            \"precio_promedio\": round(np.mean(precios_predichos), 2),\n",
    "            \"precio_mediano\": round(np.median(precios_predichos), 2),\n",
    "            \"precio_min\": round(np.min(precios_predichos), 2),\n",
    "            \"precio_max\": round(np.max(precios_predichos), 2),\n",
    "            \"desviacion_estandar\": round(np.std(precios_predichos), 2),\n",
    "            \"distribucion_confianza\": distribucion_confianza\n",
    "        }\n",
    "    else:\n",
    "        analisis_predicciones = {\n",
    "            \"mensaje\": \"No hay predicciones registradas aún\"\n",
    "        }\n",
    "    \n",
    "    # 6. Recomendaciones y alertas\n",
    "    recomendaciones = []\n",
    "    \n",
    "    # Alerta si hay muchas predicciones de baja confianza\n",
    "    if historial_predicciones:\n",
    "        pct_baja_confianza = (distribucion_confianza.get('baja', 0) / len(historial_predicciones)) * 100\n",
    "        if pct_baja_confianza > 30:\n",
    "            recomendaciones.append({\n",
    "                \"tipo\": \"warning\",\n",
    "                \"mensaje\": f\"Alto porcentaje de predicciones con baja confianza ({pct_baja_confianza:.1f}%). Considera reentrenar el modelo.\"\n",
    "            })\n",
    "    \n",
    "    # Alerta si el modelo es antiguo (ejemplo: más de 30 días)\n",
    "    # En producción real, compararías con la fecha actual\n",
    "    recomendaciones.append({\n",
    "        \"tipo\": \"info\",\n",
    "        \"mensaje\": \"Recomendación: Reentrena el modelo periódicamente con datos nuevos para mantener su precisión.\"\n",
    "    })\n",
    "    \n",
    "    # 7. Retornar todo el análisis\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"info_basica\": info_basica,\n",
    "        \"metricas_entrenamiento\": metricas_entrenamiento,\n",
    "        \"arquitectura\": arquitectura,\n",
    "        \"estadisticas_uso\": estadisticas_uso,\n",
    "        \"analisis_predicciones\": analisis_predicciones,\n",
    "        \"recomendaciones\": recomendaciones\n",
    "    }\n",
    "\n",
    "# ENDPOINT 3.1: Historial de predicciones\n",
    "@app.get(\"/monitorizar/historial\", tags=[\"Monitorización\"])\n",
    "async def obtener_historial(limite: int = 100):\n",
    "    \"\"\"\n",
    "    Devuelve las últimas N predicciones realizadas.\n",
    "    \n",
    "    Args:\n",
    "        limite (int): Número máximo de predicciones a devolver (default: 100)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Historial de predicciones\n",
    "    \"\"\"\n",
    "    # Limitar el número de resultados\n",
    "    limite = min(limite, 1000)  # Máximo 1000 predicciones\n",
    "    \n",
    "    return {\n",
    "        \"total_predicciones\": len(historial_predicciones),\n",
    "        \"limite\": limite,\n",
    "        \"predicciones\": historial_predicciones[-limite:]  # Últimas N predicciones\n",
    "    }\n",
    "\n",
    "print(\"✓ Endpoints de monitorización creados\")\n",
    "print(\"  - GET /monitorizar\")\n",
    "print(\"  - GET /monitorizar/historial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. ENDPOINT 4: Reentrenamiento del Modelo\n",
    "\n",
    "El reentrenamiento permite actualizar el modelo con nuevos datos sin tener que reconstruir toda la API.\n",
    "\n",
    "### Consideraciones importantes:\n",
    "- En producción, esto se haría de forma asíncrona (usando Celery, por ejemplo)\n",
    "- Se debería validar la calidad de los nuevos datos\n",
    "- Es recomendable hacer validación cruzada antes de actualizar el modelo en producción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Endpoints de reentrenamiento creados\n",
      "  - POST /reentrenar\n",
      "  - POST /reentrenar/validar\n"
     ]
    }
   ],
   "source": [
    "# ENDPOINT 4: Reentrenamiento\n",
    "@app.post(\"/reentrenar\", tags=[\"Reentrenamiento\"])\n",
    "async def reentrenar_modelo(datos_entrenamiento: DatosReentrenamiento):\n",
    "    \"\"\"\n",
    "    Reentrena el modelo con nuevos datos.\n",
    "    \n",
    "    IMPORTANTE: En producción, este proceso debería ser asíncrono y\n",
    "    realizarse en un worker separado (ej: Celery) para no bloquear la API.\n",
    "    \n",
    "    Args:\n",
    "        datos_entrenamiento (DatosReentrenamiento): Nuevos datos para entrenar\n",
    "        \n",
    "    Returns:\n",
    "        dict: Resultado del reentrenamiento con métricas\n",
    "        \n",
    "    Raises:\n",
    "        HTTPException: Si hay errores en los datos o el entrenamiento\n",
    "    \"\"\"\n",
    "    global modelo_actual, metadata_actual\n",
    "    \n",
    "    if modelo_actual is None or scaler_actual is None:\n",
    "        raise HTTPException(status_code=503, detail=\"Modelo no disponible\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Validar que tenemos suficientes datos\n",
    "        if len(datos_entrenamiento.datos) < 10:\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"Se necesitan al menos 10 muestras para reentrenar\"\n",
    "            )\n",
    "        \n",
    "        # 2. Validar que el número de datos coincide con el número de precios\n",
    "        if len(datos_entrenamiento.datos) != len(datos_entrenamiento.precios):\n",
    "            raise HTTPException(\n",
    "                status_code=400,\n",
    "                detail=\"El número de datos debe coincidir con el número de precios\"\n",
    "            )\n",
    "        \n",
    "        # 3. Preparar los datos de entrenamiento\n",
    "        # Convertir los datos de Pydantic a arrays numpy\n",
    "        X_nuevo = np.array([\n",
    "            [d.tamano, d.habitaciones, d.banos, d.antiguedad]\n",
    "            for d in datos_entrenamiento.datos\n",
    "        ])\n",
    "        y_nuevo = np.array(datos_entrenamiento.precios)\n",
    "        \n",
    "        # 4. Dividir en train/test para validación\n",
    "        X_train_nuevo, X_test_nuevo, y_train_nuevo, y_test_nuevo = train_test_split(\n",
    "            X_nuevo, y_nuevo,\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # 5. Normalizar los datos usando el scaler existente\n",
    "        # IMPORTANTE: Usamos transform(), NO fit_transform()\n",
    "        # para mantener la misma escala que el modelo original\n",
    "        X_train_scaled = scaler_actual.transform(X_train_nuevo)\n",
    "        X_test_scaled = scaler_actual.transform(X_test_nuevo)\n",
    "        \n",
    "        # 6. Guardar métricas del modelo antes del reentrenamiento\n",
    "        metricas_antes = metadata_actual.get('metricas', {})\n",
    "        \n",
    "        # 7. Reentrenar el modelo\n",
    "        # Usamos el modelo existente (transfer learning)\n",
    "        print(f\"Iniciando reentrenamiento con {len(X_train_nuevo)} muestras...\")\n",
    "        \n",
    "        history = modelo_actual.fit(\n",
    "            X_train_scaled,\n",
    "            y_train_nuevo,\n",
    "            epochs=datos_entrenamiento.epochs,\n",
    "            batch_size=min(32, len(X_train_nuevo) // 4),  # Ajustamos batch size\n",
    "            validation_split=0.2,\n",
    "            verbose=0  # Silencioso para no saturar logs\n",
    "        )\n",
    "        \n",
    "        # 8. Evaluar el modelo reentrenado\n",
    "        y_pred = modelo_actual.predict(X_test_scaled, verbose=0)\n",
    "        \n",
    "        # Calcular métricas\n",
    "        mse_nuevo = mean_squared_error(y_test_nuevo, y_pred)\n",
    "        rmse_nuevo = np.sqrt(mse_nuevo)\n",
    "        mae_nuevo = mean_absolute_error(y_test_nuevo, y_pred)\n",
    "        r2_nuevo = r2_score(y_test_nuevo, y_pred)\n",
    "        \n",
    "        metricas_despues = {\n",
    "            'mse': float(mse_nuevo),\n",
    "            'rmse': float(rmse_nuevo),\n",
    "            'mae': float(mae_nuevo),\n",
    "            'r2': float(r2_nuevo)\n",
    "        }\n",
    "        \n",
    "        # 9. Actualizar metadata\n",
    "        metadata_actual['fecha_entrenamiento'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        metadata_actual['metricas'] = metricas_despues\n",
    "        metadata_actual['datos_entrenamiento'] = {\n",
    "            'n_muestras_train': len(X_train_nuevo),\n",
    "            'n_muestras_test': len(X_test_nuevo),\n",
    "            'n_caracteristicas': X_nuevo.shape[1]\n",
    "        }\n",
    "        \n",
    "        # 10. Guardar el modelo reentrenado\n",
    "        modelo_actual.save('modelos/modelo_precios.keras')\n",
    "        with open('modelos/metadata.json', 'w') as f:\n",
    "            json.dump(metadata_actual, f, indent=2)\n",
    "        \n",
    "        # 11. Preparar respuesta con comparación\n",
    "        return {\n",
    "            \"status\": \"exitoso\",\n",
    "            \"timestamp\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"datos_utilizados\": {\n",
    "                \"total_muestras\": len(X_nuevo),\n",
    "                \"muestras_entrenamiento\": len(X_train_nuevo),\n",
    "                \"muestras_test\": len(X_test_nuevo),\n",
    "                \"epochs\": datos_entrenamiento.epochs\n",
    "            },\n",
    "            \"metricas_antes\": metricas_antes,\n",
    "            \"metricas_despues\": metricas_despues,\n",
    "            \"mejora\": {\n",
    "                \"mae\": f\"{((metricas_antes.get('mae', 0) - mae_nuevo) / metricas_antes.get('mae', 1)) * 100:.2f}%\" if metricas_antes.get('mae') else \"N/A\",\n",
    "                \"r2\": f\"{((r2_nuevo - metricas_antes.get('r2', 0)) / metricas_antes.get('r2', 1)) * 100:.2f}%\" if metricas_antes.get('r2') else \"N/A\"\n",
    "            },\n",
    "            \"modelo_guardado\": \"modelos/modelo_precios.keras\",\n",
    "            \"mensaje\": \"Modelo reentrenado y guardado exitosamente\"\n",
    "        }\n",
    "        \n",
    "    except HTTPException:\n",
    "        raise  # Re-lanzar excepciones HTTP\n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error durante el reentrenamiento: {str(e)}\"\n",
    "        )\n",
    "\n",
    "# ENDPOINT 4.1: Validar datos antes de reentrenar\n",
    "@app.post(\"/reentrenar/validar\", tags=[\"Reentrenamiento\"])\n",
    "async def validar_datos_reentrenamiento(datos_entrenamiento: DatosReentrenamiento):\n",
    "    \"\"\"\n",
    "    Valida los datos de reentrenamiento sin ejecutar el entrenamiento.\n",
    "    Útil para verificar que los datos son correctos antes de reentrenar.\n",
    "    \n",
    "    Args:\n",
    "        datos_entrenamiento (DatosReentrenamiento): Datos a validar\n",
    "        \n",
    "    Returns:\n",
    "        dict: Resultado de la validación\n",
    "    \"\"\"\n",
    "    # Validaciones básicas\n",
    "    problemas = []\n",
    "    advertencias = []\n",
    "    \n",
    "    # Número de muestras\n",
    "    if len(datos_entrenamiento.datos) < 10:\n",
    "        problemas.append(\"Se necesitan al menos 10 muestras (recomendado: 100+)\")\n",
    "    elif len(datos_entrenamiento.datos) < 50:\n",
    "        advertencias.append(\"Se recomienda al menos 50 muestras para un buen reentrenamiento\")\n",
    "    \n",
    "    # Concordancia de datos y precios\n",
    "    if len(datos_entrenamiento.datos) != len(datos_entrenamiento.precios):\n",
    "        problemas.append(f\"Discordancia: {len(datos_entrenamiento.datos)} datos vs {len(datos_entrenamiento.precios)} precios\")\n",
    "    \n",
    "    # Validar rangos de valores\n",
    "    precios = datos_entrenamiento.precios\n",
    "    if any(p <= 0 for p in precios):\n",
    "        problemas.append(\"Hay precios negativos o cero\")\n",
    "    \n",
    "    # Estadísticas de los datos\n",
    "    if precios:\n",
    "        estadisticas = {\n",
    "            \"precio_promedio\": np.mean(precios),\n",
    "            \"precio_min\": np.min(precios),\n",
    "            \"precio_max\": np.max(precios),\n",
    "            \"desviacion_estandar\": np.std(precios)\n",
    "        }\n",
    "    else:\n",
    "        estadisticas = {}\n",
    "    \n",
    "    # Resultado de validación\n",
    "    es_valido = len(problemas) == 0\n",
    "    \n",
    "    return {\n",
    "        \"valido\": es_valido,\n",
    "        \"n_muestras\": len(datos_entrenamiento.datos),\n",
    "        \"n_precios\": len(datos_entrenamiento.precios),\n",
    "        \"problemas\": problemas,\n",
    "        \"advertencias\": advertencias,\n",
    "        \"estadisticas\": estadisticas,\n",
    "        \"mensaje\": \"Datos válidos para reentrenamiento\" if es_valido else \"Corrige los problemas antes de reentrenar\"\n",
    "    }\n",
    "\n",
    "print(\"✓ Endpoints de reentrenamiento creados\")\n",
    "print(\"  - POST /reentrenar\")\n",
    "print(\"  - POST /reentrenar/validar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Ejecución del Servidor\n",
    "\n",
    "### Opción 1: Ejecutar desde el notebook (para desarrollo)\n",
    "\n",
    "**IMPORTANTE**: Si ejecutas el servidor desde Jupyter, la celda quedará bloqueada mientras el servidor esté corriendo. Para detenerlo, usa el botón de \"stop\" del notebook.\n",
    "\n",
    "### Opción 2: Ejecutar desde terminal (recomendado para producción)\n",
    "\n",
    "```bash\n",
    "uvicorn nombre_archivo:app --reload --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- `nombre_archivo` es el nombre de tu archivo Python (sin .py)\n",
    "- `--reload`: Reinicia automáticamente cuando cambias código (solo desarrollo)\n",
    "- `--host 0.0.0.0`: Permite acceso desde cualquier IP\n",
    "- `--port 8000`: Puerto donde escucha el servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTA: Esta celda iniciará el servidor. \n",
    "# Para detenerlo en Jupyter, usa el botón de \"stop\" o interrumpe el kernel\n",
    "\n",
    "# Cargar el modelo antes de iniciar el servidor\n",
    "print(\"Inicializando servidor...\")\n",
    "cargar_modelo()\n",
    "\n",
    "# Configuración del servidor\n",
    "# Para desarrollo local:\n",
    "# - host=\"127.0.0.1\" = solo accesible desde tu máquina\n",
    "# - host=\"0.0.0.0\" = accesible desde otras máquinas en la red\n",
    "config = uvicorn.Config(\n",
    "    app,\n",
    "    host=\"127.0.0.1\",  # Cambia a \"0.0.0.0\" para acceso externo\n",
    "    port=8000,\n",
    "    log_level=\"info\"\n",
    ")\n",
    "\n",
    "server = uvicorn.Server(config)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🚀 SERVIDOR FASTAPI INICIADO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"📍 URL: http://127.0.0.1:8000\")\n",
    "print(f\"📚 Documentación interactiva: http://127.0.0.1:8000/docs\")\n",
    "print(f\"📖 Documentación alternativa: http://127.0.0.1:8000/redoc\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPara detener el servidor: Interrumpe el kernel o presiona el botón 'stop'\\n\")\n",
    "\n",
    "# Iniciar el servidor\n",
    "# NOTA: Esta línea bloqueará la ejecución hasta que detengas el servidor\n",
    "await server.serve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Testing de la API\n",
    "\n",
    "### 10.1 Testing con Python (requests)\n",
    "\n",
    "Una vez que el servidor esté corriendo, puedes probar los endpoints desde otra terminal o notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE: Ejecuta esta celda SOLO si el servidor está corriendo en otra terminal o celda\n",
    "import requests\n",
    "\n",
    "# URL base de la API\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROBANDO ENDPOINTS DE LA API\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# 1. Test del endpoint raíz\n",
    "print(\"1️⃣ GET / (Endpoint de bienvenida)\")\n",
    "print(\"-\" * 40)\n",
    "response = requests.get(f\"{BASE_URL}/\")\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Respuesta: {response.json()}\")\n",
    "print()\n",
    "\n",
    "# 2. Test del health check\n",
    "print(\"2️⃣ GET /health (Health check)\")\n",
    "print(\"-\" * 40)\n",
    "response = requests.get(f\"{BASE_URL}/health\")\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Respuesta: {json.dumps(response.json(), indent=2)}\")\n",
    "print()\n",
    "\n",
    "# 3. Test de predicción individual\n",
    "print(\"3️⃣ POST /predecir (Predicción individual)\")\n",
    "print(\"-\" * 40)\n",
    "datos_casa = {\n",
    "    \"tamano\": 120.0,\n",
    "    \"habitaciones\": 3,\n",
    "    \"banos\": 2,\n",
    "    \"antiguedad\": 5.0\n",
    "}\n",
    "print(f\"Datos enviados: {datos_casa}\")\n",
    "response = requests.post(f\"{BASE_URL}/predecir\", json=datos_casa)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Respuesta: {json.dumps(response.json(), indent=2)}\")\n",
    "print()\n",
    "\n",
    "# 4. Test de predicción por lotes\n",
    "print(\"4️⃣ POST /predecir/lote (Predicción por lotes)\")\n",
    "print(\"-\" * 40)\n",
    "datos_lote = [\n",
    "    {\"tamano\": 100.0, \"habitaciones\": 2, \"banos\": 1, \"antiguedad\": 10.0},\n",
    "    {\"tamano\": 150.0, \"habitaciones\": 3, \"banos\": 2, \"antiguedad\": 5.0},\n",
    "    {\"tamano\": 200.0, \"habitaciones\": 4, \"banos\": 3, \"antiguedad\": 2.0}\n",
    "]\n",
    "print(f\"Número de casas: {len(datos_lote)}\")\n",
    "response = requests.post(f\"{BASE_URL}/predecir/lote\", json=datos_lote)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "resultado = response.json()\n",
    "print(f\"Predicciones obtenidas: {resultado['n_predicciones']}\")\n",
    "for pred in resultado['predicciones']:\n",
    "    print(f\"  Casa {pred['indice']}: {pred['precio_predicho']:.2f}€\")\n",
    "print()\n",
    "\n",
    "# 5. Test de monitorización\n",
    "print(\"5️⃣ GET /monitorizar (Monitorización)\")\n",
    "print(\"-\" * 40)\n",
    "response = requests.get(f\"{BASE_URL}/monitorizar\")\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "resultado = response.json()\n",
    "print(f\"Predicciones totales: {resultado['estadisticas_uso']['total_predicciones']}\")\n",
    "print(f\"Métricas del modelo:\")\n",
    "for metrica, valor in resultado['metricas_entrenamiento'].items():\n",
    "    print(f\"  - {metrica.upper()}: {valor}\")\n",
    "print()\n",
    "\n",
    "# 6. Test de validación de datos para reentrenamiento\n",
    "print(\"6️⃣ POST /reentrenar/validar (Validar datos)\")\n",
    "print(\"-\" * 40)\n",
    "datos_validacion = {\n",
    "    \"datos\": datos_lote,\n",
    "    \"precios\": [180000.0, 250000.0, 320000.0],\n",
    "    \"epochs\": 50\n",
    "}\n",
    "response = requests.post(f\"{BASE_URL}/reentrenar/validar\", json=datos_validacion)\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Respuesta: {json.dumps(response.json(), indent=2)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ TODOS LOS TESTS COMPLETADOS\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Testing con cURL (desde terminal)\n",
    "\n",
    "También puedes probar la API usando cURL desde la terminal:\n",
    "\n",
    "```bash\n",
    "# GET /\n",
    "curl http://127.0.0.1:8000/\n",
    "\n",
    "# POST /predecir\n",
    "curl -X POST \"http://127.0.0.1:8000/predecir\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"tamano\": 120, \"habitaciones\": 3, \"banos\": 2, \"antiguedad\": 5}'\n",
    "\n",
    "# GET /monitorizar\n",
    "curl http://127.0.0.1:8000/monitorizar\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Exportar la API a un Archivo Python Standalone\n",
    "\n",
    "Para usar la API en producción, es mejor tenerla en un archivo `.py` independiente.\n",
    "Esta celda exporta todo el código necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar la API a un archivo Python standalone\n",
    "codigo_api = '''\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "API REST para Predicción de Precios de Casas usando FastAPI\n",
    "Generado desde el tutorial de Jupyter Notebook\n",
    "\"\"\"\n",
    "\n",
    "# Importaciones\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import uvicorn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "import pickle\n",
    "\n",
    "# Variables globales\n",
    "modelo_actual = None\n",
    "scaler_actual = None\n",
    "metadata_actual = {}\n",
    "historial_predicciones = []\n",
    "\n",
    "# [AQUÍ IRÍA TODO EL CÓDIGO DE LOS MODELOS PYDANTIC Y ENDPOINTS]\n",
    "# Por brevedad, este es un esqueleto. En la práctica, copiarías todo el código anterior.\n",
    "\n",
    "# Función principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Cargar el modelo al inicio\n",
    "    cargar_modelo()\n",
    "    \n",
    "    # Iniciar servidor\n",
    "    uvicorn.run(\n",
    "        app,\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        log_level=\"info\"\n",
    "    )\n",
    "'''\n",
    "\n",
    "# Guardar el archivo\n",
    "with open('api_precios_casas.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(codigo_api)\n",
    "\n",
    "print(\"✓ Archivo 'api_precios_casas.py' exportado\")\n",
    "print(\"\\nPara ejecutarlo:\")\n",
    "print(\"  python api_precios_casas.py\")\n",
    "print(\"\\nO con uvicorn:\")\n",
    "print(\"  uvicorn api_precios_casas:app --reload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Resumen y Mejores Prácticas\n",
    "\n",
    "### ✅ Lo que hemos aprendido:\n",
    "\n",
    "1. **Guardado de modelos**: Diferencias entre `.keras` y `.h5`\n",
    "2. **FastAPI básico**: Crear endpoints REST\n",
    "3. **Validación con Pydantic**: Asegurar datos correctos\n",
    "4. **Predicciones**: Individual y por lotes\n",
    "5. **Monitorización**: Tracking de métricas y uso\n",
    "6. **Reentrenamiento**: Actualizar modelos sin reconstruir la API\n",
    "\n",
    "### 🎯 Mejores prácticas para producción:\n",
    "\n",
    "1. **Seguridad**:\n",
    "   - Añadir autenticación (OAuth2, API Keys)\n",
    "   - Usar HTTPS (TLS/SSL)\n",
    "   - Limitar rate limiting (evitar abuso)\n",
    "   - Validar TODOS los inputs\n",
    "\n",
    "2. **Escalabilidad**:\n",
    "   - Usar workers múltiples (`uvicorn --workers 4`)\n",
    "   - Implementar caché (Redis)\n",
    "   - Load balancing (Nginx)\n",
    "   - Contenedores Docker\n",
    "\n",
    "3. **Monitorización**:\n",
    "   - Logging estructurado\n",
    "   - Métricas con Prometheus\n",
    "   - Alertas automáticas\n",
    "   - Tracking de performance\n",
    "\n",
    "4. **Datos y Modelos**:\n",
    "   - Versionado de modelos (MLflow, DVC)\n",
    "   - A/B testing de modelos\n",
    "   - Validación de datos (Great Expectations)\n",
    "   - Backup automático\n",
    "\n",
    "5. **Código**:\n",
    "   - Tests unitarios (pytest)\n",
    "   - Tests de integración\n",
    "   - CI/CD (GitHub Actions, GitLab CI)\n",
    "   - Documentación completa\n",
    "\n",
    "### 📚 Recursos adicionales:\n",
    "\n",
    "- FastAPI: https://fastapi.tiangolo.com/\n",
    "- TensorFlow/Keras: https://www.tensorflow.org/\n",
    "- Pydantic: https://pydantic-docs.helpmanual.io/\n",
    "- MLOps: https://ml-ops.org/\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Ejercicios propuestos:\n",
    "\n",
    "1. **Básico**: Añadir un endpoint para eliminar el historial de predicciones\n",
    "2. **Intermedio**: Implementar autenticación con API Key\n",
    "3. **Avanzado**: Crear un sistema de versionado de modelos (cargar diferentes versiones)\n",
    "4. **Experto**: Implementar A/B testing entre dos modelos diferentes\n",
    "\n",
    "---\n",
    "\n",
    "**¡Gracias por completar este tutorial!** 🚀\n",
    "\n",
    "Si tienes preguntas o sugerencias, no dudes en consultarlas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "API",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
