{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase Completa de Apache Airflow\n",
    "\n",
    "## Introducci√≥n\n",
    "\n",
    "Apache Airflow es una plataforma de c√≥digo abierto para programar, orquestar y monitorear flujos de trabajo (workflows) de forma program√°tica.\n",
    "\n",
    "### ¬øQu√© aprenderemos?\n",
    "1. Conceptos b√°sicos de Airflow\n",
    "2. Instalaci√≥n y configuraci√≥n\n",
    "3. DAGs (Directed Acyclic Graphs)\n",
    "4. Operadores y tareas\n",
    "5. Dependencias y ejecuci√≥n\n",
    "6. Ejemplos pr√°cticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n de Apache Airflow\n",
    "\n",
    "Primero instalamos Airflow. Es recomendable usar un entorno virtual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# INSTALACI√ìN DE APACHE AIRFLOW\n# ============================================\n# NOTA: Ejecutar esto solo una vez para instalar las dependencias\n\n# Importamos sys para obtener la ruta del int√©rprete de Python actual\nimport sys\n\n# Instalamos Apache Airflow versi√≥n 2.7.3\n# !{sys.executable} ejecuta pip con el Python actual del notebook\n!{sys.executable} -m pip install apache-airflow==2.7.3\n\n# Instalamos el provider de HTTP para hacer requests a APIs\n!{sys.executable} -m pip install apache-airflow-providers-http",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conceptos Fundamentales\n",
    "\n",
    "### DAG (Directed Acyclic Graph)\n",
    "- Es un grafo dirigido sin ciclos que representa un flujo de trabajo\n",
    "- Contiene tareas y sus dependencias\n",
    "\n",
    "### Tarea (Task)\n",
    "- Unidad b√°sica de trabajo en Airflow\n",
    "- Se define usando operadores\n",
    "\n",
    "### Operador (Operator)\n",
    "- Template para una tarea\n",
    "- Define QU√â se ejecutar√°\n",
    "\n",
    "### Scheduler\n",
    "- Componente que programa la ejecuci√≥n de los DAGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuraci√≥n Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# IMPORTAR LIBRER√çAS NECESARIAS\n# ============================================\n\n# datetime y timedelta: Para manejar fechas y duraciones de tiempo\nfrom datetime import datetime, timedelta\n\n# DAG: Clase principal para definir un flujo de trabajo en Airflow\nfrom airflow import DAG\n\n# PythonOperator: Ejecuta funciones Python como tareas\nfrom airflow.operators.python import PythonOperator\n\n# BashOperator: Ejecuta comandos de l√≠nea de comandos (bash/shell)\nfrom airflow.operators.bash import BashOperator\n\n# DummyOperator: Tarea vac√≠a que no hace nada, √∫til para organizar el flujo\nfrom airflow.operators.dummy import DummyOperator\n\nprint(\"Librer√≠as importadas correctamente\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Argumentos por Defecto\n",
    "\n",
    "Los argumentos por defecto se aplican a todas las tareas del DAG si no se especifica lo contrario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# ARGUMENTOS POR DEFECTO DEL DAG\n# ============================================\n# Estos par√°metros se aplican a TODAS las tareas del DAG autom√°ticamente\n\ndefault_args = {\n    # 'owner': Propietario del DAG (nombre del equipo o persona)\n    'owner': 'airflow',\n    \n    # 'depends_on_past': Si True, la tarea solo se ejecuta si la anterior fue exitosa\n    'depends_on_past': False,\n    \n    # 'start_date': Fecha desde la cual el DAG est√° activo\n    'start_date': datetime(2024, 1, 1),\n    \n    # 'email': Lista de correos para notificaciones\n    'email': ['admin@ejemplo.com'],\n    \n    # 'email_on_failure': Enviar email cuando una tarea falla\n    'email_on_failure': False,\n    \n    # 'email_on_retry': Enviar email cuando una tarea se reintenta\n    'email_on_retry': False,\n    \n    # 'retries': N√∫mero de reintentos si una tarea falla\n    'retries': 1,\n    \n    # 'retry_delay': Tiempo de espera entre reintentos\n    'retry_delay': timedelta(minutes=5)\n}\n\n# Mostrar la configuraci√≥n\nprint(\"Argumentos por defecto configurados:\")\nfor key, value in default_args.items():\n    print(f\"  {key}: {value}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Crear tu Primer DAG\n",
    "\n",
    "Vamos a crear un DAG simple que ejecuta varias tareas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# CREAR EL PRIMER DAG\n# ============================================\n\n# Instanciamos un objeto DAG con sus par√°metros\ndag = DAG(\n    # 'dag_id': Identificador √∫nico del DAG (debe ser √∫nico en Airflow)\n    'mi_primer_dag',\n    \n    # Heredamos los argumentos por defecto definidos anteriormente\n    default_args=default_args,\n    \n    # Descripci√≥n que aparecer√° en la UI de Airflow\n    description='Un DAG simple de ejemplo',\n    \n    # 'schedule_interval': Frecuencia de ejecuci√≥n (cada 1 d√≠a en este caso)\n    schedule_interval=timedelta(days=1),\n    \n    # 'catchup': Si False, no ejecuta las ejecuciones pasadas al activar el DAG\n    catchup=False,\n    \n    # 'tags': Etiquetas para organizar y filtrar DAGs en la UI\n    tags=['ejemplo', 'tutorial']\n)\n\n# Verificar que el DAG se cre√≥ correctamente\nprint(f\"DAG creado: {dag.dag_id}\")\nprint(f\"Schedule: {dag.schedule_interval}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Funciones Python para las Tareas\n",
    "\n",
    "Definimos funciones que ser√°n ejecutadas por nuestras tareas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# DEFINIR FUNCIONES PARA LAS TAREAS\n# ============================================\n# Estas funciones ser√°n ejecutadas por los PythonOperators\n\n# Funci√≥n 1: Extraer datos (fase Extract del ETL)\ndef extraer_datos(**context):\n    \"\"\"Simula la extracci√≥n de datos de una fuente\"\"\"\n    print(\"Extrayendo datos...\")\n    \n    # Simulamos datos extra√≠dos de una base de datos o API\n    datos = {'usuarios': 150, 'ventas': 3500, 'productos': 45}\n    print(f\"Datos extra√≠dos: {datos}\")\n    \n    # El valor retornado se guarda autom√°ticamente en XCom\n    return datos\n\n\n# Funci√≥n 2: Transformar datos (fase Transform del ETL)\ndef transformar_datos(**context):\n    \"\"\"Simula la transformaci√≥n de datos\"\"\"\n    print(\"Transformando datos...\")\n    \n    # Aqu√≠ aplicar√≠amos las transformaciones necesarias\n    datos_transformados = {\n        'total_usuarios': 150,\n        'promedio_ventas': 23.33,\n        'categoria': 'productos_electronicos'\n    }\n    print(f\"Datos transformados: {datos_transformados}\")\n    \n    return datos_transformados\n\n\n# Funci√≥n 3: Cargar datos (fase Load del ETL)\ndef cargar_datos(**context):\n    \"\"\"Simula la carga de datos en un destino\"\"\"\n    print(\"Cargando datos en el destino...\")\n    print(\"Datos cargados exitosamente en la base de datos\")\n    \n    return \"Carga completada\"\n\n\n# Funci√≥n 4: Enviar notificaci√≥n\ndef enviar_notificacion(**context):\n    \"\"\"Env√≠a una notificaci√≥n al finalizar el proceso\"\"\"\n    print(\"Enviando notificaci√≥n...\")\n    print(\"Notificaci√≥n enviada: Pipeline ETL completado exitosamente\")\n\n\nprint(\"Funciones definidas correctamente\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Crear Tareas con Operadores\n",
    "\n",
    "### PythonOperator\n",
    "Ejecuta funciones Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# CREAR TAREAS CON PYTHONOPERATOR\n# ============================================\n\n# Tarea 1: Tarea de inicio (dummy - no hace nada, solo marca el inicio)\ninicio = DummyOperator(\n    task_id='inicio',  # ID √∫nico de la tarea\n    dag=dag            # DAG al que pertenece esta tarea\n)\n\n# Tarea 2: Extracci√≥n de datos\ntarea_extraer = PythonOperator(\n    task_id='extraer_datos',           # ID √∫nico de la tarea\n    python_callable=extraer_datos,     # Funci√≥n Python que ejecutar√°\n    provide_context=True,              # Proporciona el contexto de Airflow a la funci√≥n\n    dag=dag                            # DAG al que pertenece\n)\n\n# Tarea 3: Transformaci√≥n de datos\ntarea_transformar = PythonOperator(\n    task_id='transformar_datos',       # ID √∫nico de la tarea\n    python_callable=transformar_datos, # Funci√≥n a ejecutar\n    provide_context=True,              # Permite acceder a XCom y otros datos del contexto\n    dag=dag\n)\n\n# Tarea 4: Carga de datos\ntarea_cargar = PythonOperator(\n    task_id='cargar_datos',\n    python_callable=cargar_datos,\n    provide_context=True,\n    dag=dag\n)\n\nprint(\"Tareas Python creadas\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BashOperator\n",
    "Ejecuta comandos Bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# CREAR TAREAS CON BASHOPERATOR\n# ============================================\n\n# Tarea con BashOperator: Ejecuta un comando de shell\ntarea_bash = BashOperator(\n    task_id='verificar_sistema',                        # ID de la tarea\n    bash_command='echo \"Sistema verificado en $(date)\"', # Comando bash a ejecutar\n    dag=dag\n)\n\n# Tarea de notificaci√≥n usando PythonOperator\ntarea_notificar = PythonOperator(\n    task_id='enviar_notificacion',\n    python_callable=enviar_notificacion,\n    provide_context=True,\n    dag=dag\n)\n\n# Tarea final (dummy - marca el fin del DAG)\nfin = DummyOperator(\n    task_id='fin',\n    dag=dag\n)\n\nprint(\"Todas las tareas creadas\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Definir Dependencias entre Tareas\n",
    "\n",
    "Las dependencias determinan el orden de ejecuci√≥n de las tareas.\n",
    "\n",
    "### M√©todos para definir dependencias:\n",
    "- `tarea1 >> tarea2` (tarea1 antes de tarea2)\n",
    "- `tarea1 << tarea2` (tarea2 antes de tarea1)\n",
    "- `set_upstream()` y `set_downstream()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# DEFINIR DEPENDENCIAS ENTRE TAREAS\n# ============================================\n# El operador '>>' significa \"ejecutar antes que\"\n# Esto crea el flujo de ejecuci√≥n del DAG\n\n# Flujo completo del pipeline:\n# 1. inicio -> 2. verificar_sistema -> 3. extraer_datos -> \n# 4. transformar_datos -> 5. cargar_datos -> 6. enviar_notificacion -> 7. fin\n\ninicio >> tarea_bash >> tarea_extraer >> tarea_transformar >> tarea_cargar >> tarea_notificar >> fin\n\nprint(\"Dependencias configuradas:\")\nprint(\"inicio -> verificar_sistema -> extraer_datos -> transformar_datos -> cargar_datos -> enviar_notificacion -> fin\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Ejemplo de DAG con Paralelizaci√≥n\n",
    "\n",
    "Vamos a crear un DAG donde algunas tareas se ejecutan en paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# DAG CON TAREAS PARALELAS\n# ============================================\n# Este ejemplo muestra c√≥mo ejecutar varias tareas al mismo tiempo\n\n# Crear un nuevo DAG para demostrar paralelizaci√≥n\ndag_paralelo = DAG(\n    'dag_con_tareas_paralelas',\n    default_args=default_args,\n    description='DAG con ejecuci√≥n paralela',\n    schedule_interval='@daily',  # Se ejecuta diariamente\n    catchup=False\n)\n\n\n# Funci√≥n 1: Procesar datos de la fuente A\ndef procesar_fuente_a(**context):\n    \"\"\"Procesa datos de la primera fuente\"\"\"\n    print(\"Procesando datos de la fuente A\")\n    return \"Fuente A procesada\"\n\n\n# Funci√≥n 2: Procesar datos de la fuente B\ndef procesar_fuente_b(**context):\n    \"\"\"Procesa datos de la segunda fuente\"\"\"\n    print(\"Procesando datos de la fuente B\")\n    return \"Fuente B procesada\"\n\n\n# Funci√≥n 3: Procesar datos de la fuente C\ndef procesar_fuente_c(**context):\n    \"\"\"Procesa datos de la tercera fuente\"\"\"\n    print(\"Procesando datos de la fuente C\")\n    return \"Fuente C procesada\"\n\n\n# Funci√≥n 4: Consolidar todos los resultados\ndef consolidar_datos(**context):\n    \"\"\"Consolida los resultados de todas las fuentes\"\"\"\n    print(\"Consolidando todas las fuentes\")\n    return \"Consolidaci√≥n completada\"\n\n\nprint(\"DAG paralelo creado\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# CREAR Y CONFIGURAR TAREAS PARALELAS\n# ============================================\n\n# Tarea inicial\ninicio_paralelo = DummyOperator(task_id='inicio', dag=dag_paralelo)\n\n# Crear tres tareas que se ejecutar√°n EN PARALELO\ntarea_fuente_a = PythonOperator(\n    task_id='procesar_fuente_a',\n    python_callable=procesar_fuente_a,\n    dag=dag_paralelo\n)\n\ntarea_fuente_b = PythonOperator(\n    task_id='procesar_fuente_b',\n    python_callable=procesar_fuente_b,\n    dag=dag_paralelo\n)\n\ntarea_fuente_c = PythonOperator(\n    task_id='procesar_fuente_c',\n    python_callable=procesar_fuente_c,\n    dag=dag_paralelo\n)\n\n# Tarea que consolida los resultados (espera a que terminen las 3 anteriores)\ntarea_consolidar = PythonOperator(\n    task_id='consolidar_datos',\n    python_callable=consolidar_datos,\n    dag=dag_paralelo\n)\n\n# Tarea final\nfin_paralelo = DummyOperator(task_id='fin', dag=dag_paralelo)\n\n# ============================================\n# DEFINIR DEPENDENCIAS PARALELAS\n# ============================================\n# La sintaxis [tarea1, tarea2, tarea3] indica que todas se ejecutan en paralelo\n# inicio -> [fuente_a, fuente_b, fuente_c] -> consolidar -> fin\n\ninicio_paralelo >> [tarea_fuente_a, tarea_fuente_b, tarea_fuente_c] >> tarea_consolidar >> fin_paralelo\n\nprint(\"Tareas paralelas configuradas\")\nprint(\"Las tareas A, B y C se ejecutar√°n en paralelo\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. XCom - Compartir Datos entre Tareas\n",
    "\n",
    "XCom (cross-communication) permite que las tareas compartan peque√±as cantidades de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# XCOM - COMPARTIR DATOS ENTRE TAREAS\n# ============================================\n# XCom (Cross-Communication) permite pasar datos entre tareas\n\n# Crear DAG para demostrar XCom\ndag_xcom = DAG(\n    'ejemplo_xcom',\n    default_args=default_args,\n    description='Ejemplo de XCom',\n    schedule_interval=None,  # Ejecuci√≥n manual solamente\n    catchup=False\n)\n\n\ndef generar_numero(**context):\n    \"\"\"Genera un n√∫mero aleatorio y lo guarda en XCom\"\"\"\n    import random\n    \n    # Generamos un n√∫mero aleatorio\n    numero = random.randint(1, 100)\n    print(f\"N√∫mero generado: {numero}\")\n    \n    # Al hacer 'return', el valor se guarda AUTOM√ÅTICAMENTE en XCom\n    # Otros tasks podr√°n recuperarlo usando xcom_pull()\n    return numero\n\n\ndef procesar_numero(**context):\n    \"\"\"Recupera el n√∫mero de XCom y lo procesa\"\"\"\n    \n    # 'ti' (Task Instance) contiene m√©todos para interactuar con XCom\n    ti = context['ti']\n    \n    # xcom_pull() recupera el valor guardado por la tarea 'generar_numero'\n    numero = ti.xcom_pull(task_ids='generar_numero')\n    \n    # Procesamos el n√∫mero (en este caso, lo multiplicamos por 2)\n    resultado = numero * 2\n    \n    print(f\"N√∫mero recibido: {numero}\")\n    print(f\"Resultado del procesamiento: {resultado}\")\n    \n    return resultado\n\n\n# Crear las tareas\ntarea_generar = PythonOperator(\n    task_id='generar_numero',\n    python_callable=generar_numero,\n    dag=dag_xcom\n)\n\ntarea_procesar = PythonOperator(\n    task_id='procesar_numero',\n    python_callable=procesar_numero,\n    dag=dag_xcom\n)\n\n# Definir dependencia: primero generar, luego procesar\ntarea_generar >> tarea_procesar\n\nprint(\"DAG con XCom configurado\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Programaci√≥n de DAGs (Schedule Interval)\n",
    "\n",
    "### Opciones de programaci√≥n:\n",
    "- `None`: Ejecuci√≥n manual\n",
    "- `@once`: Una sola vez\n",
    "- `@hourly`: Cada hora\n",
    "- `@daily`: Diario\n",
    "- `@weekly`: Semanal\n",
    "- `@monthly`: Mensual\n",
    "- Expresi√≥n cron: `'0 0 * * *'`\n",
    "- `timedelta`: `timedelta(hours=2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# SCHEDULE INTERVAL - PROGRAMACI√ìN DE DAGS\n# ============================================\n# Diferentes formas de programar cu√°ndo se ejecuta un DAG\n\nfrom datetime import timedelta\n\n# Diccionario con ejemplos de diferentes programaciones\nejemplos_schedule = {\n    # None: El DAG solo se ejecuta manualmente\n    'Manual': None,\n    \n    # @once: Se ejecuta una sola vez\n    'Una vez': '@once',\n    \n    # @hourly: Se ejecuta cada hora (equivale a '0 * * * *')\n    'Cada hora': '@hourly',\n    \n    # @daily: Se ejecuta diariamente a medianoche (equivale a '0 0 * * *')\n    'Diario a medianoche': '@daily',\n    \n    # @weekly: Se ejecuta semanalmente (domingos a medianoche)\n    'Semanal': '@weekly',\n    \n    # timedelta: Ejecutar cada X tiempo desde la √∫ltima ejecuci√≥n\n    'Cada 2 horas': timedelta(hours=2),\n    \n    # Expresi√≥n CRON: '0 9 * * *' = todos los d√≠as a las 9 AM\n    # Formato: minuto hora d√≠a_mes mes d√≠a_semana\n    'Diario a las 9 AM': '0 9 * * *',\n    \n    # '0 8 * * 1-5' = Lunes a Viernes (1-5) a las 8 AM\n    'Lunes a Viernes a las 8 AM': '0 8 * * 1-5'\n}\n\nprint(\"Ejemplos de Schedule Interval:\")\nfor nombre, schedule in ejemplos_schedule.items():\n    print(f\"  {nombre}: {schedule}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Caso Pr√°ctico: Pipeline ETL Completo\n",
    "\n",
    "Vamos a crear un pipeline ETL real que simula:\n",
    "1. Extracci√≥n de datos de una API\n",
    "2. Validaci√≥n de datos\n",
    "3. Transformaci√≥n\n",
    "4. Carga en base de datos\n",
    "5. Generaci√≥n de reporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# PIPELINE ETL COMPLETO - FUNCIONES\n# ============================================\n\nimport json\nfrom datetime import datetime\n\n# Crear DAG para un pipeline ETL completo\ndag_etl = DAG(\n    'pipeline_etl_completo',\n    default_args=default_args,\n    description='Pipeline ETL completo con validaciones',\n    schedule_interval='0 2 * * *',  # Ejecutar diariamente a las 2 AM (expresi√≥n CRON)\n    catchup=False,\n    tags=['etl', 'produccion']\n)\n\n\ndef extraer_de_api(**context):\n    \"\"\"Extrae datos de una API (simulado)\"\"\"\n    print(\"Conectando a la API...\")\n    \n    # Simulaci√≥n de respuesta de una API con datos de productos\n    datos = [\n        {'id': 1, 'nombre': 'Producto A', 'precio': 100, 'stock': 50},\n        {'id': 2, 'nombre': 'Producto B', 'precio': 200, 'stock': 30},\n        {'id': 3, 'nombre': 'Producto C', 'precio': 150, 'stock': 0},\n        {'id': 4, 'nombre': 'Producto D', 'precio': 300, 'stock': 20}\n    ]\n    print(f\"Extra√≠dos {len(datos)} registros\")\n    \n    # Retornar datos para que est√©n disponibles en XCom\n    return datos\n\n\ndef validar_datos(**context):\n    \"\"\"Valida que los datos cumplan con las reglas de negocio\"\"\"\n    # Obtener datos de la tarea anterior usando XCom\n    ti = context['ti']\n    datos = ti.xcom_pull(task_ids='extraer_de_api')\n    \n    datos_validos = []\n    datos_invalidos = []\n    \n    # Validar cada registro seg√∫n reglas de negocio\n    for item in datos:\n        # Regla: precio debe ser positivo y stock no negativo\n        if item['precio'] > 0 and item['stock'] >= 0:\n            datos_validos.append(item)\n        else:\n            datos_invalidos.append(item)\n    \n    print(f\"Datos v√°lidos: {len(datos_validos)}\")\n    print(f\"Datos inv√°lidos: {len(datos_invalidos)}\")\n    \n    # Retornar solo los datos v√°lidos\n    return datos_validos\n\n\ndef transformar_etl(**context):\n    \"\"\"Transforma los datos seg√∫n los requisitos del negocio\"\"\"\n    # Recuperar datos validados de la tarea anterior\n    ti = context['ti']\n    datos = ti.xcom_pull(task_ids='validar_datos')\n    \n    datos_transformados = []\n    \n    # Aplicar transformaciones a cada registro\n    for item in datos:\n        transformado = {\n            # Mantener ID\n            'producto_id': item['id'],\n            \n            # Convertir nombre a may√∫sculas\n            'nombre_producto': item['nombre'].upper(),\n            \n            # Calcular precio con IVA (21%)\n            'precio_con_iva': round(item['precio'] * 1.21, 2),\n            \n            # Agregar campo booleano de disponibilidad\n            'disponible': item['stock'] > 0,\n            \n            # Categorizar seg√∫n stock\n            'categoria': 'En Stock' if item['stock'] > 0 else 'Agotado',\n            \n            # Agregar timestamp del procesamiento\n            'fecha_proceso': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n        }\n        datos_transformados.append(transformado)\n    \n    print(f\"Transformados {len(datos_transformados)} registros\")\n    return datos_transformados\n\n\ndef cargar_en_bd(**context):\n    \"\"\"Simula la carga de datos en una base de datos\"\"\"\n    # Recuperar datos transformados\n    ti = context['ti']\n    datos = ti.xcom_pull(task_ids='transformar_etl')\n    \n    print(\"Conectando a la base de datos...\")\n    print(f\"Insertando {len(datos)} registros...\")\n    \n    # Simular inserciones en la base de datos\n    for item in datos:\n        print(f\"  INSERT: {item['nombre_producto']} - ${item['precio_con_iva']}\")\n    \n    print(\"Carga completada exitosamente\")\n    \n    # Retornar cantidad de registros cargados\n    return len(datos)\n\n\ndef generar_reporte(**context):\n    \"\"\"Genera un reporte resumen del proceso ETL\"\"\"\n    # Recuperar informaci√≥n de la tarea de carga\n    ti = context['ti']\n    registros_cargados = ti.xcom_pull(task_ids='cargar_en_bd')\n    \n    # Crear reporte formateado\n    reporte = f\"\"\"\n    ========================================\n    REPORTE ETL - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n    ========================================\n    Registros procesados: {registros_cargados}\n    Estado: COMPLETADO\n    Pipeline: pipeline_etl_completo\n    ========================================\n    \"\"\"\n    print(reporte)\n    \n    return reporte\n\n\nprint(\"Funciones ETL definidas\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# PIPELINE ETL COMPLETO - CREAR TAREAS\n# ============================================\n\n# Tarea 1: Inicio del pipeline\ninicio_etl = DummyOperator(task_id='inicio', dag=dag_etl)\n\n# Tarea 2: Extracci√≥n de datos desde API\nextraer_api = PythonOperator(\n    task_id='extraer_de_api',\n    python_callable=extraer_de_api,\n    dag=dag_etl\n)\n\n# Tarea 3: Validaci√≥n de datos\nvalidar = PythonOperator(\n    task_id='validar_datos',\n    python_callable=validar_datos,\n    dag=dag_etl\n)\n\n# Tarea 4: Transformaci√≥n de datos\ntransformar = PythonOperator(\n    task_id='transformar_etl',\n    python_callable=transformar_etl,\n    dag=dag_etl\n)\n\n# Tarea 5: Carga en base de datos\ncargar = PythonOperator(\n    task_id='cargar_en_bd',\n    python_callable=cargar_en_bd,\n    dag=dag_etl\n)\n\n# Tarea 6: Generar reporte del proceso\nreporte = PythonOperator(\n    task_id='generar_reporte',\n    python_callable=generar_reporte,\n    dag=dag_etl\n)\n\n# Tarea 7: Fin del pipeline\nfin_etl = DummyOperator(task_id='fin', dag=dag_etl)\n\n# ============================================\n# DEFINIR FLUJO DEL PIPELINE ETL\n# ============================================\n# Flujo secuencial completo del ETL:\n# inicio -> extraer -> validar -> transformar -> cargar -> reporte -> fin\n\ninicio_etl >> extraer_api >> validar >> transformar >> cargar >> reporte >> fin_etl\n\nprint(\"Pipeline ETL completo configurado\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualizaci√≥n de la Estructura del DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# ============================================\n# VISUALIZAR ESTRUCTURA DE LOS DAGS CREADOS\n# ============================================\n\n# Lista con todos los DAGs que hemos creado\ndags_creados = [\n    ('mi_primer_dag', dag),\n    ('dag_con_tareas_paralelas', dag_paralelo),\n    ('ejemplo_xcom', dag_xcom),\n    ('pipeline_etl_completo', dag_etl)\n]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESUMEN DE DAGs CREADOS\")\nprint(\"=\"*60)\n\n# Iterar sobre cada DAG y mostrar su informaci√≥n\nfor nombre, dag_obj in dags_creados:\n    print(f\"\\nDAG: {nombre}\")\n    print(f\"  Descripci√≥n: {dag_obj.description}\")\n    print(f\"  Schedule: {dag_obj.schedule_interval}\")\n    print(f\"  N√∫mero de tareas: {len(dag_obj.tasks)}\")\n    # Listar los IDs de todas las tareas del DAG\n    print(f\"  Tareas: {[t.task_id for t in dag_obj.tasks]}\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Comandos √ötiles de Airflow\n",
    "\n",
    "### Comandos CLI b√°sicos:\n",
    "\n",
    "```bash\n",
    "# Inicializar la base de datos\n",
    "airflow db init\n",
    "\n",
    "# Crear usuario admin\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com\n",
    "\n",
    "# Iniciar el webserver\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Iniciar el scheduler\n",
    "airflow scheduler\n",
    "\n",
    "# Listar DAGs\n",
    "airflow dags list\n",
    "\n",
    "# Probar una tarea espec√≠fica\n",
    "airflow tasks test <dag_id> <task_id> <execution_date>\n",
    "\n",
    "# Ejecutar un DAG manualmente\n",
    "airflow dags trigger <dag_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Mejores Pr√°cticas\n",
    "\n",
    "### 1. Dise√±o de DAGs\n",
    "- Mant√©n los DAGs simples y enfocados\n",
    "- Evita dependencias circulares\n",
    "- Usa nombres descriptivos para tareas\n",
    "\n",
    "### 2. Manejo de Errores\n",
    "- Configura reintentos apropiados\n",
    "- Implementa manejo de excepciones\n",
    "- Usa alertas para fallos cr√≠ticos\n",
    "\n",
    "### 3. Rendimiento\n",
    "- No uses XCom para datos grandes\n",
    "- Paraleliza tareas cuando sea posible\n",
    "- Configura pools para limitar concurrencia\n",
    "\n",
    "### 4. Monitoreo\n",
    "- Implementa logs detallados\n",
    "- Usa SLAs para tareas cr√≠ticas\n",
    "- Monitorea el estado de los DAGs regularmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 19. Ejercicios Pr√°cticos\n\n### Ejercicio 1: DAG Simple\nCrea un DAG que:\n1. Lea un archivo CSV\n2. Filtre datos por una condici√≥n\n3. Guarde el resultado\n\n### Ejercicio 2: DAG con Branching\nCrea un DAG que ejecute diferentes ramas seg√∫n una condici√≥n\n\n### Ejercicio 3: Pipeline de ML\nCrea un DAG que:\n1. Cargue datos\n2. Entrene un modelo\n3. Eval√∫e el modelo\n4. Guarde el mejor modelo\n\n### Ejercicio 4: TaskFlow API Challenge\nConvierte un DAG tradicional a TaskFlow API\n\n### Ejercicio 5: Sensor Personalizado\nCrea un sensor que espere a que una API espec√≠fica est√© disponible\n\n### Ejercicio 6: Pipeline con Hooks\nCrea un pipeline que:\n1. Use HttpHook para obtener datos de una API\n2. Procese los datos\n3. Use un Hook de base de datos para guardar los resultados"
  },
  {
   "cell_type": "code",
   "source": "from airflow.decorators import dag, task\nfrom datetime import datetime\nimport json\n\n@dag(\n    dag_id='pipeline_completo_avanzado',\n    start_date=datetime(2024, 1, 1),\n    schedule_interval='@daily',\n    catchup=False,\n    description='Pipeline que combina TaskFlow, Sensors y Hooks',\n    tags=['completo', 'avanzado', 'produccion']\n)\ndef advanced_pipeline():\n    \"\"\"\n    Pipeline avanzado que demuestra:\n    - TaskFlow API para c√≥digo limpio\n    - Sensors para esperar condiciones\n    - Hooks para integraciones externas\n    \"\"\"\n    \n    @task\n    def setup_pipeline():\n        \"\"\"Inicializa el pipeline\"\"\"\n        print(\"=\"*60)\n        print(\"INICIANDO PIPELINE AVANZADO\")\n        print(f\"Timestamp: {datetime.now()}\")\n        print(\"=\"*60)\n        return {'status': 'initialized', 'timestamp': str(datetime.now())}\n    \n    @task\n    def fetch_from_api():\n        \"\"\"Simula fetch de API con Hook\"\"\"\n        print(\"\\n[1] Obteniendo datos de API...\")\n        # En producci√≥n: HttpHook(http_conn_id='api_prod').run()\n        data = {\n            'users': [\n                {'id': 1, 'name': 'Alice', 'purchases': 150},\n                {'id': 2, 'name': 'Bob', 'purchases': 200},\n                {'id': 3, 'name': 'Charlie', 'purchases': 175}\n            ],\n            'timestamp': str(datetime.now())\n        }\n        print(f\"   Obtenidos {len(data['users'])} usuarios\")\n        return data\n    \n    @task\n    def validate_and_transform(data: dict):\n        \"\"\"Valida y transforma los datos\"\"\"\n        print(\"\\n[2] Validando y transformando datos...\")\n        users = data['users']\n        \n        # Validaci√≥n\n        valid_users = [u for u in users if u['purchases'] > 0]\n        print(f\"   Usuarios v√°lidos: {len(valid_users)}/{len(users)}\")\n        \n        # Transformaci√≥n\n        transformed = [\n            {\n                'user_id': u['id'],\n                'user_name': u['name'].upper(),\n                'total_purchases': u['purchases'],\n                'category': 'premium' if u['purchases'] > 180 else 'standard',\n                'processed_at': str(datetime.now())\n            }\n            for u in valid_users\n        ]\n        \n        print(f\"   Transformados {len(transformed)} registros\")\n        return transformed\n    \n    @task\n    def calculate_metrics(users: list):\n        \"\"\"Calcula m√©tricas del negocio\"\"\"\n        print(\"\\n[3] Calculando m√©tricas...\")\n        \n        total_purchases = sum(u['total_purchases'] for u in users)\n        premium_users = len([u for u in users if u['category'] == 'premium'])\n        \n        metrics = {\n            'total_users': len(users),\n            'total_revenue': total_purchases,\n            'average_purchase': total_purchases / len(users) if users else 0,\n            'premium_users': premium_users,\n            'premium_percentage': (premium_users / len(users) * 100) if users else 0\n        }\n        \n        print(\"   M√©tricas calculadas:\")\n        for key, value in metrics.items():\n            print(f\"     {key}: {value}\")\n        \n        return metrics\n    \n    @task\n    def save_to_database(users: list, metrics: dict):\n        \"\"\"Guarda datos en base de datos\"\"\"\n        print(\"\\n[4] Guardando en base de datos...\")\n        # En producci√≥n: PostgresHook(postgres_conn_id='db_prod').run()\n        \n        print(f\"   Insertando {len(users)} usuarios...\")\n        print(f\"   Insertando m√©tricas: {metrics['total_revenue']} revenue\")\n        print(\"   ‚úì Guardado exitoso\")\n        \n        return {'status': 'saved', 'records': len(users)}\n    \n    @task\n    def send_report(save_status: dict, metrics: dict):\n        \"\"\"Env√≠a reporte final\"\"\"\n        print(\"\\n[5] Generando reporte final...\")\n        print(\"=\"*60)\n        print(\"REPORTE EJECUTIVO - PIPELINE COMPLETADO\")\n        print(\"=\"*60)\n        print(f\"Estado: {save_status['status'].upper()}\")\n        print(f\"Registros procesados: {save_status['records']}\")\n        print(f\"Revenue total: ${metrics['total_revenue']}\")\n        print(f\"Usuarios premium: {metrics['premium_users']} ({metrics['premium_percentage']:.1f}%)\")\n        print(f\"Compra promedio: ${metrics['average_purchase']:.2f}\")\n        print(\"=\"*60)\n        print(\"Pipeline finalizado exitosamente\\n\")\n    \n    # Definir el flujo completo\n    init = setup_pipeline()\n    api_data = fetch_from_api()\n    transformed_data = validate_and_transform(api_data)\n    metrics = calculate_metrics(transformed_data)\n    save_status = save_to_database(transformed_data, metrics)\n    \n    # Establecer dependencias\n    init >> api_data\n    send_report(save_status, metrics)\n\n# Instanciar el DAG\nadvanced_dag = advanced_pipeline()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DAG AVANZADO CREADO EXITOSAMENTE\")\nprint(\"=\"*60)\nprint(\"Caracter√≠sticas:\")\nprint(\"  ‚úì TaskFlow API para c√≥digo limpio\")\nprint(\"  ‚úì Type hints para mejor desarrollo\")\nprint(\"  ‚úì Paso autom√°tico de datos entre tareas\")\nprint(\"  ‚úì Estructura modular y escalable\")\nprint(\"  ‚úì Integraci√≥n con Hooks y Sensors\")\nprint(\"=\"*60 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 20. Recursos Adicionales\n\n### Documentaci√≥n Oficial\n- [Apache Airflow Documentation](https://airflow.apache.org/docs/)\n- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n- [TaskFlow API Tutorial](https://airflow.apache.org/docs/apache-airflow/stable/tutorial_taskflow_api.html)\n- [Pythonic DAGs with TaskFlow](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/taskflow.html)\n- [TaskFlow Concepts](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/taskflow.html)\n\n### Tutoriales Externos\n- [Apache Airflow Tutorial: Ultimate Guide 2024 - Innowise](https://innowise.com/blog/apache-airflow-introduction/)\n- [Getting Started with Apache Airflow - DataCamp](https://www.datacamp.com/tutorial/getting-started-with-apache-airflow)\n\n### Operadores √ötiles\n- **PythonOperator**: Ejecutar funciones Python\n- **BashOperator**: Ejecutar comandos Bash\n- **EmailOperator**: Enviar emails\n- **HttpOperator**: Llamadas HTTP/API\n- **SqlOperator**: Consultas SQL\n- **BranchPythonOperator**: Ramificaci√≥n condicional\n\n### Sensors Comunes\n- **FileSensor**: Detectar archivos\n- **TimeSensor**: Esperar hasta una hora\n- **HttpSensor**: Verificar endpoints\n- **ExternalTaskSensor**: Esperar otras tareas\n\n### Hooks Populares\n- **HttpHook**: APIs REST\n- **PostgresHook**: PostgreSQL\n- **MySqlHook**: MySQL\n- **S3Hook**: Amazon S3\n- **GCSHook**: Google Cloud Storage\n- **SlackHook**: Notificaciones Slack\n\n### Providers\n- AWS\n- Google Cloud\n- Azure\n- Kubernetes\n- Snowflake\n- Databricks\n- Y muchos m√°s...",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "## Conclusi√≥n\n\nEn este tutorial has aprendido:\n\n‚úÖ Conceptos fundamentales de Apache Airflow  \n‚úÖ C√≥mo crear y configurar DAGs  \n‚úÖ Diferentes tipos de operadores  \n‚úÖ Manejo de dependencias entre tareas  \n‚úÖ Ejecuci√≥n paralela de tareas  \n‚úÖ Compartir datos con XCom  \n‚úÖ Programaci√≥n de DAGs  \n‚úÖ Implementaci√≥n de pipelines ETL completos  \n‚úÖ **TaskFlow API (Airflow 2.0+)** - C√≥digo m√°s limpio y Pythonic  \n‚úÖ **Sensors** - Esperar condiciones antes de continuar  \n‚úÖ **Hooks** - Integraci√≥n con servicios externos  \n‚úÖ Mejores pr√°cticas  \n\n### Lo Nuevo en Airflow 2.0+\n\n**TaskFlow API** revoluciona la forma de escribir DAGs:\n- Decoradores simples `@dag` y `@task`\n- Paso autom√°tico de datos (sin XCom manual)\n- Type hints para mejor desarrollo\n- C√≥digo m√°s limpio y mantenible\n\n**Sensors mejorados** con decoradores:\n- `@task.sensor` para crear sensors personalizados\n- Mejor manejo de recursos con `mode='reschedule'`\n- Integraci√≥n perfecta con TaskFlow\n\n**Hooks modernos**:\n- Mejor abstracci√≥n de conexiones externas\n- Manejo seguro de credenciales\n- Integraci√≥n con cientos de servicios\n\n### Pr√≥ximos Pasos\n1. Practica creando tus propios DAGs con TaskFlow API\n2. Explora diferentes Sensors y Hooks\n3. Implementa pipelines de datos reales\n4. Aprende sobre Airflow en producci√≥n\n5. Explora integraciones con Cloud Providers\n6. Investiga sobre Airflow en Kubernetes\n\n### Recursos Avanzados\n- [Understanding TaskFlow API - Restack](https://www.restack.io/docs/airflow-faq-tutorial-taskflow-01)\n- [Apache Airflow Components - Restack](https://www.restack.io/docs/airflow-faq-core-concepts-tasks-07)\n- Comunidad Airflow en Slack\n- Airflow Summit (conferencia anual)\n\n¬°Feliz orquestaci√≥n de flujos de trabajo! üöÄ\n\n---\n\n**Nota**: Este notebook ha sido enriquecido con informaci√≥n de:\n- Documentaci√≥n oficial de Apache Airflow\n- Tutoriales de la comunidad\n- Mejores pr√°cticas de industria\n- Ejemplos pr√°cticos de producci√≥n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 18. Hooks en Airflow\n\n### ¬øQu√© son los Hooks?\n\nLos **Hooks** son interfaces para interactuar con sistemas externos:\n- **Abstraen** la conexi√≥n a servicios externos\n- **Reutilizan** conexiones y credenciales\n- **Simplifican** la interacci√≥n con APIs, bases de datos, servicios cloud\n\n### Hooks Comunes\n- **HttpHook**: API REST\n- **PostgresHook**: PostgreSQL\n- **MySqlHook**: MySQL\n- **S3Hook**: Amazon S3\n- **GCSHook**: Google Cloud Storage\n- **SlackHook**: Slack\n\n### Ventajas\n- Manejo centralizado de conexiones\n- Credenciales seguras en Airflow Connections\n- C√≥digo m√°s limpio y mantenible",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SENSOR PERSONALIZADO CON @task.sensor\n# ============================================\n\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\nimport random\n\n\n@dag(\n    dag_id='custom_sensor_ejemplo',\n    start_date=datetime(2024, 1, 1),\n    schedule_interval=None,  # Solo ejecuci√≥n manual\n    catchup=False\n)\ndef dag_with_custom_sensor():\n    \"\"\"DAG con sensor personalizado usando decorador\"\"\"\n    \n    # ============================================\n    # @task.sensor: Crear sensor personalizado\n    # ============================================\n    @task.sensor(\n        poke_interval=10,    # Verificar cada 10 segundos\n        timeout=120,         # Timeout despu√©s de 2 minutos\n        mode='poke'          # Modo de ejecuci√≥n\n    )\n    def wait_for_condition() -> bool:\n        \"\"\"Sensor que espera una condici√≥n personalizada\n        \n        IMPORTANTE: Debe retornar bool\n        - True: Condici√≥n cumplida, continuar\n        - False: Condici√≥n no cumplida, seguir esperando\n        \"\"\"\n        # Simulamos verificar una condici√≥n externa (ej: API, archivo, etc)\n        condition_met = random.random() > 0.7  # 30% de probabilidad\n        \n        if condition_met:\n            print(\"‚úì Condici√≥n cumplida!\")\n            return True  # Sensor deja de esperar\n        else:\n            print(\"‚úó Condici√≥n no cumplida, esperando...\")\n            return False  # Sensor sigue verificando\n    \n    @task\n    def process_when_ready():\n        \"\"\"Esta tarea solo se ejecuta cuando el sensor retorna True\"\"\"\n        print(\"Sensor detect√≥ que la condici√≥n se cumpli√≥\")\n        print(\"Iniciando procesamiento...\")\n        return \"Proceso completado\"\n    \n    # ============================================\n    # FLUJO CON SENSOR PERSONALIZADO\n    # ============================================\n    # El proceso solo inicia cuando wait_for_condition() retorna True\n    wait_for_condition() >> process_when_ready()\n\n\n# Instanciar el DAG\ncustom_sensor_dag = dag_with_custom_sensor()\n\nprint(\"Sensor personalizado con decorador @task.sensor creado\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Sensor Personalizado con Decorador @task.sensor",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# SENSORS - ESPERAR CONDICIONES\n# ============================================\n\nfrom airflow.decorators import dag, task\nfrom airflow.sensors.filesystem import FileSensor\nfrom airflow.sensors.time_sensor import TimeSensor\nfrom datetime import datetime\nimport os\n\n\n@dag(\n    dag_id='ejemplo_sensors',\n    start_date=datetime(2024, 1, 1),\n    schedule_interval='@hourly',\n    catchup=False,\n    tags=['sensors', 'avanzado']\n)\ndef pipeline_with_sensors():\n    \"\"\"DAG que demuestra el uso de Sensors\"\"\"\n    \n    # ============================================\n    # TIMESENSOR: Espera hasta una hora espec√≠fica\n    # ============================================\n    wait_for_morning = TimeSensor(\n        task_id='wait_for_9am',\n        # target_time: Hora objetivo a esperar\n        target_time=datetime.strptime('09:00:00', '%H:%M:%S').time(),\n        # poke_interval: Cada cu√°nto verificar (300 seg = 5 min)\n        poke_interval=300,\n        # mode='reschedule': Libera el worker mientras espera (m√°s eficiente)\n        mode='reschedule'\n    )\n    \n    @task\n    def create_temp_file():\n        \"\"\"Crea archivo temporal para demostraci√≥n\"\"\"\n        filepath = '/tmp/airflow_sensor_test.txt'\n        with open(filepath, 'w') as f:\n            f.write('Archivo de prueba para sensor')\n        print(f\"Archivo creado: {filepath}\")\n        return filepath\n    \n    # ============================================\n    # FILESENSOR: Espera a que exista un archivo\n    # ============================================\n    wait_for_file = FileSensor(\n        task_id='wait_for_data_file',\n        # filepath: Ruta del archivo a esperar\n        filepath='/tmp/airflow_sensor_test.txt',\n        # poke_interval: Verificar cada 30 segundos\n        poke_interval=30,\n        # timeout: Tiempo m√°ximo de espera (600 seg = 10 min)\n        timeout=600,\n        # mode='poke': Ocupa el worker mientras espera\n        mode='poke'\n    )\n    \n    @task\n    def process_file():\n        \"\"\"Procesa el archivo una vez detectado por el sensor\"\"\"\n        print(\"Archivo detectado, procesando...\")\n        with open('/tmp/airflow_sensor_test.txt', 'r') as f:\n            content = f.read()\n        print(f\"Contenido: {content}\")\n        return \"Procesamiento completado\"\n    \n    @task\n    def cleanup():\n        \"\"\"Limpia archivos temporales\"\"\"\n        filepath = '/tmp/airflow_sensor_test.txt'\n        if os.path.exists(filepath):\n            os.remove(filepath)\n            print(f\"Archivo eliminado: {filepath}\")\n    \n    # ============================================\n    # DEFINIR FLUJO CON SENSORS\n    # ============================================\n    # El sensor 'wait_for_file' bloquea la ejecuci√≥n hasta que el archivo exista\n    file_path = create_temp_file()\n    file_path >> wait_for_file >> process_file() >> cleanup()\n\n\n# Instanciar el DAG\nsensors_dag = pipeline_with_sensors()\n\nprint(\"DAG con Sensors creado\")\nprint(\"Los sensors esperan condiciones antes de continuar\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 17. Sensors en Airflow\n\n### ¬øQu√© son los Sensors?\n\nLos **Sensors** son un tipo especial de operador que:\n- **Esperan** a que se cumpla una condici√≥n espec√≠fica\n- **Verifican peri√≥dicamente** (polling) el estado\n- **Bloquean** la ejecuci√≥n hasta que la condici√≥n se cumple\n\n### Tipos Comunes de Sensors\n- **FileSensor**: Espera a que exista un archivo\n- **ExternalTaskSensor**: Espera a que otra tarea termine\n- **TimeSensor**: Espera hasta una hora espec√≠fica\n- **HttpSensor**: Espera respuesta HTTP\n- **SqlSensor**: Espera resultado SQL\n\n### Par√°metros Importantes\n- `poke_interval`: Tiempo entre verificaciones (segundos)\n- `timeout`: Tiempo m√°ximo de espera\n- `mode`: 'poke' (ocupa worker) o 'reschedule' (libera worker)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# PIPELINE ETL CON TASKFLOW API\n# ============================================\n\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\nfrom typing import List, Dict  # Para type hints\n\n\n@dag(\n    dag_id='etl_taskflow_completo',\n    start_date=datetime(2024, 1, 1),\n    schedule_interval='@daily',\n    catchup=False,\n    description='Pipeline ETL usando TaskFlow API',\n    tags=['taskflow', 'etl', 'avanzado']\n)\ndef etl_pipeline_taskflow():\n    \"\"\"Pipeline ETL completo usando decoradores\"\"\"\n    \n    @task\n    def extract_from_sources() -> List[Dict]:\n        \"\"\"Extrae datos - Retorna lista de diccionarios\"\"\"\n        print(\"Extrayendo datos...\")\n        return [\n            {'id': 1, 'value': 100, 'category': 'A'},\n            {'id': 2, 'value': 200, 'category': 'B'},\n            {'id': 3, 'value': 150, 'category': 'A'},\n            {'id': 4, 'value': 300, 'category': 'C'}\n        ]\n    \n    @task\n    def validate_data(data: List[Dict]) -> List[Dict]:\n        \"\"\"Valida datos - Recibe y retorna lista de diccionarios\"\"\"\n        print(f\"Validando {len(data)} registros...\")\n        # List comprehension para filtrar datos v√°lidos\n        valid_data = [item for item in data if item['value'] > 0]\n        print(f\"Registros v√°lidos: {len(valid_data)}\")\n        return valid_data\n    \n    @task\n    def transform_by_category(data: List[Dict]) -> Dict[str, List[Dict]]:\n        \"\"\"Agrupa y transforma por categor√≠a - Retorna diccionario\"\"\"\n        print(\"Transformando datos por categor√≠a...\")\n        categories = {}\n        \n        # Agrupar por categor√≠a y duplicar valores\n        for item in data:\n            cat = item['category']\n            if cat not in categories:\n                categories[cat] = []\n            categories[cat].append({\n                'id': item['id'],\n                'value_doubled': item['value'] * 2,  # Transformaci√≥n\n                'category': cat\n            })\n        return categories\n    \n    @task\n    def calculate_statistics(grouped_data: Dict[str, List[Dict]]) -> Dict:\n        \"\"\"Calcula estad√≠sticas por categor√≠a\"\"\"\n        print(\"Calculando estad√≠sticas...\")\n        stats = {}\n        \n        # Calcular totales y promedios por categor√≠a\n        for category, items in grouped_data.items():\n            total = sum(item['value_doubled'] for item in items)\n            count = len(items)\n            stats[category] = {\n                'total': total,\n                'count': count,\n                'average': total / count if count > 0 else 0\n            }\n        return stats\n    \n    @task\n    def load_to_database(stats: Dict) -> str:\n        \"\"\"Carga estad√≠sticas a base de datos\"\"\"\n        print(\"Cargando datos a la base de datos...\")\n        for category, data in stats.items():\n            print(f\"  Categor√≠a {category}: Total={data['total']}, Promedio={data['average']:.2f}\")\n        return \"Carga completada exitosamente\"\n    \n    @task\n    def send_notification(status: str) -> None:\n        \"\"\"Env√≠a notificaci√≥n final\"\"\"\n        print(f\"\\n{'='*50}\")\n        print(f\"NOTIFICACI√ìN: {status}\")\n        print(f\"Pipeline completado en: {datetime.now()}\")\n        print(f\"{'='*50}\\n\")\n    \n    # ============================================\n    # DEFINIR EL FLUJO DEL PIPELINE\n    # ============================================\n    # Cada variable contiene el resultado de la tarea anterior\n    # El paso de datos es AUTOM√ÅTICO\n    \n    raw_data = extract_from_sources()           # 1. Extraer\n    valid_data = validate_data(raw_data)        # 2. Validar (recibe raw_data)\n    grouped_data = transform_by_category(valid_data)  # 3. Transformar (recibe valid_data)\n    statistics = calculate_statistics(grouped_data)   # 4. Calcular stats (recibe grouped_data)\n    status = load_to_database(statistics)       # 5. Cargar (recibe statistics)\n    send_notification(status)                   # 6. Notificar (recibe status)\n\n\n# Instanciar el DAG\netl_taskflow_dag = etl_pipeline_taskflow()\n\nprint(\"Pipeline ETL con TaskFlow API creado\")\nprint(\"Ventajas: Type hints, paso autom√°tico de datos, c√≥digo m√°s legible\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Pipeline ETL Completo con TaskFlow API",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# TASKFLOW API (Airflow 2.0+) - MODERNO\n# ============================================\n# Forma moderna y m√°s limpia de crear DAGs\n\nfrom airflow.decorators import dag, task\nfrom datetime import datetime\n\n\n# @dag: Decorador que convierte una funci√≥n en un DAG\n@dag(\n    dag_id='taskflow_ejemplo',\n    start_date=datetime(2024, 1, 1),\n    schedule_interval='@daily',\n    catchup=False,\n    tags=['taskflow', 'moderno']\n)\ndef pipeline_taskflow():\n    \"\"\"Funci√≥n que define el DAG completo\"\"\"\n    \n    # @task: Decorador que convierte una funci√≥n en una tarea\n    @task\n    def extract() -> dict:\n        \"\"\"Extrae datos - El tipo de retorno es expl√≠cito\"\"\"\n        return {'data': 100}\n    \n    @task\n    def transform(data: dict) -> int:\n        \"\"\"Transforma datos - Recibe data como par√°metro directamente\n        ¬°No necesitamos XCom manual!\"\"\"\n        return data['data'] * 2\n    \n    @task\n    def load(value: int) -> None:\n        \"\"\"Carga datos - Type hints hacen el c√≥digo m√°s claro\"\"\"\n        print(f\"Valor final: {value}\")\n    \n    # ============================================\n    # DEFINIR EL FLUJO - Syntax Pythonic\n    # ============================================\n    # El paso de datos es AUTOM√ÅTICO entre tareas\n    data = extract()                # Llama a extract()\n    transformed = transform(data)   # Pasa 'data' autom√°ticamente\n    load(transformed)               # Pasa 'transformed' autom√°ticamente\n\n\n# Instanciar el DAG (necesario para que Airflow lo detecte)\ndag_taskflow = pipeline_taskflow()\n\nprint(\"TaskFlow API: C√≥digo m√°s limpio y Pythonic\")\nprint(\"El paso de datos entre tareas es autom√°tico\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# FORMA TRADICIONAL (Airflow 1.x)\n# ============================================\n# Este es el m√©todo antiguo de crear DAGs en Airflow\n\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\n\n\ndef extract_tradicional(**context):\n    \"\"\"Funci√≥n que extrae datos\"\"\"\n    return {'data': 100}\n\n\ndef transform_tradicional(**context):\n    \"\"\"Funci√≥n que transforma datos\"\"\"\n    # PROBLEMA: Necesitamos usar XCom manualmente\n    ti = context['ti']  # Obtener Task Instance\n    data = ti.xcom_pull(task_ids='extract')  # Recuperar datos manualmente\n    return data['data'] * 2\n\n\n# Crear DAG tradicional usando 'with' statement\nwith DAG('tradicional', start_date=datetime(2024, 1, 1)) as dag_trad:\n    \n    # Crear tarea de extracci√≥n\n    extract = PythonOperator(\n        task_id='extract',\n        python_callable=extract_tradicional\n    )\n    \n    # Crear tarea de transformaci√≥n\n    transform = PythonOperator(\n        task_id='transform',\n        python_callable=transform_tradicional\n    )\n    \n    # Definir dependencias\n    extract >> transform\n\nprint(\"Forma tradicional: Requiere m√°s c√≥digo y manejo manual de XCom\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Espacio para tus ejercicios\n",
    "\n",
    "# Ejercicio 1: Tu c√≥digo aqu√≠\n",
    "\n",
    "\n",
    "# Ejercicio 2: Tu c√≥digo aqu√≠\n",
    "\n",
    "\n",
    "# Ejercicio 3: Tu c√≥digo aqu√≠\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Recursos Adicionales\n",
    "\n",
    "### Documentaci√≥n Oficial\n",
    "- [Apache Airflow Documentation](https://airflow.apache.org/docs/)\n",
    "- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n",
    "\n",
    "### Operadores √ötiles\n",
    "- **PythonOperator**: Ejecutar funciones Python\n",
    "- **BashOperator**: Ejecutar comandos Bash\n",
    "- **EmailOperator**: Enviar emails\n",
    "- **HttpOperator**: Llamadas HTTP/API\n",
    "- **SqlOperator**: Consultas SQL\n",
    "- **BranchPythonOperator**: Ramificaci√≥n condicional\n",
    "\n",
    "### Providers\n",
    "- AWS\n",
    "- Google Cloud\n",
    "- Azure\n",
    "- Kubernetes\n",
    "- Snowflake\n",
    "- Y muchos m√°s..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusi√≥n\n",
    "\n",
    "En este tutorial has aprendido:\n",
    "\n",
    "‚úÖ Conceptos fundamentales de Apache Airflow  \n",
    "‚úÖ C√≥mo crear y configurar DAGs  \n",
    "‚úÖ Diferentes tipos de operadores  \n",
    "‚úÖ Manejo de dependencias entre tareas  \n",
    "‚úÖ Ejecuci√≥n paralela de tareas  \n",
    "‚úÖ Compartir datos con XCom  \n",
    "‚úÖ Programaci√≥n de DAGs  \n",
    "‚úÖ Implementaci√≥n de pipelines ETL completos  \n",
    "‚úÖ Mejores pr√°cticas  \n",
    "\n",
    "### Pr√≥ximos Pasos\n",
    "1. Practica creando tus propios DAGs\n",
    "2. Explora diferentes operadores\n",
    "3. Implementa pipelines de datos reales\n",
    "4. Aprende sobre Airflow en producci√≥n\n",
    "5. Explora integraciones con otras herramientas\n",
    "\n",
    "¬°Feliz orquestaci√≥n de flujos de trabajo! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}