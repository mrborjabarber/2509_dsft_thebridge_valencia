{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5Iw2deeevLb"
   },
   "source": [
    "# Análisis de Sentimientos IMDB\n",
    "\n",
    "Los datos están divididos equitativamente con 25k reseñas destinadas para entrenamiento y 25k para probar tu clasificador. Además, cada conjunto tiene 12.5k reseñas positivas y 12.5k negativas.\n",
    "\n",
    "IMDb permite a los usuarios calificar películas en una escala del 1 al 10. Para etiquetar estas reseñas, el curador de los datos etiquetó cualquier cosa con ≤ 4 estrellas como negativa y cualquier cosa con ≥ 7 estrellas como positiva. Las reseñas con 5 o 6 estrellas fueron excluidas.\n",
    "\n",
    "**Importar las librerías necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-jABp8j_evLc"
   },
   "outputs": [],
   "source": [
    "# En Google Colab !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dqh7AvdNevLd"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTACIÓN DE LIBRERÍAS NECESARIAS\n",
    "# =============================================================================\n",
    "# numpy: para operaciones numéricas y manejo de arrays\n",
    "# pandas: para manipulación de datos estructurados (aunque no se usa mucho aquí)\n",
    "# os: para operaciones con el sistema operativo (rutas de archivos)\n",
    "# re: para expresiones regulares (limpieza de texto)\n",
    "# warnings: para suprimir mensajes de advertencia que no son críticos\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignoramos warnings para tener output más limpio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hxr29u8evLd"
   },
   "source": [
    "**Cargar Datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "VAkwzdf8evLd"
   },
   "outputs": [],
   "source": [
    "#reviews_train = []\n",
    "#for line in open(os.getcwd() + '/data/imbd_train.txt', 'r', encoding='latin1'):\n",
    "\n",
    "#    reviews_train.append(line.strip())\n",
    "\n",
    "#reviews_test = []\n",
    "#for line in open(os.getcwd() + '/data/imbd_test.txt', 'r', encoding='latin1'):\n",
    "\n",
    "#    reviews_test.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zg6qXrtofJfX"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CARGA DE DATOS - REVIEWS DE PELÍCULAS\n",
    "# =============================================================================\n",
    "# Cargamos las reviews de entrenamiento desde el archivo de texto\n",
    "# Cada línea del archivo es una review completa de una película\n",
    "\n",
    "reviews_train = []  # Lista para almacenar las reviews de entrenamiento\n",
    "for line in open('imbd_train.txt', 'r', encoding='latin1'):\n",
    "    # strip() elimina espacios en blanco y saltos de línea al inicio y final\n",
    "    reviews_train.append(line.strip())\n",
    "\n",
    "# Cargamos las reviews de test (para evaluar el modelo después)\n",
    "reviews_test = []  # Lista para almacenar las reviews de test\n",
    "for line in open('imbd_test.txt', 'r', encoding='latin1'):\n",
    "    reviews_test.append(line.strip())\n",
    "\n",
    "# Dataset: 25,000 reviews de entrenamiento y 25,000 de test\n",
    "# Train: primeras 12,500 son positivas, últimas 12,500 son negativas\n",
    "# Test: misma estructura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWd7RlalevLe",
    "outputId": "54bd3d88-952d-421a-dad4-8a7101a4f7d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "####################\n",
      "Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.\n",
      "####################\n",
      "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPLORACIÓN INICIAL - Visualizar algunas reviews\n",
    "# =============================================================================\n",
    "# Imprimimos las primeras 3 reviews para ver cómo se ven los datos originales\n",
    "# Esto nos ayuda a identificar qué tipo de limpieza necesitaremos después\n",
    "\n",
    "for i in range(3):\n",
    "    print('####################')\n",
    "    print(reviews_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f2owbIgevLe"
   },
   "source": [
    "**Ver uno de los elementos de la lista**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "SX9w-VB6evLe",
    "outputId": "ea009ded-e73c-4053-80b5-e9d1b2c81eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFICAR TAMAÑO DEL DATASET Y VER UN EJEMPLO\n",
    "# =============================================================================\n",
    "# Comprobamos cuántas reviews tenemos en cada conjunto\n",
    "\n",
    "print(len(reviews_train))  # Debería ser 25,000\n",
    "print(len(reviews_test))   # Debería ser 25,000\n",
    "\n",
    "# Mostramos una review específica (la número 5) para ver su contenido\n",
    "reviews_train[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKmnrBdcevLe"
   },
   "source": [
    "El texto sin procesar está bastante desordenado para estas reseñas, así que antes de poder hacer cualquier análisis necesitamos limpiar las cosas\n",
    "\n",
    "\n",
    "**Usar expresiones regulares para eliminar los caracteres que no son texto y las etiquetas html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nzC4vDehevLf"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LIMPIEZA Y PREPROCESAMIENTO DEL TEXTO\n",
    "# =============================================================================\n",
    "# Las reviews tienen ruido: puntuación, etiquetas HTML, números, etc.\n",
    "# Necesitamos limpiar el texto para que el modelo se enfoque en las palabras importantes\n",
    "\n",
    "import re\n",
    "\n",
    "# PASO 1: Definir patrones de expresiones regulares para limpieza\n",
    "\n",
    "# REPLACE_NO_SPACE: elimina caracteres que NO queremos (los sustituye por cadena vacía)\n",
    "# Incluye: puntos, punto y coma, dos puntos, exclamaciones, interrogaciones, comas,\n",
    "#          comillas, paréntesis, corchetes y números\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "\n",
    "# REPLACE_WITH_SPACE: caracteres que queremos sustituir por espacios\n",
    "# Incluye: etiquetas HTML como <br/><br/>, guiones y barras\n",
    "# Los sustituimos por espacio para no juntar palabras\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "# Constantes para las sustituciones\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "# PASO 2: Función de preprocesamiento\n",
    "def preprocess_reviews(reviews):\n",
    "    \"\"\"\n",
    "    Limpia una lista de reviews de texto:\n",
    "    1. Convierte todo a minúsculas (para normalizar)\n",
    "    2. Elimina signos de puntuación y números\n",
    "    3. Sustituye etiquetas HTML y guiones por espacios\n",
    "    \n",
    "    Args:\n",
    "        reviews: lista de strings con las reviews originales\n",
    "    \n",
    "    Returns:\n",
    "        reviews_clean: lista de strings con las reviews limpias\n",
    "    \"\"\"\n",
    "    # Primera pasada: eliminar signos de puntuación y números\n",
    "    # .lower() convierte todo a minúsculas\n",
    "    # .sub() sustituye lo que coincida con el patrón\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    \n",
    "    # Segunda pasada: sustituir etiquetas HTML y guiones por espacios\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# PASO 3: Aplicar la limpieza a nuestros datos\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)\n",
    "\n",
    "# Ahora las reviews están limpias y listas para vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "azqKzkZ5evLf",
    "outputId": "bb874673-7bf5-4f30-96af-7a3a8ab7d479"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this isn't the comedic robin williams nor is it the quirky insane robin williams of recent thriller fame this is a hybrid of the classic drama without over dramatization mixed with robin's new love of the thriller but this isn't a thriller per se this is more a mystery suspense vehicle through which williams attempts to locate a sick boy and his keeper also starring sandra oh and rory culkin this suspense drama plays pretty much like a news report until william's character gets close to achieving his goal i must say that i was highly entertained though this movie fails to teach guide inspect or amuse it felt more like i was watching a guy williams as he was actually performing the actions from a third person perspective in other words it felt real and i was able to subscribe to the premise of the story all in all it's worth a watch though it's definitely not friday saturday night fare it rates a   from the fiend \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARAR: Review antes y después de la limpieza\n",
    "# =============================================================================\n",
    "# Mostramos la misma review (índice 5) después de la limpieza\n",
    "# Compara con la versión original que vimos antes\n",
    "# Observa: texto en minúsculas, sin puntuación, sin números, sin HTML\n",
    "\n",
    "reviews_train_clean[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXp9MhOAevLf"
   },
   "source": [
    "# Vectorización\n",
    "Para que estos datos tengan sentido para nuestro algoritmo de aprendizaje automático necesitaremos convertir cada reseña a una representación numérica, que llamamos vectorización.\n",
    "\n",
    "La forma más simple de esto es crear una matriz muy grande con una columna para cada palabra única en tu corpus (donde el corpus son todas las 50k reseñas en nuestro caso). Luego transformamos cada reseña en una fila que contiene 0s y 1s, donde 1 significa que la palabra del corpus correspondiente a esa columna aparece en esa reseña. Dicho esto, cada fila de la matriz será muy dispersa (mayormente ceros). Este proceso también se conoce como codificación one-hot. Usar el método *CountVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HUzFlZrbevLf"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTAR CountVectorizer\n",
    "# =============================================================================\n",
    "# CountVectorizer es la herramienta de sklearn que convierte texto en vectores numéricos\n",
    "# Es necesario porque los modelos de ML solo entienden números, no palabras\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fuNDAV6EevLg"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EJEMPLO SIMPLE - Corpus de demostración\n",
    "# =============================================================================\n",
    "# Creamos un corpus pequeño de 4 documentos para entender cómo funciona la vectorización\n",
    "# Este ejemplo nos ayudará a visualizar el proceso antes de aplicarlo a nuestras 25,000 reviews\n",
    "\n",
    "corpus = [\n",
    "     'This is the first document.',      # Documento 0\n",
    "     'This document is the second document.',  # Documento 1\n",
    "     'And this is the third one.',       # Documento 2\n",
    "     'Is this the first document?',      # Documento 3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICAK7Sd6evLg",
    "outputId": "142e04bf-0431-4a3c-d838-95b29e879983"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VECTORIZACIÓN - Ejemplo básico\n",
    "# =============================================================================\n",
    "# Proceso de vectorización:\n",
    "# 1. El vectorizador identifica todas las palabras únicas del corpus (vocabulario)\n",
    "# 2. Cada palabra única se convierte en una \"feature\" (columna)\n",
    "# 3. El vectorizador crea una matriz donde cada fila es un documento\n",
    "\n",
    "vectorizer = CountVectorizer()  # Creamos el vectorizador\n",
    "X = vectorizer.fit_transform(corpus)  # fit: aprende el vocabulario, transform: convierte a matriz\n",
    "\n",
    "# Veamos qué palabras encontró (el vocabulario, ordenado alfabéticamente)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZoKfXgpNevLg",
    "outputId": "597c1bfe-1e7a-4010-b5d4-f1e3e93d277e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TAMAÑO DEL VOCABULARIO\n",
    "# =============================================================================\n",
    "# Contamos cuántas palabras únicas hay en nuestro pequeño corpus\n",
    "# En este caso son 9 palabras diferentes\n",
    "\n",
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "rv2g2_lAevLg",
    "outputId": "69f1bca3-3d0e-46af-c178-ef13adbdc802"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         2      0   1    0       1    1      0     1\n",
       "2    1         0      0   1    1       0    1      1     1\n",
       "3    0         1      1   1    0       0    1      0     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZAR LA MATRIZ DE VECTORIZACIÓN\n",
    "# =============================================================================\n",
    "# Convertimos la matriz dispersa (sparse matrix) a DataFrame para visualizarla mejor\n",
    "# Cada fila = un documento\n",
    "# Cada columna = una palabra del vocabulario\n",
    "# Cada celda = número de veces que la palabra aparece en ese documento\n",
    "\n",
    "# Ejemplo de lectura:\n",
    "# - Fila 0 (documento 0): 'document' aparece 1 vez, 'first' aparece 1 vez, 'is' aparece 1 vez...\n",
    "# - Fila 1 (documento 1): 'document' aparece 2 veces (¡fíjate que cuenta!)\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qlWCFx4uevLg"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VECTORIZACIÓN BINARIA - Para nuestras reviews de películas\n",
    "# =============================================================================\n",
    "# Ahora aplicamos CountVectorizer a nuestras reviews reales con binary=True\n",
    "# \n",
    "# ¿Por qué binary=True?\n",
    "# - En lugar de contar cuántas veces aparece cada palabra (1, 2, 3, ...)\n",
    "# - Solo marcamos si la palabra está presente (1) o ausente (0)\n",
    "# - Esto suele funcionar mejor para análisis de sentimiento\n",
    "# - Evita que palabras muy repetidas dominen el modelo\n",
    "\n",
    "baseline_vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# fit(): el vectorizador \"aprende\" todas las palabras únicas de las 25,000 reviews de train\n",
    "baseline_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "# transform(): convierte cada review en un vector numérico\n",
    "# ¡IMPORTANTE! Aplicamos el MISMO vectorizador a test (mismo vocabulario, mismas columnas)\n",
    "X_baseline = baseline_vectorizer.transform(reviews_train_clean)\n",
    "X_test_baseline = baseline_vectorizer.transform(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NqX_V3DeevLh",
    "outputId": "7dca4698-1dff-49c1-a421-26d3d19bb7f7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 3410713 stored elements and shape (25000, 87063)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# INSPECCIONAR LA MATRIZ RESULTANTE\n",
    "# =============================================================================\n",
    "# X_baseline es una matriz dispersa (sparse matrix) porque tiene muchos ceros\n",
    "# Dimensiones: (25000 filas, 87063 columnas)\n",
    "# - 25,000 filas = 25,000 reviews\n",
    "# - 87,063 columnas = 87,063 palabras únicas encontradas en el corpus\n",
    "# - 3,410,713 elementos almacenados = celdas con valor 1 (el resto son 0)\n",
    "#\n",
    "# ¿Por qué sparse matrix?\n",
    "# - Si guardáramos todos los valores (incluyendo ceros) ocuparía muchísima memoria\n",
    "# - Sparse solo guarda las posiciones con 1, ahorrando espacio\n",
    "\n",
    "X_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uNXCZeyRevLh",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7098c18f-b38f-4118-c79f-3a4df552507b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bromwell': 9819,\n",
       " 'high': 35211,\n",
       " 'is': 39472,\n",
       " 'cartoon': 11686,\n",
       " 'comedy': 14754,\n",
       " 'it': 39642,\n",
       " 'ran': 61772,\n",
       " 'at': 4537,\n",
       " 'the': 76725,\n",
       " 'same': 66138,\n",
       " 'time': 77626,\n",
       " 'as': 4211,\n",
       " 'some': 71188,\n",
       " 'other': 54861,\n",
       " 'programs': 60156,\n",
       " 'about': 284,\n",
       " 'school': 67025,\n",
       " 'life': 44297,\n",
       " 'such': 74177,\n",
       " 'teachers': 75997,\n",
       " 'my': 51490,\n",
       " 'years': 86260,\n",
       " 'in': 37733,\n",
       " 'teaching': 76000,\n",
       " 'profession': 60088,\n",
       " 'lead': 43605,\n",
       " 'me': 47976,\n",
       " 'to': 77922,\n",
       " 'believe': 6894,\n",
       " 'that': 76671,\n",
       " 'satire': 66462,\n",
       " 'much': 51018,\n",
       " 'closer': 14125,\n",
       " 'reality': 62242,\n",
       " 'than': 76643,\n",
       " 'scramble': 67253,\n",
       " 'survive': 74812,\n",
       " 'financially': 27904,\n",
       " 'insightful': 38615,\n",
       " 'students': 73768,\n",
       " 'who': 84627,\n",
       " 'can': 11132,\n",
       " 'see': 67695,\n",
       " 'right': 64476,\n",
       " 'through': 77369,\n",
       " 'their': 76762,\n",
       " 'pathetic': 56362,\n",
       " 'pomp': 58787,\n",
       " 'pettiness': 57341,\n",
       " 'of': 53843,\n",
       " 'whole': 84639,\n",
       " 'situation': 69959,\n",
       " 'all': 2038,\n",
       " 'remind': 63323,\n",
       " 'schools': 67049,\n",
       " 'knew': 42226,\n",
       " 'and': 2762,\n",
       " 'when': 84450,\n",
       " 'saw': 66588,\n",
       " 'episode': 24853,\n",
       " 'which': 84476,\n",
       " 'student': 73764,\n",
       " 'repeatedly': 63494,\n",
       " 'tried': 79132,\n",
       " 'burn': 10435,\n",
       " 'down': 22050,\n",
       " 'immediately': 37436,\n",
       " 'recalled': 62427,\n",
       " 'classic': 13840,\n",
       " 'line': 44572,\n",
       " 'inspector': 38661,\n",
       " 'here': 34881,\n",
       " 'sack': 65826,\n",
       " 'one': 54223,\n",
       " 'your': 86541,\n",
       " 'welcome': 84135,\n",
       " 'expect': 25952,\n",
       " 'many': 46877,\n",
       " 'adults': 1131,\n",
       " 'age': 1445,\n",
       " 'think': 77056,\n",
       " 'far': 26802,\n",
       " 'fetched': 27446,\n",
       " 'what': 84379,\n",
       " 'pity': 58038,\n",
       " 'isn': 39565,\n",
       " 'homelessness': 35859,\n",
       " 'or': 54556,\n",
       " 'houselessness': 36368,\n",
       " 'george': 30901,\n",
       " 'carlin': 11519,\n",
       " 'stated': 72756,\n",
       " 'has': 34120,\n",
       " 'been': 6660,\n",
       " 'an': 2670,\n",
       " 'issue': 39614,\n",
       " 'for': 28846,\n",
       " 'but': 10574,\n",
       " 'never': 52363,\n",
       " 'plan': 58141,\n",
       " 'help': 34743,\n",
       " 'those': 77231,\n",
       " 'on': 54196,\n",
       " 'street': 73546,\n",
       " 'were': 84257,\n",
       " 'once': 54207,\n",
       " 'considered': 15688,\n",
       " 'human': 36593,\n",
       " 'did': 20215,\n",
       " 'everything': 25523,\n",
       " 'from': 29698,\n",
       " 'going': 31740,\n",
       " 'work': 85510,\n",
       " 'vote': 83129,\n",
       " 'matter': 47565,\n",
       " 'most': 50516,\n",
       " 'people': 56856,\n",
       " 'homeless': 35858,\n",
       " 'just': 41023,\n",
       " 'lost': 45327,\n",
       " 'cause': 12043,\n",
       " 'while': 84492,\n",
       " 'worrying': 85625,\n",
       " 'things': 77032,\n",
       " 'racism': 61457,\n",
       " 'war': 83515,\n",
       " 'iraq': 39359,\n",
       " 'pressuring': 59700,\n",
       " 'kids': 41828,\n",
       " 'succeed': 74135,\n",
       " 'technology': 76091,\n",
       " 'elections': 23680,\n",
       " 'inflation': 38309,\n",
       " 'if': 37183,\n",
       " 'they': 76956,\n",
       " 'll': 44861,\n",
       " 'be': 6399,\n",
       " 'next': 52455,\n",
       " 'end': 24262,\n",
       " 'up': 81499,\n",
       " 'streets': 73557,\n",
       " 'you': 86483,\n",
       " 'given': 31356,\n",
       " 'bet': 7290,\n",
       " 'live': 44794,\n",
       " 'month': 50173,\n",
       " 'without': 85171,\n",
       " 'luxuries': 45842,\n",
       " 'had': 33377,\n",
       " 'home': 35844,\n",
       " 'entertainment': 24667,\n",
       " 'sets': 68267,\n",
       " 'bathroom': 6268,\n",
       " 'pictures': 57717,\n",
       " 'wall': 83402,\n",
       " 'computer': 15211,\n",
       " 'treasure': 78953,\n",
       " 'like': 44416,\n",
       " 'goddard': 31665,\n",
       " 'bolt': 8627,\n",
       " 'lesson': 44051,\n",
       " 'mel': 48272,\n",
       " 'brooks': 9853,\n",
       " 'directs': 20600,\n",
       " 'stars': 72697,\n",
       " 'plays': 58278,\n",
       " 'rich': 64333,\n",
       " 'man': 46595,\n",
       " 'world': 85574,\n",
       " 'until': 81403,\n",
       " 'deciding': 18739,\n",
       " 'make': 46398,\n",
       " 'with': 85140,\n",
       " 'sissy': 69915,\n",
       " 'rival': 64672,\n",
       " 'jeffery': 40268,\n",
       " 'tambor': 75653,\n",
       " 'he': 34360,\n",
       " 'thirty': 77120,\n",
       " 'days': 18432,\n",
       " 'succeeds': 74139,\n",
       " 'do': 21408,\n",
       " 'wants': 83513,\n",
       " 'future': 30031,\n",
       " 'project': 60181,\n",
       " 'making': 46429,\n",
       " 'more': 50319,\n",
       " 'buildings': 10203,\n",
       " 'where': 84457,\n",
       " 'thrown': 77400,\n",
       " 'bracelet': 9221,\n",
       " 'his': 35480,\n",
       " 'leg': 43791,\n",
       " 'monitor': 50057,\n",
       " 'every': 25510,\n",
       " 'move': 50682,\n",
       " 'step': 72949,\n",
       " 'off': 53853,\n",
       " 'sidewalk': 69518,\n",
       " 'nickname': 52543,\n",
       " 'pepto': 56913,\n",
       " 'by': 10691,\n",
       " 'vagrant': 81877,\n",
       " 'after': 1360,\n",
       " 'written': 85840,\n",
       " 'forehead': 28906,\n",
       " 'meets': 48197,\n",
       " 'characters': 12612,\n",
       " 'including': 37879,\n",
       " 'woman': 85298,\n",
       " 'name': 51704,\n",
       " 'molly': 49931,\n",
       " 'lesley': 44036,\n",
       " 'ann': 3025,\n",
       " 'warren': 83612,\n",
       " 'ex': 25627,\n",
       " 'dancer': 18094,\n",
       " 'got': 32084,\n",
       " 'divorce': 21349,\n",
       " 'before': 6701,\n",
       " 'losing': 45322,\n",
       " 'her': 34853,\n",
       " 'pals': 55743,\n",
       " 'sailor': 65978,\n",
       " 'howard': 36404,\n",
       " 'morris': 50429,\n",
       " 'fumes': 29866,\n",
       " 'teddy': 76097,\n",
       " 'wilson': 84909,\n",
       " 'are': 3794,\n",
       " 'already': 2256,\n",
       " 'used': 81711,\n",
       " 're': 62137,\n",
       " 'survivors': 74826,\n",
       " 'not': 53085,\n",
       " 'reaching': 62149,\n",
       " 'mutual': 51469,\n",
       " 'agreements': 1561,\n",
       " 'being': 6826,\n",
       " 'fight': 27612,\n",
       " 'flight': 28404,\n",
       " 'kill': 41873,\n",
       " 'killed': 41878,\n",
       " 'love': 45429,\n",
       " 'connection': 15579,\n",
       " 'between': 7365,\n",
       " 'wasn': 83673,\n",
       " 'necessary': 52066,\n",
       " 'plot': 58391,\n",
       " 'found': 29165,\n",
       " 'stinks': 73159,\n",
       " 'observant': 53634,\n",
       " 'films': 27798,\n",
       " 'prior': 59895,\n",
       " 'shows': 69264,\n",
       " 'tender': 76323,\n",
       " 'side': 69480,\n",
       " 'compared': 15003,\n",
       " 'slapstick': 70221,\n",
       " 'blazing': 8028,\n",
       " 'saddles': 65866,\n",
       " 'young': 86514,\n",
       " 'frankenstein': 29331,\n",
       " 'spaceballs': 71597,\n",
       " 'show': 69221,\n",
       " 'having': 34273,\n",
       " 'something': 71222,\n",
       " 'valuable': 81966,\n",
       " 'day': 18410,\n",
       " 'hand': 33699,\n",
       " 'stupid': 73847,\n",
       " 'don': 21708,\n",
       " 'know': 42286,\n",
       " 'money': 50010,\n",
       " 'maybe': 47676,\n",
       " 'should': 69189,\n",
       " 'give': 31353,\n",
       " 'instead': 38702,\n",
       " 'using': 81743,\n",
       " 'monopoly': 50093,\n",
       " 'this': 77124,\n",
       " 'film': 27688,\n",
       " 'will': 84854,\n",
       " 'inspire': 38671,\n",
       " 'others': 54871,\n",
       " 'brilliant': 9653,\n",
       " 'over': 55140,\n",
       " 'acting': 701,\n",
       " 'best': 7257,\n",
       " 'dramatic': 22219,\n",
       " 'hobo': 35654,\n",
       " 'lady': 42885,\n",
       " 'have': 34253,\n",
       " 'ever': 25473,\n",
       " 'seen': 67745,\n",
       " 'scenes': 66851,\n",
       " 'clothes': 14144,\n",
       " 'warehouse': 83541,\n",
       " 'second': 67602,\n",
       " 'none': 52917,\n",
       " 'corn': 16276,\n",
       " 'face': 26376,\n",
       " 'good': 31862,\n",
       " 'anything': 3323,\n",
       " 'take': 75524,\n",
       " 'lawyers': 43547,\n",
       " 'also': 2267,\n",
       " 'superb': 74496,\n",
       " 'accused': 573,\n",
       " 'turncoat': 79650,\n",
       " 'selling': 67880,\n",
       " 'out': 54934,\n",
       " 'boss': 9016,\n",
       " 'dishonest': 20910,\n",
       " 'lawyer': 43544,\n",
       " 'shrugs': 69345,\n",
       " 'indifferently': 38087,\n",
       " 'says': 66648,\n",
       " 'three': 77306,\n",
       " 'funny': 29934,\n",
       " 'words': 85490,\n",
       " 'jeffrey': 40270,\n",
       " 'favorite': 27073,\n",
       " 'later': 43335,\n",
       " 'larry': 43248,\n",
       " 'sanders': 66221,\n",
       " 'fantastic': 26765,\n",
       " 'too': 78178,\n",
       " 'mad': 46082,\n",
       " 'millionaire': 49123,\n",
       " 'crush': 17438,\n",
       " 'ghetto': 31039,\n",
       " 'character': 12575,\n",
       " 'malevolent': 46488,\n",
       " 'usual': 81760,\n",
       " 'hospital': 36253,\n",
       " 'scene': 66831,\n",
       " 'invade': 39197,\n",
       " 'demolition': 19284,\n",
       " 'site': 69944,\n",
       " 'classics': 13852,\n",
       " 'look': 45171,\n",
       " 'legs': 43844,\n",
       " 'two': 79829,\n",
       " 'big': 7529,\n",
       " 'diggers': 20340,\n",
       " 'fighting': 27621,\n",
       " 'bleeds': 8055,\n",
       " 'movie': 50708,\n",
       " 'gets': 31005,\n",
       " 'better': 7327,\n",
       " 'each': 23040,\n",
       " 'quite': 61361,\n",
       " 'often': 53959,\n",
       " 'easily': 23134,\n",
       " 'underrated': 80490,\n",
       " 'inn': 38496,\n",
       " 'cannon': 11230,\n",
       " 'sure': 74701,\n",
       " 'its': 39760,\n",
       " 'flawed': 28310,\n",
       " 'does': 21526,\n",
       " 'realistic': 62227,\n",
       " 'view': 82635,\n",
       " 'unlike': 80984,\n",
       " 'say': 66614,\n",
       " 'how': 36403,\n",
       " 'citizen': 13703,\n",
       " 'kane': 41231,\n",
       " 'gave': 30600,\n",
       " 'lounge': 45400,\n",
       " 'singers': 69816,\n",
       " 'titanic': 77850,\n",
       " 'italians': 39652,\n",
       " 'idiots': 37148,\n",
       " 'jokes': 40668,\n",
       " 'fall': 26577,\n",
       " 'flat': 28259,\n",
       " 'still': 73100,\n",
       " 'very': 82434,\n",
       " 'lovable': 45426,\n",
       " 'way': 83855,\n",
       " 'comedies': 14748,\n",
       " 'pull': 60756,\n",
       " 'story': 73341,\n",
       " 'traditionally': 78617,\n",
       " 'reviled': 64143,\n",
       " 'members': 48361,\n",
       " 'society': 70981,\n",
       " 'truly': 79392,\n",
       " 'impressive': 37672,\n",
       " 'fisher': 28086,\n",
       " 'king': 41986,\n",
       " 'crap': 16866,\n",
       " 'either': 23591,\n",
       " 'only': 54317,\n",
       " 'complaint': 15087,\n",
       " 'cast': 11820,\n",
       " 'someone': 71200,\n",
       " 'else': 23861,\n",
       " 'director': 20579,\n",
       " 'writer': 85815,\n",
       " 'so': 70920,\n",
       " 'typical': 79884,\n",
       " 'was': 83638,\n",
       " 'less': 44043,\n",
       " 'movies': 50829,\n",
       " 'actually': 855,\n",
       " 'followable': 28733,\n",
       " 'leslie': 44037,\n",
       " 'made': 46110,\n",
       " 'she': 68705,\n",
       " 'under': 80396,\n",
       " 'rated': 61957,\n",
       " 'actress': 833,\n",
       " 'there': 76890,\n",
       " 'moments': 49956,\n",
       " 'could': 16521,\n",
       " 'fleshed': 28358,\n",
       " 'bit': 7772,\n",
       " 'probably': 59962,\n",
       " 'cut': 17774,\n",
       " 'room': 65130,\n",
       " 'worth': 85666,\n",
       " 'price': 59798,\n",
       " 'rent': 63439,\n",
       " 'overall': 55154,\n",
       " 'himself': 35370,\n",
       " 'job': 40537,\n",
       " 'characteristic': 12595,\n",
       " 'speaking': 71740,\n",
       " 'directly': 20577,\n",
       " 'audience': 4783,\n",
       " 'again': 1399,\n",
       " 'actor': 780,\n",
       " 'fume': 29865,\n",
       " 'both': 9037,\n",
       " 'played': 58246,\n",
       " 'parts': 56219,\n",
       " 'well': 84151,\n",
       " 'comedic': 14744,\n",
       " 'robin': 64814,\n",
       " 'williams': 84873,\n",
       " 'nor': 52989,\n",
       " 'quirky': 61355,\n",
       " 'insane': 38570,\n",
       " 'recent': 62460,\n",
       " 'thriller': 77328,\n",
       " 'fame': 26628,\n",
       " 'hybrid': 36886,\n",
       " 'drama': 22201,\n",
       " 'dramatization': 22232,\n",
       " 'mixed': 49686,\n",
       " 'new': 52377,\n",
       " 'per': 56917,\n",
       " 'se': 67482,\n",
       " 'mystery': 51547,\n",
       " 'suspense': 74862,\n",
       " 'vehicle': 82215,\n",
       " 'attempts': 4687,\n",
       " 'locate': 44940,\n",
       " 'sick': 69453,\n",
       " 'boy': 9173,\n",
       " 'keeper': 41529,\n",
       " 'starring': 72693,\n",
       " 'sandra': 66238,\n",
       " 'oh': 53985,\n",
       " 'rory': 65179,\n",
       " 'culkin': 17571,\n",
       " 'pretty': 59753,\n",
       " 'news': 52419,\n",
       " 'report': 63566,\n",
       " 'william': 84871,\n",
       " 'close': 14119,\n",
       " 'achieving': 607,\n",
       " 'goal': 31618,\n",
       " 'must': 51411,\n",
       " 'highly': 35233,\n",
       " 'entertained': 24647,\n",
       " 'though': 77238,\n",
       " 'fails': 26490,\n",
       " 'teach': 75993,\n",
       " 'guide': 33006,\n",
       " 'inspect': 38657,\n",
       " 'amuse': 2656,\n",
       " 'felt': 27310,\n",
       " 'watching': 83742,\n",
       " 'guy': 33230,\n",
       " 'performing': 57030,\n",
       " 'actions': 757,\n",
       " 'third': 77100,\n",
       " 'person': 57183,\n",
       " 'perspective': 57215,\n",
       " 'real': 62209,\n",
       " 'able': 235,\n",
       " 'subscribe': 74033,\n",
       " 'premise': 59569,\n",
       " 'watch': 83719,\n",
       " 'definitely': 18994,\n",
       " 'friday': 29572,\n",
       " 'saturday': 66505,\n",
       " 'night': 52612,\n",
       " 'fare': 26820,\n",
       " 'rates': 61962,\n",
       " 'fiend': 27579,\n",
       " 'yes': 86332,\n",
       " 'art': 4097,\n",
       " 'successfully': 74149,\n",
       " 'slow': 70470,\n",
       " 'paced': 55502,\n",
       " 'unfolds': 80731,\n",
       " 'nice': 52498,\n",
       " 'volumes': 83084,\n",
       " 'even': 25439,\n",
       " 'notice': 53128,\n",
       " 'happening': 33861,\n",
       " 'fine': 27921,\n",
       " 'performance': 56992,\n",
       " 'sexuality': 68378,\n",
       " 'angles': 2926,\n",
       " 'seem': 67728,\n",
       " 'unnecessary': 81064,\n",
       " 'affect': 1257,\n",
       " 'enjoy': 24488,\n",
       " 'however': 36415,\n",
       " 'core': 16248,\n",
       " 'engaging': 24414,\n",
       " 'doesn': 21532,\n",
       " 'rush': 65646,\n",
       " 'onto': 54355,\n",
       " 'grips': 32693,\n",
       " 'enough': 24547,\n",
       " 'keep': 41527,\n",
       " 'wondering': 85359,\n",
       " 'direction': 20560,\n",
       " 'use': 81708,\n",
       " 'lights': 44400,\n",
       " 'achieve': 598,\n",
       " 'desired': 19699,\n",
       " 'affects': 1270,\n",
       " 'unexpectedness': 80672,\n",
       " 'looking': 45182,\n",
       " 'lay': 43553,\n",
       " 'back': 5359,\n",
       " 'hear': 34444,\n",
       " 'thrilling': 77337,\n",
       " 'short': 69132,\n",
       " 'critically': 17210,\n",
       " 'acclaimed': 495,\n",
       " 'psychological': 60623,\n",
       " 'based': 6143,\n",
       " 'true': 79365,\n",
       " 'events': 25456,\n",
       " 'gabriel': 30096,\n",
       " 'celebrated': 12164,\n",
       " 'late': 43323,\n",
       " 'talk': 75597,\n",
       " 'host': 36267,\n",
       " 'becomes': 6593,\n",
       " 'captivated': 11371,\n",
       " 'harrowing': 34081,\n",
       " 'listener': 44714,\n",
       " 'adoptive': 1077,\n",
       " 'mother': 50538,\n",
       " 'toni': 78155,\n",
       " 'collette': 14580,\n",
       " 'troubling': 79314,\n",
       " 'questions': 61267,\n",
       " 'arise': 3879,\n",
       " 'finds': 27919,\n",
       " 'drawn': 22275,\n",
       " 'into': 39115,\n",
       " 'widening': 84736,\n",
       " 'hides': 35190,\n",
       " 'deadly': 18510,\n",
       " 'secretâ': 67641,\n",
       " 'according': 530,\n",
       " 'official': 53900,\n",
       " 'synopsis': 75320,\n",
       " 'really': 62265,\n",
       " 'stop': 73281,\n",
       " 'reading': 62185,\n",
       " 'these': 76937,\n",
       " 'comments': 14879,\n",
       " 'now': 53226,\n",
       " 'lose': 45314,\n",
       " 'ending': 24303,\n",
       " 'ms': 50987,\n",
       " 'planning': 58166,\n",
       " 'chopped': 13293,\n",
       " 'sent': 68019,\n",
       " 'deleted': 19128,\n",
       " 'land': 43073,\n",
       " 'overkill': 55263,\n",
       " 'nature': 51920,\n",
       " 'physical': 57624,\n",
       " 'mental': 48474,\n",
       " 'ailments': 1646,\n",
       " 'obvious': 53691,\n",
       " 'mr': 50948,\n",
       " 'returns': 64042,\n",
       " 'york': 86457,\n",
       " 'possibly': 59098,\n",
       " 'blindness': 8110,\n",
       " 'question': 61253,\n",
       " 'revelation': 64076,\n",
       " 'certain': 12301,\n",
       " 'highway': 35251,\n",
       " 'video': 82575,\n",
       " 'tape': 75755,\n",
       " 'would': 85679,\n",
       " 'benefit': 7031,\n",
       " 'editing': 23318,\n",
       " 'bobby': 8437,\n",
       " 'cannavale': 11210,\n",
       " 'jess': 40372,\n",
       " 'initially': 38453,\n",
       " 'believable': 6887,\n",
       " 'couple': 16620,\n",
       " 'establishing': 25212,\n",
       " 'relationship': 63138,\n",
       " 'might': 48976,\n",
       " 'helped': 34744,\n",
       " 'set': 68256,\n",
       " 'stage': 72472,\n",
       " 'otherwise': 54887,\n",
       " 'exemplary': 25833,\n",
       " 'offers': 53885,\n",
       " 'exceptionally': 25720,\n",
       " 'strong': 73682,\n",
       " 'characterization': 12598,\n",
       " 'gay': 30616,\n",
       " 'impersonation': 37559,\n",
       " 'anna': 3026,\n",
       " 'joe': 40583,\n",
       " 'morton': 50477,\n",
       " 'ashe': 4251,\n",
       " 'pete': 57300,\n",
       " 'logand': 45009,\n",
       " 'perfect': 56960,\n",
       " 'donna': 21755,\n",
       " 'belongs': 6958,\n",
       " 'creepy': 17065,\n",
       " 'hall': 33549,\n",
       " 'correct': 16342,\n",
       " 'saying': 66629,\n",
       " 'psycho': 60606,\n",
       " 'several': 68313,\n",
       " 'organizations': 54663,\n",
       " 'giving': 31364,\n",
       " 'awards': 5121,\n",
       " 'seemed': 67732,\n",
       " 'reach': 62145,\n",
       " 'women': 85324,\n",
       " 'due': 22653,\n",
       " 'slighter': 70362,\n",
       " 'dispersion': 21061,\n",
       " 'roles': 64981,\n",
       " 'certainly': 12302,\n",
       " 'noticed': 53132,\n",
       " 'award': 5117,\n",
       " 'consideration': 15685,\n",
       " 'patrick': 56410,\n",
       " 'stettner': 73022,\n",
       " 'evokes': 25584,\n",
       " 'hitchcock': 35526,\n",
       " 'makes': 46416,\n",
       " 'getting': 31012,\n",
       " 'sandwich': 66251,\n",
       " 'vending': 82254,\n",
       " 'machine': 46013,\n",
       " 'suspenseful': 74868,\n",
       " 'finally': 27894,\n",
       " 'writers': 85819,\n",
       " 'armistead': 3951,\n",
       " 'maupin': 47621,\n",
       " 'terry': 76519,\n",
       " 'anderson': 2788,\n",
       " 'deserve': 19663,\n",
       " 'gratitude': 32392,\n",
       " 'attendants': 4693,\n",
       " 'everywhere': 25539,\n",
       " 'john': 40618,\n",
       " 'cullum': 17580,\n",
       " 'lisa': 44694,\n",
       " 'emery': 24033,\n",
       " 'becky': 6587,\n",
       " 'baker': 5609,\n",
       " 'dir': 20546,\n",
       " 'hitchcockian': 35528,\n",
       " 'suspenser': 74874,\n",
       " 'gives': 31362,\n",
       " 'stand': 72580,\n",
       " 'low': 45508,\n",
       " 'key': 41690,\n",
       " 'celebrities': 12172,\n",
       " 'fans': 26731,\n",
       " 'near': 52032,\n",
       " 'paranoia': 55977,\n",
       " 'associates': 4435,\n",
       " 'why': 84699,\n",
       " 'almost': 2189,\n",
       " 'norm': 53009,\n",
       " 'latest': 43352,\n",
       " 'derange': 19552,\n",
       " 'fan': 26677,\n",
       " 'scenario': 66823,\n",
       " 'no': 52802,\n",
       " 'radio': 61513,\n",
       " 'personality': 57191,\n",
       " 'named': 51706,\n",
       " 'reads': 62194,\n",
       " 'stories': 73317,\n",
       " 'penned': 56813,\n",
       " 'airwaves': 1726,\n",
       " 'accumulated': 556,\n",
       " 'interesting': 38921,\n",
       " 'form': 29012,\n",
       " 'submitted': 74015,\n",
       " 'manuscript': 46869,\n",
       " 'travails': 78905,\n",
       " 'troubled': 79306,\n",
       " 'youth': 86562,\n",
       " 'editor': 23326,\n",
       " 'read': 62171,\n",
       " 'naturally': 51917,\n",
       " 'disturbed': 21265,\n",
       " 'ultimately': 80048,\n",
       " 'intrigued': 39146,\n",
       " 'nightmarish': 52635,\n",
       " 'existence': 25891,\n",
       " 'abducted': 182,\n",
       " 'sexually': 68381,\n",
       " 'abused': 396,\n",
       " 'rescued': 63693,\n",
       " 'nurse': 53411,\n",
       " 'excellent': 25704,\n",
       " 'adopted': 1071,\n",
       " 'correspondence': 16367,\n",
       " 'reveals': 64072,\n",
       " 'dying': 22996,\n",
       " 'aids': 1630,\n",
       " 'meet': 48193,\n",
       " 'suddenly': 74216,\n",
       " 'doubt': 21986,\n",
       " 'devious': 19982,\n",
       " 'ulterior': 80046,\n",
       " 'motives': 50579,\n",
       " 'seed': 67700,\n",
       " 'planted': 58178,\n",
       " 'estranged': 25257,\n",
       " 'lover': 45470,\n",
       " 'whose': 84687,\n",
       " 'sudden': 74215,\n",
       " 'departure': 19459,\n",
       " 'city': 13712,\n",
       " 'apartment': 3390,\n",
       " 'emotional': 24092,\n",
       " 'tailspin': 75492,\n",
       " 'grown': 32841,\n",
       " 'tempest': 76272,\n",
       " 'teacup': 76004,\n",
       " 'decides': 18738,\n",
       " 'investigating': 39248,\n",
       " 'backgrounds': 5388,\n",
       " 'discovering': 20806,\n",
       " 'truths': 79431,\n",
       " 'didn': 20233,\n",
       " 'anticipate': 3200,\n",
       " 'co': 14252,\n",
       " 'wrote': 85871,\n",
       " 'screenplay': 67319,\n",
       " 'former': 29035,\n",
       " 'novice': 53219,\n",
       " 'hoax': 35630,\n",
       " 'run': 65599,\n",
       " 'full': 29845,\n",
       " 'tilt': 77608,\n",
       " 'any': 3294,\n",
       " 'old': 54090,\n",
       " 'fashioned': 26929,\n",
       " 'pot': 59149,\n",
       " 'boiler': 8562,\n",
       " 'helps': 34758,\n",
       " 'conflicted': 15468,\n",
       " 'hearted': 34473,\n",
       " 'genuinely': 30869,\n",
       " 'number': 53362,\n",
       " 'fact': 26418,\n",
       " 'him': 35332,\n",
       " 'thing': 77008,\n",
       " 'escaped': 25094,\n",
       " 'own': 55440,\n",
       " 'unsettling': 81295,\n",
       " 'dreadful': 22285,\n",
       " 'trait': 78683,\n",
       " 'leave': 43698,\n",
       " 'unmentioned': 81035,\n",
       " 'underlines': 80458,\n",
       " 'desperation': 19741,\n",
       " 'rattle': 62010,\n",
       " 'runs': 65618,\n",
       " 'gas': 30515,\n",
       " 'eventually': 25466,\n",
       " 'repetitive': 63524,\n",
       " 'predictable': 59457,\n",
       " 'despite': 19755,\n",
       " 'finely': 27926,\n",
       " 'directed': 20549,\n",
       " 'piece': 57740,\n",
       " 'hoodwink': 36012,\n",
       " 'pays': 56546,\n",
       " 'listen': 44709,\n",
       " 'inner': 38507,\n",
       " 'voice': 83029,\n",
       " 'careful': 11456,\n",
       " 'hope': 36069,\n",
       " 'god': 31648,\n",
       " 'bless': 8078,\n",
       " 'constantly': 15741,\n",
       " 'shooting': 69096,\n",
       " 'foot': 28803,\n",
       " 'lately': 43330,\n",
       " 'dumb': 22725,\n",
       " 'done': 21731,\n",
       " 'decade': 18672,\n",
       " 'perhaps': 57038,\n",
       " 'exception': 25718,\n",
       " 'death': 18575,\n",
       " 'smoochy': 70667,\n",
       " 'bombed': 8654,\n",
       " 'came': 11029,\n",
       " 'cult': 17593,\n",
       " 'dramas': 22214,\n",
       " 'especially': 25159,\n",
       " 'insomnia': 38649,\n",
       " 'hour': 36337,\n",
       " 'photo': 57572,\n",
       " 'mediocre': 48148,\n",
       " 'reviews': 64137,\n",
       " 'quick': 61287,\n",
       " 'dvd': 22928,\n",
       " 'release': 63167,\n",
       " 'among': 2589,\n",
       " 'period': 57050,\n",
       " 'chilling': 13146,\n",
       " 'include': 37874,\n",
       " 'serial': 68161,\n",
       " 'killer': 41885,\n",
       " 'anyone': 3315,\n",
       " 'physically': 57628,\n",
       " 'dangerous': 18132,\n",
       " 'concept': 15267,\n",
       " 'actual': 849,\n",
       " 'case': 11735,\n",
       " 'fraud': 29381,\n",
       " 'yet': 86357,\n",
       " 'officially': 53903,\n",
       " 'confirmed': 15456,\n",
       " 'autobiography': 4952,\n",
       " 'child': 13096,\n",
       " 'anthony': 3178,\n",
       " 'godby': 31657,\n",
       " 'johnson': 40633,\n",
       " 'suffered': 74242,\n",
       " 'horrific': 36168,\n",
       " 'abuse': 394,\n",
       " 'contracted': 15934,\n",
       " 'result': 63915,\n",
       " 'moved': 50686,\n",
       " 'reports': 63578,\n",
       " 'online': 54311,\n",
       " 'may': 47670,\n",
       " 'exist': 25883,\n",
       " 'confused': 15501,\n",
       " 'feelings': 27230,\n",
       " 'brilliantly': 9661,\n",
       " 'portrayed': 59020,\n",
       " 'resurfaced': 63930,\n",
       " 'mind': 49177,\n",
       " 'sociopathic': 70993,\n",
       " 'caretaker': 11474,\n",
       " 'role': 64969,\n",
       " 'cry': 17457,\n",
       " 'little': 44780,\n",
       " 'miss': 49561,\n",
       " 'sunshine': 74481,\n",
       " 'times': 77686,\n",
       " 'looked': 45175,\n",
       " 'camera': 11039,\n",
       " 'thought': 77250,\n",
       " 'staring': 72668,\n",
       " 'takes': 75543,\n",
       " 'play': 58237,\n",
       " 'sort': 71432,\n",
       " 'understated': 80513,\n",
       " 'reviewed': 64130,\n",
       " 'actresses': 837,\n",
       " 'generation': 30764,\n",
       " 'nominated': 52888,\n",
       " 'academy': 428,\n",
       " 'incredible': 37968,\n",
       " 'least': 43674,\n",
       " 'scary': 66784,\n",
       " 'dark': 18225,\n",
       " 'recommend': 62546,\n",
       " 'prepared': 59594,\n",
       " 'unsettled': 81292,\n",
       " 'because': 6560,\n",
       " 'leaves': 43704,\n",
       " 'strange': 73467,\n",
       " 'feeling': 27227,\n",
       " 'first': 28060,\n",
       " 'maupins': 47622,\n",
       " 'taken': 75530,\n",
       " 'displayed': 21070,\n",
       " 'cares': 11468,\n",
       " 'loves': 45480,\n",
       " 'said': 65957,\n",
       " 'we': 83914,\n",
       " 'version': 82401,\n",
       " 'expected': 25964,\n",
       " 'past': 56302,\n",
       " 'gloss': 31549,\n",
       " 'hollywood': 35790,\n",
       " 'succeeded': 74136,\n",
       " 'amount': 2604,\n",
       " 'restraint': 63896,\n",
       " 'captures': 11382,\n",
       " 'fragile': 29241,\n",
       " 'essence': 25199,\n",
       " 'lets': 44077,\n",
       " 'us': 81693,\n",
       " 'struggle': 73715,\n",
       " 'issues': 39619,\n",
       " 'trust': 79410,\n",
       " 'personnel': 57212,\n",
       " 'lifejess': 44320,\n",
       " 'around': 4008,\n",
       " 'himdonna': 35340,\n",
       " 'introduced': 39159,\n",
       " 'players': 58253,\n",
       " 'reminded': 63324,\n",
       " 'nothing': 53115,\n",
       " 'seems': 67739,\n",
       " 'smallest': 70549,\n",
       " 'event': 25449,\n",
       " 'change': 12491,\n",
       " 'our': 54924,\n",
       " 'lives': 44816,\n",
       " 'irrevocably': 39449,\n",
       " 'request': 63664,\n",
       " 'review': 64127,\n",
       " 'book': 8778,\n",
       " 'turns': 79662,\n",
       " 'changing': 12507,\n",
       " 'find': 27910,\n",
       " 'strength': 73570,\n",
       " 'within': 85167,\n",
       " 'carry': 11656,\n",
       " 'forward': 29124,\n",
       " 'bad': 5454,\n",
       " 'avoid': 5075,\n",
       " 'average': 5035,\n",
       " 'american': 2512,\n",
       " 'serious': 68186,\n",
       " 'please': 58315,\n",
       " 'chance': 12456,\n",
       " 'touches': 78441,\n",
       " 'darkness': 18246,\n",
       " 'go': 31613,\n",
       " 'ourselves': 54928,\n",
       " 'stepped': 72974,\n",
       " 'another': 3113,\n",
       " 'quality': 61144,\n",
       " 'forget': 28972,\n",
       " 'steals': 72855,\n",
       " 'leading': 43617,\n",
       " 'looks': 45194,\n",
       " 'screen': 67300,\n",
       " 'presence': 59640,\n",
       " 'hacks': 33375,\n",
       " 'opinion': 54463,\n",
       " 'liked': 44424,\n",
       " 'action': 741,\n",
       " 'tense': 76369,\n",
       " 'opening': 54422,\n",
       " 'semi': 67908,\n",
       " 'truck': 79345,\n",
       " 'transitional': 78780,\n",
       " 'filmed': 27725,\n",
       " 'ways': 83885,\n",
       " 'lapse': 43197,\n",
       " 'photography': 57589,\n",
       " 'unusual': 81438,\n",
       " 'colors': 14657,\n",
       " 'evil': 25559,\n",
       " 'illnesses': 37291,\n",
       " 'born': 8972,\n",
       " 'modern': 49804,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VOCABULARIO DEL VECTORIZADOR\n",
    "# =============================================================================\n",
    "# Mostramos el tamaño de la matriz: (25000 reviews, 87063 palabras únicas)\n",
    "print(X_baseline.shape)\n",
    "\n",
    "# El atributo vocabulary_ es un diccionario donde:\n",
    "# - Clave: la palabra\n",
    "# - Valor: el índice/columna que le corresponde en la matriz\n",
    "# Ejemplo: {'good': 35421, 'bad': 8792, ...}\n",
    "# Esto significa que la columna 35421 representa la palabra 'good'\n",
    "\n",
    "baseline_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2GXBlhGHevLh"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VECTORIZACIÓN CON CONTEO (no binaria)\n",
    "# =============================================================================\n",
    "# Ahora probamos SIN binary=True para ver la diferencia\n",
    "# En este caso, la matriz contendrá el NÚMERO DE VECES que aparece cada palabra\n",
    "# (no solo 0 o 1, sino 0, 1, 2, 3, 4, ...)\n",
    "\n",
    "vectorizer_c = CountVectorizer()  # Sin binary=True\n",
    "vectorizer_c.fit(reviews_train_clean)\n",
    "\n",
    "# Esta matriz contendrá conteos reales\n",
    "X_baseline_c = vectorizer_c.transform(reviews_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XzA1CNzAevLh",
    "outputId": "d0eb0ee8-23ab-462e-fc25-b7c37710c6f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "87063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(25000, 87063))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARACIÓN: Conteo vs Binario\n",
    "# =============================================================================\n",
    "# Dimensiones: (25000, 87063) - ¡igual que antes!\n",
    "# El número de palabras únicas (columnas) es el mismo\n",
    "# Lo que cambia son los VALORES dentro de la matriz\n",
    "\n",
    "print(X_baseline_c.shape)\n",
    "print(len(vectorizer_c.get_feature_names_out()))  # Las mismas 87,063 palabras\n",
    "\n",
    "# Si descomentamos la siguiente línea, veríamos la matriz completa\n",
    "# pero es ENORME (25000 x 87063 = 2,176,575,000 celdas!)\n",
    "X_baseline_c.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TNlncpIevLh",
    "outputId": "f0de4e2f-041a-411a-8994-6ba2b4240553"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 3410713 stored elements and shape (25000, 87063)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# REPRESENTACIÓN DE LA MATRIZ SPARSE\n",
    "# =============================================================================\n",
    "# Matriz demasiado grande como para que numpy la imprima completa por pantalla\n",
    "# Python nos muestra el formato comprimido (Compressed Sparse Row)\n",
    "# 3,410,713 elementos almacenados de 2+ mil millones de posiciones posibles\n",
    "\n",
    "X_baseline_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xykEdi7AevLi"
   },
   "source": [
    "# Entrenar un Modelo Base\n",
    "\n",
    "Entrenar un modelo de Regresión Logística después de transformar los datos con CountVectorizer\n",
    "\n",
    "* Son fáciles de interpretar\n",
    "* Los modelos lineales tienden a funcionar bien en conjuntos de datos dispersos como este\n",
    "* Aprenden muy rápido en comparación con otros algoritmos.\n",
    "\n",
    "Probar modelos con valores de C de [0.01, 0.05, 0.25, 0.5, 1] y ver cuál es el mejor valor para C, y calcular la precisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DHIRZAegevLi"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODELO BASELINE - Regresión Logística con Grid Search\n",
    "# =============================================================================\n",
    "# Vamos a entrenar nuestro primer modelo de clasificación para predecir el sentimiento\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# PASO 1: Crear las etiquetas (targets)\n",
    "# Las reviews están ordenadas: primeras 12,500 son positivas (1), últimas 12,500 negativas (0)\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "# Equivalente más explícito (comentado):\n",
    "# target = []\n",
    "# for i in range(25000):\n",
    "#     if i < 12500:\n",
    "#         target.append(1)  # Positivo\n",
    "#     else:\n",
    "#         target.append(0)  # Negativo\n",
    "\n",
    "# PASO 2: Función para entrenar el modelo con validación cruzada\n",
    "def train_model(X_TRAIN, X_TEST):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de Regresión Logística con Grid Search\n",
    "    \n",
    "    Grid Search:\n",
    "    - Prueba diferentes valores del hiperparámetro C\n",
    "    - C controla la regularización (penalización por complejidad)\n",
    "    - C pequeño = más regularización = modelo más simple\n",
    "    - C grande = menos regularización = modelo más complejo\n",
    "    \n",
    "    Cross-Validation (cv=5):\n",
    "    - Divide los datos de entrenamiento en 5 partes\n",
    "    - Entrena 5 veces, cada vez usando 4 partes para entrenar y 1 para validar\n",
    "    - Esto nos da una estimación más robusta del rendimiento\n",
    "    \n",
    "    Args:\n",
    "        X_TRAIN: matriz de features de entrenamiento\n",
    "        X_TEST: matriz de features de test\n",
    "    \"\"\"\n",
    "    \n",
    "    lr = LogisticRegression()  # Creamos el modelo\n",
    "    \n",
    "    # Hiperparámetros a probar\n",
    "    params = {\n",
    "        'C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    # GridSearchCV prueba todas las combinaciones y elige la mejor\n",
    "    grid = GridSearchCV(lr, params, cv=5)\n",
    "    grid.fit(X_TRAIN, target)\n",
    "    \n",
    "    # Evaluamos el mejor modelo encontrado en el conjunto de test\n",
    "    print(\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rIzphl1VevLi",
    "outputId": "e1739366-946d-4d8a-c619-0ef05de257af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88184\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENTRENAR Y EVALUAR EL MODELO BASELINE\n",
    "# =============================================================================\n",
    "# Entrenamos el modelo con las matrices vectorizadas (binarias)\n",
    "# Este es nuestro BASELINE - el modelo más simple contra el que compararemos mejoras\n",
    "\n",
    "# X_baseline: reviews de train vectorizadas\n",
    "# X_test_baseline: reviews de test vectorizadas\n",
    "\n",
    "train_model(X_baseline, X_test_baseline)\n",
    "\n",
    "# Resultado: ~88.18% de accuracy\n",
    "# Esto significa que el modelo clasifica correctamente el 88.18% de las reviews\n",
    "# ¡No está mal para un primer intento!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1ekCzxqevLi"
   },
   "source": [
    "# Eliminar Stop Words\n",
    "\n",
    "Las stop words son palabras muy comunes como 'if', 'but', 'we', 'he', 'she' y 'they'. Normalmente podemos eliminar estas palabras sin cambiar la semántica de un texto y hacerlo a menudo (pero no siempre) mejora el rendimiento de un modelo. Eliminar estas stop words se vuelve mucho más útil cuando comenzamos a usar secuencias de palabras más largas como características del modelo (ver n-gramas más adelante).\n",
    "\n",
    "Antes de aplicar el CountVectorizer, eliminemos las stopwords incluidas en nltk.corpus\n",
    "\n",
    "Luego aplicar el CountVectorizer, entrenar el modelo de Regresión Logística y obtener la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "TL4DICY8evLi"
   },
   "outputs": [],
   "source": [
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ypZ7jZL5evLi",
    "outputId": "14312258-8198-46e7-adfd-fc44cf066c3b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.6'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFICAR VERSIÓN DE NUMPY\n",
    "# =============================================================================\n",
    "# Comprobamos la versión de numpy instalada\n",
    "# Nota: versiones antiguas pueden tener compatibilidad diferente con otras librerías\n",
    "\n",
    "np.__version__\n",
    "# Versión antigua era 1.26.4, ahora está actualizada a 2.2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVNGDMgmevLj",
    "outputId": "c4db9c34-ab9b-4da0-b9e3-fd589bb5dbf3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\borja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DESCARGAR STOPWORDS DE NLTK\n",
    "# =============================================================================\n",
    "# NLTK (Natural Language Toolkit) es una librería muy popular para NLP\n",
    "# Necesitamos descargar el dataset de stopwords la primera vez que lo usamos\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')  # Descarga las stopwords en múltiples idiomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eZY5oiUevLj",
    "outputId": "2dbf6a67-5c1d-4267-efcb-95b9a53cb8c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZAR STOPWORDS EN INGLÉS\n",
    "# =============================================================================\n",
    "# ¿Qué son las stopwords?\n",
    "# Son palabras muy comunes que aparecen en casi todos los textos\n",
    "# Ejemplos: 'the', 'a', 'is', 'in', 'of', 'and', etc.\n",
    "# \n",
    "# ¿Por qué eliminarlas?\n",
    "# - No aportan mucho significado al sentimiento\n",
    "# - Reducen el ruido en el modelo\n",
    "# - Disminuyen el tamaño del vocabulario (menos features)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Mostramos las primeras 20 stopwords en inglés\n",
    "stopwords.words('english')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qi-otvE8evLk",
    "outputId": "3d2ece59-afd9-476e-e8e9-91f1a41ba6da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONTAR STOPWORDS EN INGLÉS\n",
    "# =============================================================================\n",
    "# NLTK tiene 198 stopwords predefinidas para inglés\n",
    "\n",
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C35F9bnZevLk",
    "outputId": "ae0255a4-f8d0-4878-ac1c-8600a4cf773c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de',\n",
       " 'la',\n",
       " 'que',\n",
       " 'el',\n",
       " 'en',\n",
       " 'y',\n",
       " 'a',\n",
       " 'los',\n",
       " 'del',\n",
       " 'se',\n",
       " 'las',\n",
       " 'por',\n",
       " 'un',\n",
       " 'para',\n",
       " 'con',\n",
       " 'no',\n",
       " 'una',\n",
       " 'su',\n",
       " 'al',\n",
       " 'lo']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STOPWORDS EN OTROS IDIOMAS - Ejemplo en español\n",
    "# =============================================================================\n",
    "# NLTK incluye stopwords para múltiples idiomas\n",
    "# Aquí vemos las primeras 20 en español\n",
    "\n",
    "stopwords.words('spanish')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oNCdFEbpevLl",
    "outputId": "44c163a0-2323-4bcd-aa84-44016a530835"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CANTIDAD DE STOPWORDS EN ESPAÑOL\n",
    "# =============================================================================\n",
    "# El español tiene 313 stopwords en NLTK (más que inglés)\n",
    "# Esto depende de la complejidad morfológica de cada idioma\n",
    "\n",
    "len(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k2r-9RpaevLl",
    "outputId": "b4a3ba5e-a249-4f3d-ad1e-4862f94c2b5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STOPWORDS EN CHINO\n",
    "# =============================================================================\n",
    "# Ejemplo de stopwords en chino\n",
    "# Los idiomas asiáticos tienen sistemas de escritura muy diferentes\n",
    "\n",
    "stopwords.words('chinese')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TV9MALqXevLl",
    "outputId": "3d72da20-3f5c-4915-de68-b4e4aba1c9d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "841"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CANTIDAD DE STOPWORDS EN CHINO\n",
    "# =============================================================================\n",
    "# El chino tiene 841 stopwords - ¡mucho más que inglés o español!\n",
    "\n",
    "len(stopwords.words('chinese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQdlB-iqevLl",
    "outputId": "b6efe901-2c2a-4d7f-c8d5-1b631f3f8325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ahala',\n",
       " 'aitzitik',\n",
       " 'al',\n",
       " 'ala ',\n",
       " 'alabadere',\n",
       " 'alabaina',\n",
       " 'alabaina',\n",
       " 'aldiz ',\n",
       " 'alta',\n",
       " 'amaitu']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STOPWORDS EN EUSKERA (VASCO)\n",
    "# =============================================================================\n",
    "# NLTK incluye hasta idiomas menos comunes como el euskera\n",
    "# Primeras 10 stopwords en vasco\n",
    "\n",
    "stopwords.words('basque')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKy0QojJevLm",
    "outputId": "3bca5fab-406e-4014-f7d0-db887d851029"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CANTIDAD DE STOPWORDS EN EUSKERA\n",
    "# =============================================================================\n",
    "# El euskera tiene 326 stopwords en NLTK\n",
    "\n",
    "len(stopwords.words('basque'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0xdbuIRHevLw"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÉTODO 1: ELIMINAR STOPWORDS MANUALMENTE (antes de vectorizar)\n",
    "# =============================================================================\n",
    "# Este método elimina stopwords aplicando un filtro sobre las reviews\n",
    "# Es el enfoque \"manual\" antes de usar CountVectorizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargamos la lista de stopwords en inglés\n",
    "english_stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(corpus):\n",
    "    \"\"\"\n",
    "    Elimina stopwords de cada review en el corpus\n",
    "    \n",
    "    Proceso:\n",
    "    1. Para cada review, la dividimos en palabras (split())\n",
    "    2. Filtramos las palabras que NO están en la lista de stopwords\n",
    "    3. Unimos las palabras que quedan de nuevo en un string\n",
    "    \n",
    "    Args:\n",
    "        corpus: lista de reviews (strings)\n",
    "    \n",
    "    Returns:\n",
    "        removed_stop_words: lista de reviews sin stopwords\n",
    "    \"\"\"\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        # List comprehension que:\n",
    "        # - Divide la review en palabras (review.split())\n",
    "        # - Convierte cada palabra a minúsculas\n",
    "        # - Solo incluye palabras que NO están en english_stop_words\n",
    "        # - Une todo con espacios (' '.join())\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word.lower() for word in review.split() \n",
    "                      if word.lower() not in english_stop_words])\n",
    "        )\n",
    "    \n",
    "    return removed_stop_words\n",
    "\n",
    "# Aplicamos la eliminación de stopwords ANTES de vectorizar\n",
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)\n",
    "\n",
    "# NOTA: Este método es más \"manual\" y menos eficiente que usar el parámetro\n",
    "# stop_words del CountVectorizer (ver más abajo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "FmryB0ZKevLw"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VECTORIZAR DESPUÉS DE ELIMINAR STOPWORDS (método manual)\n",
    "# =============================================================================\n",
    "# Ahora vectorizamos las reviews que ya tienen las stopwords eliminadas\n",
    "# \n",
    "# Documentación de CountVectorizer:\n",
    "# - lowercase=True: convierte todo a minúsculas antes de vectorizar (por defecto)\n",
    "# - stop_words: permite pasar una lista de stopwords (¡mejor método, ver abajo!)\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(no_stop_words_train)\n",
    "\n",
    "X = cv.transform(no_stop_words_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mv6LdF7VevLw",
    "outputId": "0808841a-0fcb-4056-9bf0-3f958184f929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87046)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPROBAR DIMENSIONES\n",
    "# =============================================================================\n",
    "# Verificamos el tamaño de la matriz después de eliminar stopwords\n",
    "# Esperamos menos columnas (palabras) que antes\n",
    "\n",
    "print(X.shape)  # Debería ser (25000, algo menos que 87063)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-r-4k07evLw",
    "outputId": "e7569d70-4b97-4807-bc3b-35548d019781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.879\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENTRENAR MODELO CON STOPWORDS ELIMINADAS (método manual)\n",
    "# =============================================================================\n",
    "# Aplicamos la misma transformación a test y entrenamos el modelo\n",
    "\n",
    "X_test = cv.transform(no_stop_words_test)\n",
    "\n",
    "# Evaluamos: ¿mejora o empeora el accuracy?\n",
    "train_model(X, X_test)\n",
    "\n",
    "# Resultado: ~87.9% - ¡ligeramente peor que el baseline!\n",
    "# Esto puede pasar: no siempre eliminar stopwords mejora el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf-LODZ5evLw",
    "outputId": "cca91271-145f-4d2b-c303-9c97b451ae34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 87046)\n",
      "Stop words eliminadas: 17\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARACIÓN: ¿Cuántas palabras eliminamos?\n",
    "# =============================================================================\n",
    "# Comparamos el número de features (columnas) antes y después\n",
    "\n",
    "print(X_baseline.shape)  # Baseline: 87,063 palabras\n",
    "print(X.shape)           # Con stopwords eliminadas: 87,046 palabras\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "# ¡Solo eliminó 17 palabras! ¿Por qué tan pocas?\n",
    "# Porque nuestra función remove_stop_words() hace un split() simple\n",
    "# que no tokeniza tan bien como CountVectorizer\n",
    "# Por ejemplo: \"it's\" no se separa correctamente en \"it\" y \"'s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7L0mbQwMevLx"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MÉTODO 2: ELIMINAR STOPWORDS CON CountVectorizer (MEJOR MÉTODO)\n",
    "# =============================================================================\n",
    "# Este es el método RECOMENDADO: pasar stop_words directamente al vectorizador\n",
    "# \n",
    "# Ventajas:\n",
    "# - Más simple (menos código)\n",
    "# - Más eficiente\n",
    "# - Mejor tokenización (CountVectorizer es más inteligente)\n",
    "# - Elimina más stopwords correctamente\n",
    "\n",
    "cv = CountVectorizer(binary=True,\n",
    "                     stop_words=english_stop_words)  # ¡Solo añadimos este parámetro!\n",
    "\n",
    "# Aplicamos sobre las reviews originales (limpias, pero sin eliminar stopwords manualmente)\n",
    "cv.fit(reviews_train_clean)\n",
    "\n",
    "X = cv.transform(reviews_train_clean)\n",
    "X_test = cv.transform(reviews_test_clean)\n",
    "\n",
    "# train_model(X, X_test)  # Comentado para no ejecutar ahora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1jSkT8XevLx",
    "outputId": "14e2a606-3965-4870-832c-c88e701080e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 86918)\n",
      "Stop words eliminadas: 145\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARACIÓN: Método manual vs CountVectorizer\n",
    "# =============================================================================\n",
    "# Comparamos cuántas stopwords eliminó cada método\n",
    "\n",
    "print(X_baseline.shape)  # Baseline: 87,063 palabras\n",
    "print(X.shape)           # Con CountVectorizer stop_words: 86,918 palabras\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "# ¡Ahora sí! Eliminó 145 palabras\n",
    "# Mucho mejor que las 17 del método manual\n",
    "# \n",
    "# ¿Por qué?\n",
    "# CountVectorizer tokeniza mejor (separa \"it's\" en \"it\" y \"'s\")\n",
    "# Luego elimina todas las stopwords correctamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgRiJVWDevLx"
   },
   "source": [
    "**Nota:** En la práctica, una forma más fácil de eliminar stop words es simplemente usar el argumento stop_words con cualquiera de las clases 'Vectorizer' de scikit-learn. Si quieres usar la lista completa de stop words de NLTK puedes hacer stop_words='english'. En la práctica he encontrado que usar la lista de NLTK en realidad disminuye mi rendimiento porque es demasiado expansiva, así que normalmente proporciono mi propia lista de palabras. Por ejemplo, stop_words=['in','of','at','a','the']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L54gvI6evLx"
   },
   "source": [
    "Un siguiente paso común en el preprocesamiento de texto es normalizar las palabras en tu corpus intentando convertir todas las diferentes formas de una palabra dada en una. Dos métodos que existen para esto son Stemming y Lemmatization.\n",
    "\n",
    "# Stemming\n",
    "\n",
    "Stemming se considera el enfoque más crudo/de fuerza bruta para la normalización (aunque esto no necesariamente significa que tendrá peor rendimiento). Hay varios algoritmos, pero en general todos usan reglas básicas para cortar los finales de las palabras.\n",
    "\n",
    "NLTK tiene varias implementaciones de algoritmos de stemming. Usaremos el Porter stemmer. Los más usados:\n",
    "* PorterStemmer\n",
    "* SnowballStemmer\n",
    "\n",
    "Aplicar un PorterStemmer, vectorizar y entrenar el modelo nuevamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSG7ZITVevLx",
    "outputId": "e23a555b-dbac-4a4c-b82f-c51d20813046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli fli flight flown die die mule deni deni die agre agre own humbl size meet state siez item sensat tradit refer colon colon plot\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEMMING - Reducción de palabras a su raíz (método \"bruto\")\n",
    "# =============================================================================\n",
    "# ¿Qué es stemming?\n",
    "# Es un proceso que recorta las palabras eliminando sufijos para obtener su \"raíz\"\n",
    "# Es un método rápido pero \"bruto\" (no siempre linguísticamente correcto)\n",
    "#\n",
    "# Ejemplos:\n",
    "# - \"running\", \"runs\", \"ran\" → \"run\"\n",
    "# - \"cats\", \"catty\" → \"cat\"\n",
    "# - \"flies\", \"flying\" → \"fli\"\n",
    "#\n",
    "# Algoritmos de stemming:\n",
    "# - PorterStemmer: el más común, desarrollado por Martin Porter en 1980\n",
    "# - SnowballStemmer: una mejora del PorterStemmer\n",
    "# - LancasterStemmer: más agresivo (recorta más)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Lista de palabras de ejemplo en diferentes formas\n",
    "plurals = ['caresses', 'flies', 'fly','flight', 'flown', 'dies', 'die', 'mules', 'denied', 'deny',\n",
    "            'died', 'agreed','agree', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer', 'colonizing',\n",
    "            'plotted']\n",
    "\n",
    "# Aplicamos stemming a cada palabra\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "# Observa cómo se recortan:\n",
    "# - 'flies' y 'fly' → 'fli' (pierde sentido)\n",
    "# - 'agreed' y 'agree' → 'agre'\n",
    "# - 'sensational' → 'sensat'\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYt6sEfqevLx",
    "outputId": "91accf71-55e6-4b07-fb72-df332115cdb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli fli flight die mule deni deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SNOWBALL STEMMER - Versión mejorada del Porter Stemmer\n",
    "# =============================================================================\n",
    "# SnowballStemmer es una evolución del PorterStemmer\n",
    "# Ventaja: soporta múltiples idiomas (15 idiomas diferentes)\n",
    "# Sintaxis: SnowballStemmer('idioma')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')  # Especificamos el idioma\n",
    "\n",
    "plurals = ['caresses', 'flies', 'fly','flight', 'dies', 'mules', 'denied', 'deny',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "# Los resultados son muy similares al PorterStemmer\n",
    "# Pero Snowball suele ser ligeramente más preciso\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VibI7GZMevLy",
    "outputId": "5b25a4eb-be01-4a2f-f28d-8e924051dc4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recorr corr correl corr cas caser cas play vol vol volv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SNOWBALL STEMMER EN ESPAÑOL\n",
    "# =============================================================================\n",
    "# Ejemplo de stemming en español\n",
    "# Es importante porque cada idioma tiene reglas morfológicas diferentes\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')  # Cambiamos el idioma\n",
    "\n",
    "plurals = ['recorrer', 'corriendo', 'correlación', 'correré', \n",
    "           'casas', 'casero', 'caso', 'playa', \n",
    "           'volando', 'volar', 'volveré']\n",
    "\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "# Observa los resultados:\n",
    "# - 'recorrer', 'corriendo', 'correré' → 'recorr', 'corr', 'corr'\n",
    "# - 'casas', 'casero', 'caso' → 'cas', 'caser', 'cas'\n",
    "# - 'volando', 'volar', 'volveré' → 'vol', 'vol', 'volv'\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OhCpJ9O3evLy"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APLICAR STEMMING A NUESTRAS REVIEWS\n",
    "# =============================================================================\n",
    "# Ahora aplicamos stemming a todas las reviews de películas\n",
    "# Objetivo: reducir el vocabulario agrupando palabras similares\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def get_stemmed_text(corpus):\n",
    "    \"\"\"\n",
    "    Aplica stemming a cada palabra de cada review\n",
    "    \n",
    "    Proceso:\n",
    "    1. Para cada review, dividirla en palabras\n",
    "    2. Aplicar stemming a cada palabra\n",
    "    3. Unir las palabras procesadas de nuevo en un string\n",
    "    \n",
    "    Args:\n",
    "        corpus: lista de reviews\n",
    "    \n",
    "    Returns:\n",
    "        lista de reviews con palabras \"stemmed\"\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Para cada review:\n",
    "    # - Dividir en palabras (.split())\n",
    "    # - Aplicar stemming a cada palabra (stemmer.stem())\n",
    "    # - Unir con espacios (' '.join())\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) \n",
    "            for review in corpus]\n",
    "\n",
    "# Aplicamos stemming a train y test\n",
    "stemmed_reviews_train = get_stemmed_text(reviews_train_clean)\n",
    "stemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n",
    "\n",
    "# Ahora vectorizamos las reviews \"stemmed\"\n",
    "cv = CountVectorizer(binary=True, stop_words=english_stop_words)\n",
    "cv.fit(stemmed_reviews_train)\n",
    "\n",
    "X_stem = cv.transform(stemmed_reviews_train)\n",
    "X_test = cv.transform(stemmed_reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVc3cfv1evLy",
    "outputId": "01b20c5e-d6f5-482a-8892-9ce6585c9493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8768\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EVALUAR MODELO CON STEMMING\n",
    "# =============================================================================\n",
    "# Entrenamos el modelo con las reviews procesadas con stemming\n",
    "# ¿Mejora el accuracy?\n",
    "\n",
    "train_model(X_stem, X_test)\n",
    "\n",
    "# Resultado: ~87.68%\n",
    "# Similar al modelo anterior (sin stemming con stopwords eliminadas)\n",
    "# Stemming no siempre mejora, pero reduce significativamente el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyT5mSLhevLy",
    "outputId": "cc58a9c4-a3fd-4509-99d3-c76e4041021d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 66715)\n",
      "Diff X normal y X tras stemmer y vectorización: 20348\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPACTO DEL STEMMING EN EL VOCABULARIO\n",
    "# =============================================================================\n",
    "# Comparamos el tamaño del vocabulario antes y después del stemming\n",
    "\n",
    "print(X_baseline.shape)      # Sin stemming: 87,063 palabras\n",
    "print(X_stem.shape)          # Con stemming: 66,715 palabras\n",
    "print(\"Diff X normal y X tras stemmer y vectorización:\", \n",
    "      X_baseline.shape[1] - X_stem.shape[1])\n",
    "\n",
    "# ¡Reducción de 20,348 palabras!\n",
    "# Esto es una reducción del ~23% del vocabulario\n",
    "# \n",
    "# ¿Por qué?\n",
    "# Porque stemming agrupa formas de la misma palabra:\n",
    "# - \"amazing\", \"amazed\", \"amazement\" → \"amaz\"\n",
    "# - \"loving\", \"loved\", \"loves\" → \"love\"\n",
    "# \n",
    "# Ventajas:\n",
    "# - Menos features = modelo más rápido de entrenar\n",
    "# - Agrupa conceptos similares\n",
    "# - Reduce overfitting\n",
    "#\n",
    "# Desventajas:\n",
    "# - Pierde matices del lenguaje\n",
    "# - Puede crear \"palabras\" sin sentido (\"fli\" en lugar de \"fly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDXm5YoFevLy"
   },
   "source": [
    "# Lemmatization\n",
    "\n",
    "La lemmatization funciona identificando la parte del discurso de una palabra dada y luego aplicando reglas más complejas para transformar la palabra en su raíz verdadera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "fJT8m90NevLz"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FppH1l9wevLz",
    "outputId": "12e11fb9-102c-4583-eb2f-3454e10cfd2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\borja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DESCARGAR WORDNET - Base de datos léxica para lemmatization\n",
    "# =============================================================================\n",
    "# WordNet es una base de datos léxica del inglés\n",
    "# Contiene información sobre relaciones entre palabras, sinónimos, etc.\n",
    "# Es necesaria para hacer lemmatization correctamente\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJwmAwFRevLz",
    "outputId": "48571f6d-2ce8-448f-af8a-575a6db74ea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fly fly flight dy mule study died agreed owned humbled sized meeting stating siezing itemization sensational traditional reference colonizer plotted\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# LEMMATIZATION - Reducción de palabras a su forma base (método \"inteligente\")\n",
    "# =============================================================================\n",
    "# ¿Qué es lemmatization?\n",
    "# Es un proceso más sofisticado que stemming\n",
    "# Usa un diccionario (WordNet) para encontrar la forma base real de cada palabra\n",
    "#\n",
    "# Diferencias clave con stemming:\n",
    "# - Stemming: recorta sufijos (método bruto) → \"flies\" → \"fli\"\n",
    "# - Lemmatization: busca la raíz real → \"flies\" → \"fly\"\n",
    "#\n",
    "# Ventajas de lemmatization:\n",
    "# - Produce palabras reales (no \"raíces inventadas\")\n",
    "# - Más preciso linguísticamente\n",
    "# - Considera el contexto (parte del discurso: verbo, sustantivo, etc.)\n",
    "#\n",
    "# Desventajas:\n",
    "# - Más lento que stemming (requiere búsqueda en diccionario)\n",
    "# - Necesita WordNet u otro diccionario\n",
    "# - No disponible fácilmente en todos los idiomas\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Mismas palabras de ejemplo que con stemming\n",
    "plurals = ['caresses', 'flies','fly','flight', 'dies', 'mules', 'studies',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "\n",
    "# Aplicamos lemmatization\n",
    "singles = [lemmatizer.lemmatize(plural) for plural in plurals]\n",
    "\n",
    "# Compara con stemming:\n",
    "# - Stemming: 'flies' → 'fli' (sin sentido)\n",
    "# - Lemmatization: 'flies' → 'fly' (palabra real)\n",
    "#\n",
    "# - Stemming: 'agreed' → 'agre'\n",
    "# - Lemmatization: 'agreed' → 'agreed' (no detecta que es verbo sin contexto)\n",
    "#\n",
    "# Nota: por defecto, lemmatize() asume que la palabra es un sustantivo\n",
    "# Para mejor precisión se puede especificar: lemmatize(word, pos='v') para verbos\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvFDXfv2evLz",
    "outputId": "2029d781-eafe-4d9a-8920-009062c37ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87824\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APLICAR LEMMATIZATION A NUESTRAS REVIEWS\n",
    "# =============================================================================\n",
    "# Aplicamos lemmatization a todas las reviews de películas\n",
    "# El proceso es similar a stemming pero con resultados más precisos\n",
    "\n",
    "def get_lemmatized_text(corpus):\n",
    "    \"\"\"\n",
    "    Aplica lemmatization a cada palabra de cada review\n",
    "    \n",
    "    Proceso similar a get_stemmed_text() pero usando WordNetLemmatizer\n",
    "    \n",
    "    Args:\n",
    "        corpus: lista de reviews\n",
    "    \n",
    "    Returns:\n",
    "        lista de reviews con palabras lemmatizadas\n",
    "    \"\"\"\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Para cada review:\n",
    "    # - Dividir en palabras\n",
    "    # - Aplicar lemmatization a cada palabra\n",
    "    # - Unir de nuevo\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) \n",
    "            for review in corpus]\n",
    "\n",
    "# Lemmatizamos las reviews\n",
    "lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\n",
    "lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n",
    "\n",
    "# Vectorizamos con conteo tras lematizar\n",
    "cv = CountVectorizer(binary=True, stop_words=english_stop_words)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "# Entrenamos y evaluamos\n",
    "train_model(X, X_test)\n",
    "\n",
    "# Resultado: ~87.82%\n",
    "# Similar a stemming, ligeramente mejor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYk6uutsevL0",
    "outputId": "43fef4a6-b2db-4b42-a113-b0887aa1fdfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 80215)\n",
      "Diff X normal y X tras lematizador y vectorización: 6848\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPACTO DE LEMMATIZATION EN EL VOCABULARIO\n",
    "# =============================================================================\n",
    "# Comparamos el tamaño del vocabulario con lemmatization vs baseline\n",
    "\n",
    "print(X_baseline.shape)  # Baseline: 87,063 palabras\n",
    "print(X.shape)           # Con lemmatization: 80,215 palabras\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", \n",
    "      X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "# Reducción de 6,848 palabras (7.9%)\n",
    "# Mucho MENOS reducción que stemming (que eliminó 20,348 palabras)\n",
    "#\n",
    "# ¿Por qué?\n",
    "# Lemmatization es menos agresivo:\n",
    "# - Stemming: \"running\", \"runs\", \"ran\" → \"run\" (todas iguales)\n",
    "# - Lemmatization: \"running\" → \"running\", \"runs\" → \"run\", \"ran\" → \"ran\"\n",
    "#   (preserva algunas diferencias si no tiene contexto)\n",
    "#\n",
    "# Conclusión:\n",
    "# - Lemmatization: más preciso, preserva más información\n",
    "# - Stemming: más agresivo, reduce más el vocabulario\n",
    "# - En este caso, ninguno mejora significativamente el accuracy del baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjD2rQxuevL0"
   },
   "source": [
    "# n-gramas\n",
    "\n",
    "Potencialmente podemos agregar más poder predictivo a nuestro modelo agregando secuencias de dos o tres palabras (bigramas o trigramas) también. Por ejemplo, si una reseña tiene la secuencia de tres palabras \"didn't love movie\" solo consideraríamos estas palabras individualmente con un modelo de solo unigramas y probablemente no capturaríamos que esto es en realidad un sentimiento negativo porque la palabra 'love' por sí misma estará altamente correlacionada con una reseña positiva.\n",
    "\n",
    "La librería scikit-learn hace esto muy fácil de probar. Solo usa el argumento ngram_range con cualquiera de las clases 'Vectorizer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lW6kUowYevL0",
    "outputId": "dfc2c752-2b72-41fc-fd3a-d3e01b804ebd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMAS (palabras individuales):\n",
      "(\"didn't\",)\n",
      "('love',)\n",
      "('music',)\n",
      "('at',)\n",
      "('all',)\n",
      "('my',)\n",
      "('love',)\n",
      "###############\n",
      "BIGRAMAS (pares de palabras):\n",
      "(\"didn't\", 'love')\n",
      "('love', 'music')\n",
      "('music', 'at')\n",
      "('at', 'all')\n",
      "('all', 'my')\n",
      "('my', 'love')\n",
      "###############\n",
      "TRIGRAMAS (tríos de palabras):\n",
      "(\"didn't\", 'love', 'music')\n",
      "('love', 'music', 'at')\n",
      "('music', 'at', 'all')\n",
      "('at', 'all', 'my')\n",
      "('all', 'my', 'love')\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# N-GRAMAS - Capturando secuencias de palabras\n",
    "# =============================================================================\n",
    "# ¿Qué son los n-gramas?\n",
    "# Son secuencias de N palabras consecutivas en un texto\n",
    "#\n",
    "# Tipos:\n",
    "# - Unigrama (n=1): palabras individuales → \"didn't\", \"love\", \"music\"\n",
    "# - Bigrama (n=2): pares de palabras → \"didn't love\", \"love music\"\n",
    "# - Trigrama (n=3): tríos de palabras → \"didn't love music\", \"love music at\"\n",
    "#\n",
    "# ¿Por qué usar n-gramas?\n",
    "# - Capturan contexto: \"not good\" tiene significado negativo\n",
    "# - Solo con unigramas: \"not\" (negativo?) + \"good\" (positivo?) = confuso\n",
    "# - Con bigramas: \"not good\" es claramente negativo\n",
    "#\n",
    "# Ejemplos útiles en análisis de sentimiento:\n",
    "# - \"didn't love\" (negativo, aunque \"love\" solo sea positivo)\n",
    "# - \"not bad\" (positivo, aunque \"not\" y \"bad\" sean negativos)\n",
    "# - \"very good\" (muy positivo)\n",
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"didn't love music at all my love\"\n",
    "\n",
    "# Creamos unigramas, bigramas y trigramas\n",
    "one = ngrams(sentence.split(), 1)    # Palabras individuales\n",
    "two = ngrams(sentence.split(), 2)    # Pares de palabras\n",
    "three = ngrams(sentence.split(), 3)  # Tríos de palabras\n",
    "\n",
    "# Unigramas\n",
    "print(\"UNIGRAMAS (palabras individuales):\")\n",
    "for grams in one:\n",
    "    print(grams)\n",
    "print('###############')\n",
    "\n",
    "# Bigramas\n",
    "print(\"BIGRAMAS (pares de palabras):\")\n",
    "for grams in two:\n",
    "    print(grams)\n",
    "print('###############')\n",
    "\n",
    "# Trigramas\n",
    "print(\"TRIGRAMAS (tríos de palabras):\")\n",
    "for grams in three:\n",
    "    print(grams)\n",
    "\n",
    "# Observa cómo los bigramas capturan mejor el contexto:\n",
    "# - (\"didn't\", \"love\") transmite negación del amor\n",
    "# - Solo \"love\" podría parecer positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VfYGKmDevL1",
    "outputId": "88983c1e-76cd-44b1-efeb-2b28ea86b16b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# N-GRAMAS CON CountVectorizer - Ejemplo simple\n",
    "# =============================================================================\n",
    "# CountVectorizer puede generar n-gramas automáticamente con ngram_range\n",
    "# \n",
    "# ngram_range es una tupla (min_n, max_n):\n",
    "# - (1, 1): solo unigramas\n",
    "# - (2, 2): solo bigramas\n",
    "# - (1, 2): unigramas Y bigramas\n",
    "# - (1, 3): unigramas, bigramas Y trigramas\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 2))  # Unigramas + Bigramas\n",
    "\n",
    "# Aplicamos al ejemplo simple\n",
    "vector = ngram_vectorizer.fit_transform([sentence]).toarray()\n",
    "print(vector)\n",
    "print(len(vector[0]))\n",
    "\n",
    "# El vector tiene 12 elementos:\n",
    "# - 7 unigramas únicos\n",
    "# - 5 bigramas únicos (6 bigramas totales, pero \"love\" se repite)\n",
    "# \n",
    "# NOTA: algunas palabras pueden ser eliminadas automáticamente por CountVectorizer\n",
    "# (por ejemplo, palabras de una sola letra como 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXexWbebevL1",
    "outputId": "9df978bf-b2a8-4c6b-8d36-18e2727d0ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8874\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APLICAR N-GRAMAS A NUESTRAS REVIEWS - Con límite de features\n",
    "# =============================================================================\n",
    "# Ahora aplicamos n-gramas a todas nuestras reviews de películas\n",
    "# \n",
    "# IMPORTANTE: ngram_range=(1, 2) crea MUCHAS features\n",
    "# - Unigramas: ~87,000\n",
    "# - Bigramas: cientos de miles o millones\n",
    "# - Total: puede explotar en tamaño\n",
    "#\n",
    "# Solución: max_features limita el número de features\n",
    "# Selecciona solo las N palabras/n-gramas más frecuentes\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True, \n",
    "                                   stop_words=english_stop_words,\n",
    "                                   ngram_range=(1, 2),      # Unigramas + bigramas\n",
    "                                   max_features=30000)      # Solo las 30,000 más frecuentes\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "train_model(X, X_test)\n",
    "\n",
    "# Resultado: ~88.74% - ¡MEJORA respecto al baseline!\n",
    "# Los bigramas ayudan al modelo a entender mejor el contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTvaVdSbevL1",
    "outputId": "16021f12-1a49-4dcb-bbb4-ad41979a0386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 30000)\n",
      "Diff X normal y X tras n-gramas: 57063\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# IMPACTO DE max_features\n",
    "# =============================================================================\n",
    "# Comparamos el tamaño de features con y sin límite\n",
    "\n",
    "print(X_baseline.shape)  # Baseline: 87,063 palabras (solo unigramas)\n",
    "print(X.shape)           # Con n-gramas: 30,000 features (limitado por max_features)\n",
    "print(\"Diff X normal y X tras n-gramas:\", X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "# Sin max_features, ¡tendríamos 1,448,047 features!\n",
    "# \n",
    "# ¿Por qué tantas?\n",
    "# Número de bigramas posibles = vocabulario × vocabulario\n",
    "# Si tenemos ~87,000 palabras, hay millones de combinaciones posibles\n",
    "#\n",
    "# max_features=30000 nos da un buen balance:\n",
    "# - Suficientes features para capturar patrones importantes\n",
    "# - No tantas que el modelo sea inmanejable\n",
    "# - Selecciona las más frecuentes/informativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qN6ZyKxcevL2",
    "outputId": "b15a8165-012a-46c2-e121-98231bc003d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 10000)\n",
      "Diff X normal y X tras n-gramas: 77063\n",
      "Final Accuracy: 0.81536\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENTO: Solo bigramas (sin unigramas)\n",
    "# =============================================================================\n",
    "# ¿Qué pasa si usamos SOLO bigramas?\n",
    "# ngram_range=(2, 2) → solo pares de palabras, no palabras individuales\n",
    "\n",
    "ngram_vectorizer = CountVectorizer(binary=True, \n",
    "                                   stop_words=english_stop_words,\n",
    "                                   ngram_range=(2, 2),      # SOLO bigramas\n",
    "                                   max_features=10000)      # 10,000 bigramas más frecuentes\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "print(X_baseline.shape)  # Baseline: 87,063 unigramas\n",
    "print(X.shape)           # Solo bigramas: 10,000 features\n",
    "print(\"Diff X normal y X tras n-gramas:\", X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "# Entrenamos el modelo\n",
    "train_model(X, X_test)\n",
    "\n",
    "# Resultado: ~81.54% - PEOR que el baseline\n",
    "# \n",
    "# Conclusión:\n",
    "# - Los bigramas solos NO son suficientes\n",
    "# - Necesitamos COMBINAR unigramas + bigramas para mejor rendimiento\n",
    "# - Los unigramas capturan palabras individuales importantes\n",
    "# - Los bigramas capturan contexto y relaciones entre palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYjtZdpIevL2"
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "Otra forma común de representar cada documento en un corpus es usar la estadística tf-idf (term frequency-inverse document frequency) para cada palabra, que es un factor de ponderación que podemos usar en lugar de representaciones binarias o de conteo de palabras.\n",
    "\n",
    "Hay varias formas de hacer la transformación tf-idf pero en resumen, **tf-idf busca representar el número de veces que una palabra dada aparece en un documento (una reseña de película en nuestro caso) en relación con el número de documentos en el corpus en los que aparece la palabra**.\n",
    "\n",
    "**Nota:** Ahora que hemos visto los n-gramas, cuando me refiero a 'palabras' realmente quiero decir cualquier n-grama (secuencia de palabras) si el modelo está usando una n mayor que uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DBp_rX83evL2",
    "outputId": "a0980269-82ac-4dcb-b1f8-76821231789a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.2876820724517808)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CÁLCULO MANUAL DE IDF (Inverse Document Frequency)\n",
    "# =============================================================================\n",
    "# Antes de usar TfidfVectorizer, entendamos cómo funciona el cálculo IDF\n",
    "# \n",
    "# IDF (Inverse Document Frequency):\n",
    "# Penaliza palabras muy comunes y realza palabras raras\n",
    "# Fórmula: 1 + ln((N + 1) / (count + 1))\n",
    "# \n",
    "# Donde:\n",
    "# - N = número total de documentos en el corpus\n",
    "# - count = número de documentos que contienen la palabra\n",
    "#\n",
    "# Ejemplo: si una palabra aparece en muchos documentos, su IDF será bajo\n",
    "\n",
    "# Número de documentos\n",
    "N = 3\n",
    "\n",
    "# Número de veces que aparece una palabra específica (en cuántos documentos)\n",
    "count = 2\n",
    "\n",
    "# Calculamos el IDF\n",
    "# Si la palabra aparece en 2 de 3 documentos:\n",
    "# IDF = 1 + ln((3 + 1) / (2 + 1)) = 1 + ln(4/3) = 1.288\n",
    "1 + np.log((N + 1)/(count + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "io0cL4HAevL2",
    "outputId": "346a6f87-dbbb-4f07-a0ba-e7bd85c087b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.28768207 1.69314718 1.69314718 1.69314718 1.        ]\n",
      "['is' 'my' 'name' 'nice' 'ralph']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TF-IDF - Term Frequency - Inverse Document Frequency\n",
    "# =============================================================================\n",
    "# TF-IDF combina dos métricas:\n",
    "# \n",
    "# 1. TF (Term Frequency): \n",
    "#    ¿Cuántas veces aparece la palabra en un documento específico?\n",
    "# \n",
    "# 2. IDF (Inverse Document Frequency):\n",
    "#    ¿Qué tan rara/común es la palabra en todo el corpus?\n",
    "#    - Palabras muy comunes (ej: \"the\", \"is\") → IDF bajo\n",
    "#    - Palabras raras pero informativas → IDF alto\n",
    "#\n",
    "# TF-IDF = TF × IDF\n",
    "#\n",
    "# ¿Por qué usar TF-IDF en lugar de conteos simples?\n",
    "# - Reduce el peso de palabras muy comunes\n",
    "# - Aumenta el peso de palabras distintivas\n",
    "# - Mejor representación del \"contenido\" real del documento\n",
    "#\n",
    "# Ejemplo práctico:\n",
    "# - Palabra \"ralph\" aparece en 3 documentos → IDF = 1.0 (muy común en este corpus)\n",
    "# - Palabra \"nice\" aparece en 1 documento → IDF = 1.69 (más distintiva)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Corpus de ejemplo\n",
    "sent1 = 'My name is Ralph'\n",
    "sent2 = 'Ralph is nice'\n",
    "sent3 = 'Ralph'\n",
    "\n",
    "# Creamos el TfidfVectorizer\n",
    "test = TfidfVectorizer()\n",
    "test.fit_transform([sent1, sent2, sent3])\n",
    "\n",
    "# Valores IDF para cada palabra\n",
    "# Cuanto más común es la palabra, más bajo es su IDF\n",
    "print(test.idf_)\n",
    "print(test.get_feature_names_out())\n",
    "\n",
    "# Resultados:\n",
    "# 'ralph': 1.0 (aparece en los 3 documentos, muy común)\n",
    "# 'my', 'name', 'nice': 1.69 (aparecen en 1 solo documento, distintivas)\n",
    "# 'is': 1.29 (aparece en 2 documentos, medianamente común)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ylcN8G3yevL3",
    "outputId": "92d668b6-5d39-4bc3-eed6-377c0980e481"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERIFICAR EL CÁLCULO IDF MANUALMENTE\n",
    "# =============================================================================\n",
    "# Verificamos que el cálculo coincide con lo que hace TfidfVectorizer\n",
    "\n",
    "# 'ralph' aparece en 3 documentos de un total de 3\n",
    "# Fórmula: 1 + ln((N + 1) / (count + 1))\n",
    "# 1 + ln((3 + 1) / (3 + 1)) = 1 + ln(4/4) = 1 + ln(1) = 1 + 0 = 1.0\n",
    "\n",
    "1 + np.log((3 + 1)/(3 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grzH7PgAevL3",
    "outputId": "6c0afa12-f902-4230-953e-ef4ff08615b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# APLICAR TF-IDF A NUESTRAS REVIEWS\n",
    "# =============================================================================\n",
    "# Ahora aplicamos TfidfVectorizer en lugar de CountVectorizer\n",
    "# Esto convierte cada review en un vector de valores TF-IDF\n",
    "# en lugar de simples conteos binarios o frecuencias\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creamos el vectorizador TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustamos y transformamos las reviews\n",
    "tfidf_vectorizer.fit(reviews_train_clean)\n",
    "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
    "print(X.shape)\n",
    "\n",
    "# Transformamos test con el mismo vectorizador\n",
    "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "# Ahora cada celda de la matriz NO contiene 0 o 1\n",
    "# Contiene valores continuos (floats) que representan TF-IDF\n",
    "# Valores más altos = palabras más importantes para ese documento específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smppaV2hevL3",
    "outputId": "7b4ca924-2f43-49cb-d760-36371292a517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88176\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENTRENAR MODELO CON TF-IDF\n",
    "# =============================================================================\n",
    "# Entrenamos Regresión Logística con las features TF-IDF\n",
    "# ¿Mejora respecto al baseline con CountVectorizer?\n",
    "\n",
    "train_model(X, X_test)\n",
    "\n",
    "# Resultado: ~88.18% \n",
    "# Prácticamente IGUAL que el baseline con CountVectorizer binario\n",
    "# \n",
    "# Conclusión:\n",
    "# - TF-IDF no siempre es mejor que conteos binarios\n",
    "# - Para análisis de sentimiento, la presencia/ausencia (binario) suele funcionar bien\n",
    "# - TF-IDF es más útil en tareas como búsqueda de información o clasificación de tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvK1aSuKevL3",
    "outputId": "918dc1f5-145d-4a67-b2d4-15cf0133e09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 87063)\n",
      "Diff X normal y X tras TF-IDF: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARACIÓN DE DIMENSIONES: TF-IDF vs Baseline\n",
    "# =============================================================================\n",
    "# Comparamos el tamaño de las matrices\n",
    "\n",
    "print(X_baseline.shape)  # CountVectorizer binario: 87,063 palabras\n",
    "print(X.shape)           # TfidfVectorizer: 87,063 palabras\n",
    "print(\"Diff X normal y X tras TF-IDF:\", X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "# ¡Mismo número de features! (diferencia = 0)\n",
    "# Lo que cambia son los VALORES dentro de la matriz:\n",
    "# - CountVectorizer binario: solo 0 y 1\n",
    "# - TfidfVectorizer: valores continuos (floats) entre 0 y ~1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tonxn9kevL4"
   },
   "source": [
    "# Máquinas de Vectores de Soporte (SVM)\n",
    "\n",
    "Recordemos que los clasificadores lineales tienden a funcionar bien en conjuntos de datos muy dispersos (como el que tenemos). Otro algoritmo que puede producir excelentes resultados con un tiempo de entrenamiento rápido son las Máquinas de Vectores de Soporte con un kernel lineal.\n",
    "\n",
    "Construir un modelo con un rango de n-gramas de 1 a 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gdwj1YkTevL4",
    "outputId": "ffa66563-213a-4b24-fd6f-fe8562d427cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88648\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SVM (Support Vector Machine) CON N-GRAMAS\n",
    "# =============================================================================\n",
    "# Ahora probamos un algoritmo diferente: SVM con kernel lineal\n",
    "# \n",
    "# ¿Por qué SVM?\n",
    "# - Funciona muy bien con datos de alta dimensionalidad (muchas features)\n",
    "# - Especialmente bueno con datos sparse (matrices con muchos ceros)\n",
    "# - A menudo más rápido que Regresión Logística en estos casos\n",
    "# - Encuentra el hiperplano óptimo que separa las clases\n",
    "#\n",
    "# LinearSVC = Linear Support Vector Classifier\n",
    "# \"Linear\" porque usa un kernel lineal (decisiones lineales)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuramos CountVectorizer con n-gramas (1-3)\n",
    "# Incluimos unigramas, bigramas Y trigramas\n",
    "ngram_vectorizer = CountVectorizer(binary=True, \n",
    "                                   ngram_range=(1, 3),      # Hasta trigramas\n",
    "                                   max_features=20000)      # Limitamos a 20,000 features\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "# Función para entrenar SVM con Grid Search\n",
    "def train_model_svm(X_TRAIN, X_TEST):\n",
    "    \"\"\"\n",
    "    Entrena un modelo SVM con Grid Search\n",
    "    \n",
    "    Hiperparámetro C:\n",
    "    - Similar a Regresión Logística\n",
    "    - C alto: modelo más complejo, puede hacer overfitting\n",
    "    - C bajo: modelo más simple, puede hacer underfitting\n",
    "    \n",
    "    Args:\n",
    "        X_TRAIN: matriz de features de entrenamiento\n",
    "        X_TEST: matriz de features de test\n",
    "    \"\"\"\n",
    "    \n",
    "    svm = LinearSVC()  # Creamos el clasificador SVM\n",
    "    \n",
    "    # Valores de C a probar\n",
    "    params = {\n",
    "        'C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    # Grid Search con validación cruzada\n",
    "    grid = GridSearchCV(svm, params, cv=5)\n",
    "    grid.fit(X_TRAIN, target)\n",
    "    \n",
    "    # Evaluamos en test\n",
    "    print(\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))\n",
    "\n",
    "# Entrenamos el SVM\n",
    "train_model_svm(X, X_test)\n",
    "\n",
    "# Resultado: ~88.65%\n",
    "# ¡Mejor que el baseline! Los trigramas + SVM ayudan un poco más"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWUMedCkevL4"
   },
   "source": [
    "# Modelo Final\n",
    "\n",
    "Eliminar un pequeño conjunto de stop words junto con un rango de n-gramas de 1 a 3 y un clasificador de vectores de soporte lineal muestra los mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9CnJ4kblevL4",
    "outputId": "3a3059de-cc14-4230-8f27-134a1f8fab42"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODELO FINAL - Mejor configuración encontrada\n",
    "# =============================================================================\n",
    "# Después de probar diferentes técnicas, el mejor modelo combina:\n",
    "# - N-gramas (1-3): unigramas, bigramas y trigramas\n",
    "# - Lista pequeña de stopwords (no la lista completa de NLTK)\n",
    "# - Regresión Logística (funciona casi tan bien como SVM)\n",
    "#\n",
    "# ¿Por qué esta configuración?\n",
    "# - Los trigramas capturan frases completas como \"not very good\"\n",
    "# - Eliminar TODAS las stopwords puede ser contraproducente\n",
    "# - Solo eliminamos las palabras más comunes que claramente no aportan\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Lista REDUCIDA de stopwords\n",
    "# Solo incluimos las más comunes y menos informativas\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "\n",
    "# CountVectorizer con la mejor configuración\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 3),         # Hasta trigramas\n",
    "                                   stop_words=stop_words)      # Pocas stopwords\n",
    "\n",
    "# Ajustamos y transformamos\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "# Entrenamos con Regresión Logística\n",
    "train_model(X, X_test)\n",
    "\n",
    "# Resultado: ~90.01% - ¡EL MEJOR HASTA AHORA!\n",
    "# \n",
    "# Mejoras respecto al baseline (88.18%):\n",
    "# - +1.83% de accuracy\n",
    "# - Mejor captura de contexto con n-gramas\n",
    "# - Balance entre complejidad y generalización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MeRFH8KevL5"
   },
   "source": [
    "# Características Positivas y Negativas Principales\n",
    "\n",
    "Obtener las características más importantes del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZF_AZ1WevL6",
    "outputId": "9c6324d6-5d38-4d33-daf4-1dca30e237a5"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANÁLISIS DE FEATURES - Palabras más importantes\n",
    "# =============================================================================\n",
    "# Ahora vamos a investigar qué palabras son más importantes para el modelo\n",
    "# Esto nos ayuda a entender QUÉ está aprendiendo el modelo\n",
    "#\n",
    "# Entrenamos un modelo simple (sin n-gramas) para análisis\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train_clean)\n",
    "X = cv.transform(reviews_train_clean)\n",
    "\n",
    "# Entrenamos Regresión Logística\n",
    "log_reg = LogisticRegression(C=0.5)\n",
    "log_reg.fit(X, target)\n",
    "\n",
    "# Importancia de los coeficientes\n",
    "# log_reg.coef_ contiene los pesos aprendidos por el modelo\n",
    "# - Coeficiente positivo alto → palabra fuertemente asociada con clase positiva\n",
    "# - Coeficiente negativo alto → palabra fuertemente asociada con clase negativa\n",
    "print(len(log_reg.coef_[0]))  # Total: 87,063 coeficientes (uno por palabra)\n",
    "\n",
    "# Cada palabra tiene su coeficiente asociado\n",
    "cv.get_feature_names_out()\n",
    "\n",
    "# Creamos un diccionario: palabra → coeficiente\n",
    "# Esto nos permite ordenar y encontrar las palabras más importantes\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names_out(), log_reg.coef_[0]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZAdTW3R0evL6",
    "outputId": "1075c7c2-9da7-4e9c-a268-d375d3df8740"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRUEBA DEL MODELO - Review negativa\n",
    "# =============================================================================\n",
    "# Probamos el modelo con una review claramente negativa\n",
    "# predict() devuelve 0 (negativo) o 1 (positivo)\n",
    "\n",
    "log_reg.predict(cv.transform(['This movie is horrible']))\n",
    "\n",
    "# Resultado esperado: [0] (negativo)\n",
    "# ¡El modelo identifica correctamente el sentimiento negativo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMdRuQNpevL6",
    "outputId": "08f1ad64-28b3-4dcb-dd83-3ac7bba1e1f1"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRUEBA DEL MODELO - Review positiva\n",
    "# =============================================================================\n",
    "# Probamos con una review claramente positiva\n",
    "\n",
    "log_reg.predict(cv.transform(['This movie is incredible']))\n",
    "\n",
    "# Resultado esperado: [1] (positivo)\n",
    "# ¡El modelo identifica correctamente el sentimiento positivo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5F-4y4_evL6",
    "outputId": "99c348bd-402f-4cba-c98b-58175f3d5263"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TOP 5 PALABRAS MÁS POSITIVAS Y NEGATIVAS\n",
    "# =============================================================================\n",
    "# Identificamos las palabras con mayor peso en cada sentimiento\n",
    "# Esto nos ayuda a interpretar qué aprendió el modelo\n",
    "\n",
    "# TOP 5 PALABRAS MÁS POSITIVAS\n",
    "# Ordenamos por coeficiente de mayor a menor (reverse=True)\n",
    "print(\"TOP 5 PALABRAS MÁS POSITIVAS:\")\n",
    "print(\"=\" * 50)\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1],      # Ordenar por coeficiente\n",
    "    reverse=True)[:5]:        # Top 5\n",
    "    print(f\"{best_positive[0]}: {best_positive[1]:.4f}\")\n",
    "\n",
    "print('\\n' + '=' * 50)\n",
    "print(\"TOP 5 PALABRAS MÁS NEGATIVAS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TOP 5 PALABRAS MÁS NEGATIVAS\n",
    "# Ordenamos por coeficiente de menor a mayor (sin reverse)\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(),\n",
    "    key=lambda x: x[1])[:5]:  # Top 5 más negativas\n",
    "    print(f\"{best_negative[0]}: {best_negative[1]:.4f}\")\n",
    "\n",
    "# Resultados típicos:\n",
    "# POSITIVAS: 'excellent', 'perfect', 'superb', 'wonderful', 'brilliant'\n",
    "# NEGATIVAS: 'worst', 'waste', 'awful', 'boring', 'terrible'\n",
    "#\n",
    "# ¡Tiene sentido! El modelo aprendió palabras que realmente indican sentimiento\n",
    "#\n",
    "# Interpretación de coeficientes:\n",
    "# - 'excellent': +1.38 → fuerte indicador de review positiva\n",
    "# - 'worst': -2.08 → fuerte indicador de review negativa\n",
    "# - Cuanto mayor el valor absoluto, más importante es la palabra para clasificar"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "90139cb9a825bf3d63f6f6704e828dbd1ff7edbd4d0c6e906a71235d6efc74af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
