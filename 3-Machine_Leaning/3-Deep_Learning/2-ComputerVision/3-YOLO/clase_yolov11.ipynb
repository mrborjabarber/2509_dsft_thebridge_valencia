{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Detección de Objetos YOLOv11\n",
        "\n",
        "## ¿Qué es YOLO?\n",
        "\n",
        "**YOLO** (You Only Look Once) es uno de los algoritmos más populares y rápidos para **detección de objetos en tiempo real**. A diferencia de otros métodos que analizan la imagen múltiples veces, YOLO procesa la imagen completa en una sola pasada, haciéndolo extremadamente rápido.\n",
        "\n",
        "### Características principales:\n",
        "-  **Velocidad**: Procesa imágenes en tiempo real (30+ FPS)\n",
        "-  **Precisión**: Detecta múltiples objetos simultáneamente\n",
        "-  **80+ Clases**: Puede detectar personas, vehículos, animales, etc.\n",
        "-  **Bounding Boxes**: Dibuja cajas delimitadoras alrededor de objetos\n",
        "-  **Confianza**: Proporciona un score de confianza para cada detección\n",
        "\n",
        "### ¿Qué es YOLOv11?\n",
        "\n",
        "YOLOv11 es la versión más reciente de la familia YOLO, desarrollada por Ultralytics. Mejora sobre versiones anteriores con:\n",
        "-  Mayor velocidad de inferencia\n",
        "-  Mejor precisión en la detección\n",
        "-  Arquitectura más eficiente\n",
        "-  Más fácil de usar e implementar\n",
        "\n",
        "### Aplicaciones en el mundo real:\n",
        "-  Vehículos autónomos\n",
        "-  Sistemas de seguridad y vigilancia\n",
        "-  Conteo de personas en retail\n",
        "-  Control de calidad en manufactura\n",
        "-  Análisis de tráfico urbano\n",
        "-  Aplicaciones móviles de realidad aumentada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Instalación de Dependencias\n",
        "\n",
        "Vamos a instalar todas las librerías necesarias para trabajar con YOLOv11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar las librerías necesarias\n",
        "# ultralytics: Contiene YOLOv11 y todas sus utilidades\n",
        "# opencv-python: Para procesamiento de imágenes y video\n",
        "# numpy: Para operaciones numéricas\n",
        "# matplotlib: Para visualización\n",
        "\n",
        "#!pip install ultralytics opencv-python numpy matplotlib pillow\n",
        "\n",
        "#print(\"\\n✓ Instalación completada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Librerías importadas correctamente\n",
            "Versión de OpenCV: 4.12.0\n"
          ]
        }
      ],
      "source": [
        "# Importar las librerías necesarias\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import time\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Configurar el tamaño de las figuras\n",
        "rcParams['figure.figsize'] = 14, 10\n",
        "\n",
        "print(\"✓ Librerías importadas correctamente\")\n",
        "print(f\"Versión de OpenCV: {cv2.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cargar el Modelo YOLOv11\n",
        "\n",
        "YOLOv11 viene en diferentes tamaños, cada uno con un balance diferente entre velocidad y precisión:\n",
        "\n",
        "| Modelo | Tamaño | Velocidad | Precisión | Uso recomendado |\n",
        "|--------|---------|-----------|-----------|------------------|\n",
        "| YOLOv11n | Nano | ⚡⚡⚡⚡⚡ | ⭐⭐⭐ | Dispositivos móviles, Edge computing |\n",
        "| YOLOv11s | Small | ⚡⚡⚡⚡ | ⭐⭐⭐⭐ | Aplicaciones en tiempo real |\n",
        "| YOLOv11m | Medium | ⚡⚡⚡ | ⭐⭐⭐⭐ | Balance velocidad-precisión |\n",
        "| YOLOv11l | Large | ⚡⚡ | ⭐⭐⭐⭐⭐ | Alta precisión |\n",
        "| YOLOv11x | Extra Large | ⚡ | ⭐⭐⭐⭐⭐ | Máxima precisión |\n",
        "\n",
        "Para este tutorial usaremos **YOLOv11n** (Nano) por su velocidad, ideal para webcam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando modelo YOLOv11n...\n",
            "✓ Modelo YOLOv11n cargado exitosamente\n",
            "\n",
            "Modelo: yolo11s.pt\n",
            "Tipo de tarea: detect\n"
          ]
        }
      ],
      "source": [
        "# Cargar el modelo YOLOv11 Nano (el más rápido)\n",
        "# La primera vez que ejecutes esto, descargará los pesos del modelo (~6 MB para nano)\n",
        "\n",
        "print(\"Cargando modelo YOLOv11n...\")\n",
        "modelo = YOLO('yolo11s.pt')  # Descarga automáticamente si no existe\n",
        "\n",
        "print(\"✓ Modelo YOLOv11n cargado exitosamente\")\n",
        "print(f\"\\nModelo: {modelo.model_name}\")\n",
        "print(f\"Tipo de tarea: {modelo.task}\")\n",
        "\n",
        "# Si prefieres otro modelo, puedes usar:\n",
        "# modelo = YOLO('yolo11s.pt')  # Small - más preciso pero más lento\n",
        "# modelo = YOLO('yolo11m.pt')  # Medium\n",
        "# modelo = YOLO('yolo11l.pt')  # Large\n",
        "# modelo = YOLO('yolo11x.pt')  # Extra Large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Clases que Puede Detectar YOLO\n",
        "\n",
        "El modelo está entrenado en el dataset COCO (Common Objects in Context) que contiene 80 clases de objetos comunes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener la lista de clases que puede detectar el modelo\n",
        "nombres_clases = modelo.names\n",
        "\n",
        "print(f\"El modelo puede detectar {len(nombres_clases)} clases diferentes:\\n\")\n",
        "\n",
        "# Mostrar las clases en columnas para mejor visualización\n",
        "clases_lista = list(nombres_clases.values())\n",
        "num_columnas = 4\n",
        "filas = [clases_lista[i:i+num_columnas] for i in range(0, len(clases_lista), num_columnas)]\n",
        "\n",
        "for fila in filas:\n",
        "    print(\"  \".join([f\"{clase:20s}\" for clase in fila]))\n",
        "\n",
        "print(\"\\nAlgunas clases destacadas:\")\n",
        "print(\"   Personas: person\")\n",
        "print(\"   Vehículos: car, truck, bus, motorcycle, bicycle\")\n",
        "print(\"   Animales: dog, cat, bird, horse, cow, etc.\")\n",
        "print(\"   Objetos: cell phone, laptop, keyboard, mouse\")\n",
        "print(\"   Comida: pizza, apple, sandwich, banana, etc.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Primera Detección: Imagen Estática\n",
        "\n",
        "Antes de trabajar con la webcam, vamos a probar YOLO con una imagen estática para entender cómo funciona."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Crear una imagen de ejemplo\n",
        "imagen_prueba = cv2.imread(\"./img/mix.jpg\")\n",
        "# Mostrar la imagen original\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(cv2.cvtColor(imagen_prueba, cv2.COLOR_BGR2RGB))\n",
        "plt.title('Imagen Original')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Primero trabajamos dentro del notebook\n",
        "\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Carga la imagen desde el archivo especificado\n",
        "img = cv2.imread(\"./img/mix.jpg\")\n",
        "\n",
        "# Ejecuta el modelo sobre la imagen para obtener predicciones\n",
        "results = modelo(img)\n",
        "\n",
        "# Genera una versión de la imagen con las detecciones dibujadas\n",
        "annotated_frame = results[0].plot()\n",
        "\n",
        "# Convertir de BGR (OpenCV) a RGB (Matplotlib)\n",
        "annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Mostrar la imagen directamente en el notebook\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(annotated_frame_rgb)\n",
        "plt.axis(\"off\")  # Oculta los ejes\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar la detección con YOLOv11\n",
        "# El modelo procesa la imagen y devuelve las detecciones\n",
        "\n",
        "print(\"Ejecutando detección con YOLOv11...\")\n",
        "\n",
        "# Realizar la predicción\n",
        "# conf: umbral de confianza mínimo (0.0 a 1.0)\n",
        "# iou: umbral de Intersection over Union para NMS (Non-Maximum Suppression)\n",
        "resultados = modelo.predict(\n",
        "    source=imagen_prueba,\n",
        "    conf=0.25,  # Confianza mínima del 25%\n",
        "    iou=0.45,   # IOU threshold para eliminar detecciones duplicadas\n",
        "    verbose=False  # No mostrar logs detallados\n",
        ")\n",
        "\n",
        "# Obtener la primera (y única) imagen de resultados\n",
        "resultado = resultados[0]\n",
        "\n",
        "# Información sobre las detecciones\n",
        "num_detecciones = len(resultado.boxes)\n",
        "print(f\"\\n✓ Detección completada\")\n",
        "print(f\"Objetos detectados: {num_detecciones}\")\n",
        "\n",
        "# Visualizar el resultado con las cajas dibujadas\n",
        "imagen_con_detecciones = resultado.plot()  # Dibuja las cajas automáticamente\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.imshow(cv2.cvtColor(imagen_con_detecciones, cv2.COLOR_BGR2RGB))\n",
        "plt.title(f'Detecciones de YOLOv11 ({num_detecciones} objetos)')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Mostrar detalles de cada detección\n",
        "if num_detecciones > 0:\n",
        "    print(\"\\nDetalles de las detecciones:\")\n",
        "    for i, box in enumerate(resultado.boxes):\n",
        "        clase_id = int(box.cls[0])\n",
        "        clase_nombre = nombres_clases[clase_id]\n",
        "        confianza = float(box.conf[0])\n",
        "        coordenadas = box.xyxy[0].cpu().numpy()\n",
        "        \n",
        "        print(f\"\\n  Objeto {i+1}:\")\n",
        "        print(f\"    Clase: {clase_nombre}\")\n",
        "        print(f\"    Confianza: {confianza*100:.2f}%\")\n",
        "        print(f\"    Coordenadas: x1={coordenadas[0]:.0f}, y1={coordenadas[1]:.0f}, \"\n",
        "              f\"x2={coordenadas[2]:.0f}, y2={coordenadas[3]:.0f}\")\n",
        "else:\n",
        "    print(\"\\nNo se detectaron objetos con la confianza mínima establecida.\")\n",
        "    print(\"Tip: Usa una imagen real con objetos del mundo real para mejores resultados.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Pero si queremos trabajar en un script de Python para usarlo en una aplicación, debemos usar este código:\n",
        "\n",
        "# Carga la imagen desde el archivo especificado\n",
        "img = cv2.imread(\"./img/mix.jpg\")\n",
        "\n",
        "# Ejecuta el modelo sobre la imagen para obtener predicciones\n",
        "results = modelo(img)\n",
        "\n",
        "# Genera una versión de la imagen con las detecciones dibujadas\n",
        "annotated_frame = results[0].plot()\n",
        "\n",
        "# Muestra la imagen anotada en una ventana\n",
        "cv2.imshow(\"Detección - Imagen\", annotated_frame)\n",
        "\n",
        "# Espera a que se presione una tecla para cerrar la ventana\n",
        "cv2.waitKey(0)\n",
        "\n",
        "# Cierra todas las ventanas abiertas por OpenCV\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## probemos con video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Abre el archivo de video indicado\n",
        "cap = cv2.VideoCapture(\"./video/people.mp4\")\n",
        "\n",
        "# Bucle principal: se ejecuta mientras el video esté abierto\n",
        "while cap.isOpened():\n",
        "\n",
        "    # Lee un fotograma del video\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Si no se pudo leer (fin del video o error), salir del bucle\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Ejecuta el modelo sobre el fotograma para obtener predicciones\n",
        "    results = modelo(frame)\n",
        "\n",
        "    # Genera un fotograma anotado con las detecciones dibujadas\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Muestra el fotograma procesado en una ventana\n",
        "    cv2.imshow(\"Detección - Video\", annotated_frame)\n",
        "\n",
        "    # Espera una tecla; si se presiona 'q', se detiene la reproducción\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Presiona 'q' para salir\n",
        "        break\n",
        "\n",
        "# Libera el recurso del video\n",
        "cap.release()\n",
        "\n",
        "# Cierra todas las ventanas de OpenCV\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detección en Webcam - Versión Básica \n",
        "\n",
        "Ahora vamos a usar la webcam para hacer detección en tiempo real.\n",
        "\n",
        "### Importante:\n",
        "- Asegúrate de tener una webcam conectada\n",
        "- La primera ejecución puede tardar unos segundos\n",
        "- Presiona **'q'** para detener la detección\n",
        "\n",
        "### Cómo funciona:\n",
        "1. Se captura cada frame de la webcam\n",
        "2. YOLO procesa el frame y detecta objetos\n",
        "3. Se dibujan las cajas y etiquetas\n",
        "4. Se muestra el frame procesado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 480x640 (no detections), 83.8ms\n",
            "Speed: 7.6ms preprocess, 83.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 95.4ms\n",
            "Speed: 1.0ms preprocess, 95.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 87.4ms\n",
            "Speed: 1.2ms preprocess, 87.4ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 90.5ms\n",
            "Speed: 1.4ms preprocess, 90.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 89.2ms\n",
            "Speed: 1.0ms preprocess, 89.2ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 93.2ms\n",
            "Speed: 0.9ms preprocess, 93.2ms inference, 0.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 91.7ms\n",
            "Speed: 1.3ms preprocess, 91.7ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 95.8ms\n",
            "Speed: 1.4ms preprocess, 95.8ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 89.5ms\n",
            "Speed: 0.9ms preprocess, 89.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 93.4ms\n",
            "Speed: 1.0ms preprocess, 93.4ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 (no detections), 88.5ms\n",
            "Speed: 1.6ms preprocess, 88.5ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 bench, 1 chair, 91.7ms\n",
            "Speed: 1.7ms preprocess, 91.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 chair, 87.9ms\n",
            "Speed: 1.0ms preprocess, 87.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 2 chairs, 81.8ms\n",
            "Speed: 1.2ms preprocess, 81.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 5 chairs, 82.5ms\n",
            "Speed: 0.9ms preprocess, 82.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 5 chairs, 1 tv, 1 laptop, 83.2ms\n",
            "Speed: 0.9ms preprocess, 83.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 85.9ms\n",
            "Speed: 1.4ms preprocess, 85.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 5 chairs, 83.0ms\n",
            "Speed: 1.1ms preprocess, 83.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 6 chairs, 85.9ms\n",
            "Speed: 1.3ms preprocess, 85.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 89.1ms\n",
            "Speed: 1.0ms preprocess, 89.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 87.3ms\n",
            "Speed: 1.2ms preprocess, 87.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 1 tv, 93.3ms\n",
            "Speed: 1.2ms preprocess, 93.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 5 chairs, 88.6ms\n",
            "Speed: 0.8ms preprocess, 88.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 3 chairs, 1 laptop, 84.8ms\n",
            "Speed: 0.9ms preprocess, 84.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 89.5ms\n",
            "Speed: 1.1ms preprocess, 89.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 1 laptop, 86.2ms\n",
            "Speed: 0.9ms preprocess, 86.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 4 chairs, 84.5ms\n",
            "Speed: 0.9ms preprocess, 84.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 5 chairs, 83.6ms\n",
            "Speed: 0.9ms preprocess, 83.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 88.8ms\n",
            "Speed: 1.4ms preprocess, 88.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 4 chairs, 1 laptop, 82.3ms\n",
            "Speed: 0.9ms preprocess, 82.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 5 chairs, 91.2ms\n",
            "Speed: 0.8ms preprocess, 91.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 5 chairs, 82.0ms\n",
            "Speed: 0.8ms preprocess, 82.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 82.6ms\n",
            "Speed: 1.2ms preprocess, 82.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 3 chairs, 79.9ms\n",
            "Speed: 0.9ms preprocess, 79.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 4 chairs, 1 laptop, 78.6ms\n",
            "Speed: 0.9ms preprocess, 78.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 79.7ms\n",
            "Speed: 1.0ms preprocess, 79.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 5 chairs, 1 tv, 80.5ms\n",
            "Speed: 1.2ms preprocess, 80.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 4 chairs, 80.9ms\n",
            "Speed: 1.0ms preprocess, 80.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 3 chairs, 1 laptop, 83.5ms\n",
            "Speed: 0.9ms preprocess, 83.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 3 chairs, 1 tv, 81.4ms\n",
            "Speed: 1.0ms preprocess, 81.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 3 chairs, 82.7ms\n",
            "Speed: 0.8ms preprocess, 82.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 4 chairs, 80.6ms\n",
            "Speed: 1.0ms preprocess, 80.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 81.3ms\n",
            "Speed: 0.8ms preprocess, 81.3ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 4 chairs, 85.9ms\n",
            "Speed: 0.8ms preprocess, 85.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 backpacks, 4 chairs, 80.6ms\n",
            "Speed: 0.9ms preprocess, 80.6ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 backpacks, 1 handbag, 3 chairs, 77.2ms\n",
            "Speed: 0.8ms preprocess, 77.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 86.1ms\n",
            "Speed: 1.0ms preprocess, 86.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 5 chairs, 1 laptop, 81.8ms\n",
            "Speed: 0.9ms preprocess, 81.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 3 chairs, 1 laptop, 84.3ms\n",
            "Speed: 0.8ms preprocess, 84.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 4 chairs, 80.6ms\n",
            "Speed: 0.9ms preprocess, 80.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 4 chairs, 1 tv, 1 laptop, 77.0ms\n",
            "Speed: 1.3ms preprocess, 77.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 4 chairs, 1 tv, 1 laptop, 80.8ms\n",
            "Speed: 0.8ms preprocess, 80.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 backpacks, 1 handbag, 2 chairs, 1 laptop, 82.4ms\n",
            "Speed: 1.4ms preprocess, 82.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 2 chairs, 1 tv, 81.7ms\n",
            "Speed: 1.0ms preprocess, 81.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 3 backpacks, 2 chairs, 1 laptop, 78.3ms\n",
            "Speed: 0.8ms preprocess, 78.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 3 chairs, 1 tv, 2 laptops, 81.4ms\n",
            "Speed: 0.9ms preprocess, 81.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 1 tv, 2 laptops, 77.5ms\n",
            "Speed: 0.8ms preprocess, 77.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 76.2ms\n",
            "Speed: 1.2ms preprocess, 76.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 2 laptops, 78.9ms\n",
            "Speed: 0.9ms preprocess, 78.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 1 laptop, 81.4ms\n",
            "Speed: 1.0ms preprocess, 81.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 3 chairs, 84.1ms\n",
            "Speed: 0.9ms preprocess, 84.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 3 chairs, 1 laptop, 81.1ms\n",
            "Speed: 0.9ms preprocess, 81.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 1 laptop, 83.5ms\n",
            "Speed: 1.1ms preprocess, 83.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 2 laptops, 83.5ms\n",
            "Speed: 0.9ms preprocess, 83.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 3 laptops, 88.7ms\n",
            "Speed: 0.9ms preprocess, 88.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 suitcase, 2 chairs, 3 laptops, 82.6ms\n",
            "Speed: 0.8ms preprocess, 82.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 84.4ms\n",
            "Speed: 0.9ms preprocess, 84.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 85.5ms\n",
            "Speed: 1.4ms preprocess, 85.5ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 suitcase, 2 chairs, 3 laptops, 88.2ms\n",
            "Speed: 1.4ms preprocess, 88.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 2 laptops, 85.2ms\n",
            "Speed: 1.2ms preprocess, 85.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 1 tv, 2 laptops, 83.9ms\n",
            "Speed: 0.8ms preprocess, 83.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 1 tv, 2 laptops, 86.2ms\n",
            "Speed: 0.8ms preprocess, 86.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 2 laptops, 85.3ms\n",
            "Speed: 0.9ms preprocess, 85.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 1 tv, 2 laptops, 85.0ms\n",
            "Speed: 0.9ms preprocess, 85.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 77.7ms\n",
            "Speed: 0.8ms preprocess, 77.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 82.9ms\n",
            "Speed: 1.4ms preprocess, 82.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 74.7ms\n",
            "Speed: 0.8ms preprocess, 74.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 77.8ms\n",
            "Speed: 0.9ms preprocess, 77.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 2 laptops, 77.7ms\n",
            "Speed: 1.3ms preprocess, 77.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 85.2ms\n",
            "Speed: 0.9ms preprocess, 85.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 80.2ms\n",
            "Speed: 1.2ms preprocess, 80.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 79.9ms\n",
            "Speed: 0.9ms preprocess, 79.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 78.1ms\n",
            "Speed: 0.9ms preprocess, 78.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 1 tv, 2 laptops, 77.4ms\n",
            "Speed: 1.3ms preprocess, 77.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 1 tv, 2 laptops, 79.5ms\n",
            "Speed: 0.8ms preprocess, 79.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 1 tv, 2 laptops, 77.3ms\n",
            "Speed: 1.1ms preprocess, 77.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 76.7ms\n",
            "Speed: 1.2ms preprocess, 76.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 78.9ms\n",
            "Speed: 0.8ms preprocess, 78.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 77.0ms\n",
            "Speed: 0.9ms preprocess, 77.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 74.3ms\n",
            "Speed: 0.8ms preprocess, 74.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 76.0ms\n",
            "Speed: 0.8ms preprocess, 76.0ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 81.4ms\n",
            "Speed: 0.9ms preprocess, 81.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 2 laptops, 80.4ms\n",
            "Speed: 0.8ms preprocess, 80.4ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 77.3ms\n",
            "Speed: 1.5ms preprocess, 77.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 2 chairs, 2 laptops, 80.8ms\n",
            "Speed: 1.5ms preprocess, 80.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 bottle, 2 chairs, 1 tv, 2 laptops, 81.6ms\n",
            "Speed: 1.0ms preprocess, 81.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 2 laptops, 78.8ms\n",
            "Speed: 0.8ms preprocess, 78.8ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 77.1ms\n",
            "Speed: 1.2ms preprocess, 77.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 3 chairs, 1 tv, 2 laptops, 81.7ms\n",
            "Speed: 1.2ms preprocess, 81.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 suitcase, 2 chairs, 1 tv, 2 laptops, 79.0ms\n",
            "Speed: 1.1ms preprocess, 79.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 handbag, 3 chairs, 1 tv, 79.4ms\n",
            "Speed: 0.9ms preprocess, 79.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 78.8ms\n",
            "Speed: 1.3ms preprocess, 78.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 2 chairs, 73.8ms\n",
            "Speed: 0.9ms preprocess, 73.8ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 1 backpack, 1 suitcase, 2 chairs, 79.2ms\n",
            "Speed: 1.7ms preprocess, 79.2ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 75.3ms\n",
            "Speed: 1.2ms preprocess, 75.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 78.2ms\n",
            "Speed: 1.1ms preprocess, 78.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 72.3ms\n",
            "Speed: 0.8ms preprocess, 72.3ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 77.0ms\n",
            "Speed: 0.9ms preprocess, 77.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 77.3ms\n",
            "Speed: 0.8ms preprocess, 77.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 72.7ms\n",
            "Speed: 0.9ms preprocess, 72.7ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 80.2ms\n",
            "Speed: 1.1ms preprocess, 80.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 78.8ms\n",
            "Speed: 1.2ms preprocess, 78.8ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 78.9ms\n",
            "Speed: 0.8ms preprocess, 78.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 2 chairs, 78.6ms\n",
            "Speed: 0.9ms preprocess, 78.6ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 persons, 2 chairs, 80.0ms\n",
            "Speed: 0.8ms preprocess, 80.0ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 1 backpack, 1 handbag, 2 chairs, 78.2ms\n",
            "Speed: 0.8ms preprocess, 78.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 bottle, 3 chairs, 1 laptop, 83.2ms\n",
            "Speed: 0.8ms preprocess, 83.2ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 bottle, 3 chairs, 84.0ms\n",
            "Speed: 0.8ms preprocess, 84.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 handbag, 1 bottle, 2 chairs, 2 laptops, 80.3ms\n",
            "Speed: 0.8ms preprocess, 80.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 handbag, 1 bottle, 2 chairs, 1 laptop, 79.2ms\n",
            "Speed: 0.9ms preprocess, 79.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 person, 2 backpacks, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 79.6ms\n",
            "Speed: 0.9ms preprocess, 79.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 80.8ms\n",
            "Speed: 0.9ms preprocess, 80.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 73.4ms\n",
            "Speed: 0.8ms preprocess, 73.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 4 chairs, 2 laptops, 79.4ms\n",
            "Speed: 1.0ms preprocess, 79.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 78.9ms\n",
            "Speed: 0.9ms preprocess, 78.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 77.7ms\n",
            "Speed: 0.8ms preprocess, 77.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 79.4ms\n",
            "Speed: 0.8ms preprocess, 79.4ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 78.1ms\n",
            "Speed: 0.8ms preprocess, 78.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 4 chairs, 2 laptops, 74.0ms\n",
            "Speed: 1.2ms preprocess, 74.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 78.0ms\n",
            "Speed: 0.9ms preprocess, 78.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 4 chairs, 2 laptops, 72.1ms\n",
            "Speed: 1.3ms preprocess, 72.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 79.6ms\n",
            "Speed: 0.8ms preprocess, 79.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 71.5ms\n",
            "Speed: 0.8ms preprocess, 71.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 4 chairs, 2 laptops, 79.4ms\n",
            "Speed: 0.9ms preprocess, 79.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 78.7ms\n",
            "Speed: 0.9ms preprocess, 78.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 3 chairs, 1 laptop, 83.3ms\n",
            "Speed: 0.9ms preprocess, 83.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 78.9ms\n",
            "Speed: 0.9ms preprocess, 78.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 4 chairs, 2 laptops, 77.3ms\n",
            "Speed: 1.0ms preprocess, 77.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 73.7ms\n",
            "Speed: 0.9ms preprocess, 73.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 75.9ms\n",
            "Speed: 1.3ms preprocess, 75.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 4 chairs, 1 laptop, 72.8ms\n",
            "Speed: 1.2ms preprocess, 72.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 76.1ms\n",
            "Speed: 0.9ms preprocess, 76.1ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 3 laptops, 80.5ms\n",
            "Speed: 0.9ms preprocess, 80.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 77.9ms\n",
            "Speed: 0.9ms preprocess, 77.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 79.7ms\n",
            "Speed: 0.8ms preprocess, 79.7ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 75.1ms\n",
            "Speed: 0.8ms preprocess, 75.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 72.6ms\n",
            "Speed: 0.9ms preprocess, 72.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 73.7ms\n",
            "Speed: 0.8ms preprocess, 73.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 79.9ms\n",
            "Speed: 0.9ms preprocess, 79.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 78.0ms\n",
            "Speed: 0.8ms preprocess, 78.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 75.8ms\n",
            "Speed: 1.0ms preprocess, 75.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 76.7ms\n",
            "Speed: 0.9ms preprocess, 76.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 78.5ms\n",
            "Speed: 0.9ms preprocess, 78.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 78.1ms\n",
            "Speed: 1.4ms preprocess, 78.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 76.6ms\n",
            "Speed: 0.8ms preprocess, 76.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 78.6ms\n",
            "Speed: 0.8ms preprocess, 78.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 75.9ms\n",
            "Speed: 0.8ms preprocess, 75.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 81.9ms\n",
            "Speed: 1.2ms preprocess, 81.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 3 laptops, 82.5ms\n",
            "Speed: 0.9ms preprocess, 82.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 76.1ms\n",
            "Speed: 0.9ms preprocess, 76.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 73.8ms\n",
            "Speed: 1.0ms preprocess, 73.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.5ms\n",
            "Speed: 0.9ms preprocess, 80.5ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 74.7ms\n",
            "Speed: 1.0ms preprocess, 74.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 77.6ms\n",
            "Speed: 1.1ms preprocess, 77.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 73.4ms\n",
            "Speed: 0.8ms preprocess, 73.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 72.6ms\n",
            "Speed: 0.8ms preprocess, 72.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 76.5ms\n",
            "Speed: 0.9ms preprocess, 76.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 78.6ms\n",
            "Speed: 0.8ms preprocess, 78.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 76.3ms\n",
            "Speed: 0.8ms preprocess, 76.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 77.3ms\n",
            "Speed: 1.3ms preprocess, 77.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 3 laptops, 74.5ms\n",
            "Speed: 0.9ms preprocess, 74.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 77.2ms\n",
            "Speed: 0.8ms preprocess, 77.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 79.0ms\n",
            "Speed: 0.9ms preprocess, 79.0ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 78.2ms\n",
            "Speed: 0.9ms preprocess, 78.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 75.9ms\n",
            "Speed: 1.0ms preprocess, 75.9ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 78.6ms\n",
            "Speed: 0.9ms preprocess, 78.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 78.7ms\n",
            "Speed: 0.8ms preprocess, 78.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 3 laptops, 82.1ms\n",
            "Speed: 0.9ms preprocess, 82.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 77.5ms\n",
            "Speed: 0.8ms preprocess, 77.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.4ms\n",
            "Speed: 1.1ms preprocess, 80.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 81.2ms\n",
            "Speed: 0.8ms preprocess, 81.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 77.6ms\n",
            "Speed: 1.1ms preprocess, 77.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 80.6ms\n",
            "Speed: 0.8ms preprocess, 80.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 77.9ms\n",
            "Speed: 0.9ms preprocess, 77.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 75.4ms\n",
            "Speed: 1.2ms preprocess, 75.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 75.3ms\n",
            "Speed: 0.9ms preprocess, 75.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.4ms\n",
            "Speed: 0.9ms preprocess, 80.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 82.1ms\n",
            "Speed: 0.8ms preprocess, 82.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 81.3ms\n",
            "Speed: 0.9ms preprocess, 81.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 84.5ms\n",
            "Speed: 0.9ms preprocess, 84.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 77.2ms\n",
            "Speed: 0.8ms preprocess, 77.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 75.5ms\n",
            "Speed: 1.2ms preprocess, 75.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 75.9ms\n",
            "Speed: 0.8ms preprocess, 75.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 74.7ms\n",
            "Speed: 0.8ms preprocess, 74.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 85.1ms\n",
            "Speed: 0.8ms preprocess, 85.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 77.5ms\n",
            "Speed: 1.2ms preprocess, 77.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 77.5ms\n",
            "Speed: 1.3ms preprocess, 77.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 82.8ms\n",
            "Speed: 1.1ms preprocess, 82.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 78.9ms\n",
            "Speed: 0.9ms preprocess, 78.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 85.7ms\n",
            "Speed: 1.2ms preprocess, 85.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 78.2ms\n",
            "Speed: 0.8ms preprocess, 78.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.6ms\n",
            "Speed: 0.9ms preprocess, 80.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 77.4ms\n",
            "Speed: 0.9ms preprocess, 77.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 78.7ms\n",
            "Speed: 0.9ms preprocess, 78.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 71.9ms\n",
            "Speed: 0.8ms preprocess, 71.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 79.5ms\n",
            "Speed: 0.9ms preprocess, 79.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 78.9ms\n",
            "Speed: 0.8ms preprocess, 78.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 87.7ms\n",
            "Speed: 1.3ms preprocess, 87.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.0ms\n",
            "Speed: 0.9ms preprocess, 80.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 85.3ms\n",
            "Speed: 1.0ms preprocess, 85.3ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 86.4ms\n",
            "Speed: 0.9ms preprocess, 86.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 86.9ms\n",
            "Speed: 1.1ms preprocess, 86.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 82.1ms\n",
            "Speed: 0.8ms preprocess, 82.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 81.3ms\n",
            "Speed: 0.9ms preprocess, 81.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 86.7ms\n",
            "Speed: 0.8ms preprocess, 86.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 79.4ms\n",
            "Speed: 0.9ms preprocess, 79.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 83.7ms\n",
            "Speed: 0.9ms preprocess, 83.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 77.9ms\n",
            "Speed: 0.8ms preprocess, 77.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 82.9ms\n",
            "Speed: 0.9ms preprocess, 82.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 86.3ms\n",
            "Speed: 1.0ms preprocess, 86.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.2ms\n",
            "Speed: 0.8ms preprocess, 80.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 88.1ms\n",
            "Speed: 0.9ms preprocess, 88.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.4ms\n",
            "Speed: 0.9ms preprocess, 80.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 81.9ms\n",
            "Speed: 1.4ms preprocess, 81.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 79.9ms\n",
            "Speed: 1.2ms preprocess, 79.9ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 83.5ms\n",
            "Speed: 1.3ms preprocess, 83.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 74.5ms\n",
            "Speed: 1.3ms preprocess, 74.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 79.6ms\n",
            "Speed: 0.9ms preprocess, 79.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 80.0ms\n",
            "Speed: 0.9ms preprocess, 80.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 79.9ms\n",
            "Speed: 0.8ms preprocess, 79.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 75.6ms\n",
            "Speed: 0.9ms preprocess, 75.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 78.9ms\n",
            "Speed: 0.9ms preprocess, 78.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 78.7ms\n",
            "Speed: 0.9ms preprocess, 78.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.0ms\n",
            "Speed: 1.0ms preprocess, 80.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.4ms\n",
            "Speed: 0.8ms preprocess, 80.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 77.9ms\n",
            "Speed: 0.8ms preprocess, 77.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 80.8ms\n",
            "Speed: 0.9ms preprocess, 80.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 72.2ms\n",
            "Speed: 0.9ms preprocess, 72.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 81.3ms\n",
            "Speed: 0.8ms preprocess, 81.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 81.0ms\n",
            "Speed: 1.2ms preprocess, 81.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 86.1ms\n",
            "Speed: 1.4ms preprocess, 86.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.3ms\n",
            "Speed: 0.9ms preprocess, 80.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 86.3ms\n",
            "Speed: 1.0ms preprocess, 86.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 84.9ms\n",
            "Speed: 1.3ms preprocess, 84.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 86.4ms\n",
            "Speed: 1.0ms preprocess, 86.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 97.9ms\n",
            "Speed: 1.2ms preprocess, 97.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 106.7ms\n",
            "Speed: 1.2ms preprocess, 106.7ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 110.2ms\n",
            "Speed: 1.4ms preprocess, 110.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 113.3ms\n",
            "Speed: 1.2ms preprocess, 113.3ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 105.6ms\n",
            "Speed: 1.0ms preprocess, 105.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 105.0ms\n",
            "Speed: 1.1ms preprocess, 105.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 100.3ms\n",
            "Speed: 0.9ms preprocess, 100.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 97.2ms\n",
            "Speed: 1.2ms preprocess, 97.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 99.5ms\n",
            "Speed: 2.1ms preprocess, 99.5ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 100.4ms\n",
            "Speed: 1.5ms preprocess, 100.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 101.7ms\n",
            "Speed: 1.7ms preprocess, 101.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 93.8ms\n",
            "Speed: 1.2ms preprocess, 93.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 95.6ms\n",
            "Speed: 1.7ms preprocess, 95.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 97.6ms\n",
            "Speed: 1.0ms preprocess, 97.6ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 7 chairs, 3 laptops, 89.7ms\n",
            "Speed: 1.3ms preprocess, 89.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 93.4ms\n",
            "Speed: 1.1ms preprocess, 93.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 92.0ms\n",
            "Speed: 1.3ms preprocess, 92.0ms inference, 3.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 98.8ms\n",
            "Speed: 1.2ms preprocess, 98.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 94.6ms\n",
            "Speed: 1.0ms preprocess, 94.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 101.7ms\n",
            "Speed: 1.1ms preprocess, 101.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 98.2ms\n",
            "Speed: 1.6ms preprocess, 98.2ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 93.8ms\n",
            "Speed: 0.8ms preprocess, 93.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 94.6ms\n",
            "Speed: 1.6ms preprocess, 94.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 94.0ms\n",
            "Speed: 1.4ms preprocess, 94.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 98.3ms\n",
            "Speed: 1.2ms preprocess, 98.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 99.3ms\n",
            "Speed: 1.3ms preprocess, 99.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 90.8ms\n",
            "Speed: 1.4ms preprocess, 90.8ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 96.3ms\n",
            "Speed: 1.0ms preprocess, 96.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 86.8ms\n",
            "Speed: 1.5ms preprocess, 86.8ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 91.1ms\n",
            "Speed: 1.1ms preprocess, 91.1ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 85.1ms\n",
            "Speed: 1.0ms preprocess, 85.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 95.5ms\n",
            "Speed: 1.5ms preprocess, 95.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 93.8ms\n",
            "Speed: 1.4ms preprocess, 93.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 91.0ms\n",
            "Speed: 1.3ms preprocess, 91.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 93.7ms\n",
            "Speed: 1.3ms preprocess, 93.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 91.5ms\n",
            "Speed: 1.4ms preprocess, 91.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 89.5ms\n",
            "Speed: 0.9ms preprocess, 89.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 94.3ms\n",
            "Speed: 1.3ms preprocess, 94.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 97.3ms\n",
            "Speed: 0.8ms preprocess, 97.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 99.1ms\n",
            "Speed: 1.4ms preprocess, 99.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 91.2ms\n",
            "Speed: 1.4ms preprocess, 91.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 85.3ms\n",
            "Speed: 0.9ms preprocess, 85.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 92.3ms\n",
            "Speed: 1.3ms preprocess, 92.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 87.0ms\n",
            "Speed: 1.6ms preprocess, 87.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 83.6ms\n",
            "Speed: 1.3ms preprocess, 83.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 84.8ms\n",
            "Speed: 2.0ms preprocess, 84.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 83.8ms\n",
            "Speed: 1.2ms preprocess, 83.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 79.1ms\n",
            "Speed: 1.1ms preprocess, 79.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 84.0ms\n",
            "Speed: 0.9ms preprocess, 84.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 83.8ms\n",
            "Speed: 0.8ms preprocess, 83.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 97.8ms\n",
            "Speed: 1.7ms preprocess, 97.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 85.1ms\n",
            "Speed: 0.8ms preprocess, 85.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 83.2ms\n",
            "Speed: 0.8ms preprocess, 83.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 87.5ms\n",
            "Speed: 0.9ms preprocess, 87.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 87.4ms\n",
            "Speed: 0.9ms preprocess, 87.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 82.3ms\n",
            "Speed: 1.1ms preprocess, 82.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 81.3ms\n",
            "Speed: 6.4ms preprocess, 81.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 1 laptop, 83.3ms\n",
            "Speed: 1.4ms preprocess, 83.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 87.3ms\n",
            "Speed: 1.0ms preprocess, 87.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 84.8ms\n",
            "Speed: 0.9ms preprocess, 84.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 78.6ms\n",
            "Speed: 1.0ms preprocess, 78.6ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 82.5ms\n",
            "Speed: 0.9ms preprocess, 82.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 81.6ms\n",
            "Speed: 0.8ms preprocess, 81.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.7ms\n",
            "Speed: 1.1ms preprocess, 80.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 80.2ms\n",
            "Speed: 0.9ms preprocess, 80.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 84.0ms\n",
            "Speed: 1.2ms preprocess, 84.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 83.1ms\n",
            "Speed: 1.2ms preprocess, 83.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 84.2ms\n",
            "Speed: 1.0ms preprocess, 84.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 1 laptop, 77.2ms\n",
            "Speed: 1.0ms preprocess, 77.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 3 laptops, 75.7ms\n",
            "Speed: 1.1ms preprocess, 75.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 78.0ms\n",
            "Speed: 0.9ms preprocess, 78.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 7 chairs, 3 laptops, 78.2ms\n",
            "Speed: 0.8ms preprocess, 78.2ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 77.6ms\n",
            "Speed: 0.8ms preprocess, 77.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 5 chairs, 2 laptops, 78.4ms\n",
            "Speed: 0.9ms preprocess, 78.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 1 backpack, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 82.4ms\n",
            "Speed: 1.0ms preprocess, 82.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 90.1ms\n",
            "Speed: 0.9ms preprocess, 90.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "0: 480x640 2 backpacks, 1 handbag, 1 bottle, 6 chairs, 2 laptops, 79.1ms\n",
            "Speed: 0.9ms preprocess, 79.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n"
          ]
        }
      ],
      "source": [
        "# Activa la webcam (índice 0 normalmente corresponde a la cámara principal)\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Bucle infinito para capturar fotogramas en tiempo real\n",
        "while True:\n",
        "\n",
        "    # Captura un fotograma desde la webcam\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Si no se pudo capturar el fotograma, salir del bucle\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Ejecuta el modelo sobre el fotograma para obtener predicciones\n",
        "    results = modelo(frame)\n",
        "\n",
        "    # Genera un fotograma anotado con las detecciones dibujadas\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Muestra el fotograma procesado en una ventana\n",
        "    cv2.imshow(\"Detección - Webcam\", annotated_frame)\n",
        "\n",
        "    # Si se presiona la tecla 'q', se detiene la visualización\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Presiona 'q' para salir\n",
        "        break\n",
        "\n",
        "# Libera la cámara\n",
        "cap.release()\n",
        "\n",
        "# Cierra todas las ventanas abiertas por OpenCV\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Entendiendo los Resultados de YOLO\n",
        "\n",
        "Cada detección de YOLO contiene:\n",
        "\n",
        "1. **Bounding Box**: Coordenadas (x1, y1, x2, y2) del rectángulo\n",
        "2. **Clase**: El tipo de objeto detectado\n",
        "3. **Confianza (Confidence)**: Probabilidad de que la detección sea correcta (0-1)\n",
        "\n",
        "### Parámetros importantes:\n",
        "\n",
        "- **conf (confidence threshold)**: Umbral de confianza mínimo\n",
        "  - Valores bajos (0.1-0.3): Más detecciones, pero más falsos positivos\n",
        "  - Valores altos (0.5-0.9): Menos detecciones, pero más precisas\n",
        "  \n",
        "- **iou (Intersection over Union)**: Para eliminar cajas duplicadas\n",
        "  - Valores típicos: 0.45 - 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función auxiliar para procesar y mostrar resultados de forma personalizada\n",
        "\n",
        "def procesar_detecciones(resultados, imagen_original):\n",
        "    \"\"\"\n",
        "    Procesa los resultados de YOLO y dibuja las detecciones de forma personalizada\n",
        "    \n",
        "    Args:\n",
        "        resultados: Resultados de modelo.predict()\n",
        "        imagen_original: Imagen original en formato BGR\n",
        "    \n",
        "    Returns:\n",
        "        imagen_anotada: Imagen con las detecciones dibujadas\n",
        "        info_detecciones: Lista con información de cada detección\n",
        "    \"\"\"\n",
        "    # Crear una copia de la imagen para dibujar\n",
        "    imagen_anotada = imagen_original.copy()\n",
        "    info_detecciones = []\n",
        "    \n",
        "    resultado = resultados[0]\n",
        "    \n",
        "    # Procesar cada detección\n",
        "    for box in resultado.boxes:\n",
        "        # Extraer información\n",
        "        clase_id = int(box.cls[0])\n",
        "        clase_nombre = nombres_clases[clase_id]\n",
        "        confianza = float(box.conf[0])\n",
        "        coordenadas = box.xyxy[0].cpu().numpy().astype(int)\n",
        "        x1, y1, x2, y2 = coordenadas\n",
        "        \n",
        "        # Guardar información\n",
        "        info_detecciones.append({\n",
        "            'clase': clase_nombre,\n",
        "            'confianza': confianza,\n",
        "            'bbox': (x1, y1, x2, y2)\n",
        "        })\n",
        "        \n",
        "        # Colores diferentes según la clase (para visualización)\n",
        "        color = (\n",
        "            int(np.random.randint(0, 255)),\n",
        "            int(np.random.randint(0, 255)),\n",
        "            int(np.random.randint(0, 255))\n",
        "        )\n",
        "        \n",
        "        # Dibujar el rectángulo\n",
        "        cv2.rectangle(imagen_anotada, (x1, y1), (x2, y2), color, 2)\n",
        "        \n",
        "        # Preparar el texto\n",
        "        texto = f\"{clase_nombre} {confianza*100:.1f}%\"\n",
        "        \n",
        "        # Calcular el tamaño del texto para el fondo\n",
        "        (ancho_texto, alto_texto), baseline = cv2.getTextSize(\n",
        "            texto, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2\n",
        "        )\n",
        "        \n",
        "        # Dibujar rectángulo de fondo para el texto\n",
        "        cv2.rectangle(\n",
        "            imagen_anotada,\n",
        "            (x1, y1 - alto_texto - baseline - 5),\n",
        "            (x1 + ancho_texto, y1),\n",
        "            color,\n",
        "            -1\n",
        "        )\n",
        "        \n",
        "        # Dibujar el texto\n",
        "        cv2.putText(\n",
        "            imagen_anotada,\n",
        "            texto,\n",
        "            (x1, y1 - 5),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.5,\n",
        "            (255, 255, 255),\n",
        "            2\n",
        "        )\n",
        "    \n",
        "    return imagen_anotada, info_detecciones\n",
        "\n",
        "print(\"✓ Función de procesamiento personalizado creada\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Aplicación Práctica: Contador de Personas\n",
        "\n",
        "Vamos a crear una aplicación específica que cuenta personas en tiempo real.\n",
        "Útil para:\n",
        "- 🏪 Conteo de clientes en tiendas\n",
        "- 🏢 Control de aforo\n",
        "- 📊 Análisis de tráfico peatonal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contador_personas_tiempo_real(duracion_segundos=30, conf_threshold=0.6):\n",
        "    \"\"\"\n",
        "    Aplicación específica para contar personas en tiempo real\n",
        "    \n",
        "    Args:\n",
        "        duracion_segundos: Duración de la captura\n",
        "        conf_threshold: Umbral de confianza (recomendado: 0.6 para personas)\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"CONTADOR DE PERSONAS EN TIEMPO REAL\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Duración: {duracion_segundos} segundos\")\n",
        "    print(f\"Umbral de confianza: {conf_threshold}\")\n",
        "    print(\"\\nPresiona 'q' para salir\\n\")\n",
        "    \n",
        "    cap = cv2.VideoCapture(0)\n",
        "    \n",
        "    if not cap.isOpened():\n",
        "        print(\"❌ Error: No se pudo abrir la webcam\")\n",
        "        return\n",
        "    \n",
        "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
        "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "    \n",
        "    ancho = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    alto = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    \n",
        "    print(\"✓ Sistema iniciado\")\n",
        "    print(\"Detectando personas...\\n\")\n",
        "    \n",
        "    # Estadísticas\n",
        "    tiempo_inicio = time.time()\n",
        "    personas_maximas = 0\n",
        "    historial_personas = []\n",
        "    total_detecciones = 0\n",
        "    \n",
        "    try:\n",
        "        while True:\n",
        "            tiempo_transcurrido = time.time() - tiempo_inicio\n",
        "            if tiempo_transcurrido > duracion_segundos:\n",
        "                break\n",
        "            \n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            \n",
        "            # Detectar\n",
        "            resultados = modelo.predict(\n",
        "                source=frame,\n",
        "                conf=conf_threshold,\n",
        "                classes=[0],  # Clase 0 = person\n",
        "                verbose=False\n",
        "            )\n",
        "            \n",
        "            resultado = resultados[0]\n",
        "            frame_anotado = frame.copy()\n",
        "            \n",
        "            # Contar personas\n",
        "            num_personas = len(resultado.boxes)\n",
        "            total_detecciones += num_personas\n",
        "            historial_personas.append(num_personas)\n",
        "            \n",
        "            if num_personas > personas_maximas:\n",
        "                personas_maximas = num_personas\n",
        "            \n",
        "            # Dibujar cada persona\n",
        "            for i, box in enumerate(resultado.boxes):\n",
        "                coordenadas = box.xyxy[0].cpu().numpy().astype(int)\n",
        "                x1, y1, x2, y2 = coordenadas\n",
        "                confianza = float(box.conf[0])\n",
        "                \n",
        "                # Color según la confianza\n",
        "                if confianza > 0.8:\n",
        "                    color = (0, 255, 0)  # Verde - alta confianza\n",
        "                elif confianza > 0.6:\n",
        "                    color = (0, 255, 255)  # Amarillo - media confianza\n",
        "                else:\n",
        "                    color = (0, 165, 255)  # Naranja - baja confianza\n",
        "                \n",
        "                # Dibujar rectángulo\n",
        "                cv2.rectangle(frame_anotado, (x1, y1), (x2, y2), color, 3)\n",
        "                \n",
        "                # Etiqueta\n",
        "                texto = f\"Persona {i+1} ({confianza*100:.0f}%)\"\n",
        "                (w_text, h_text), _ = cv2.getTextSize(\n",
        "                    texto, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2\n",
        "                )\n",
        "                cv2.rectangle(frame_anotado, (x1, y1-h_text-10), \n",
        "                            (x1+w_text, y1), color, -1)\n",
        "                cv2.putText(frame_anotado, texto, (x1, y1-5),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "            \n",
        "            # Panel de información grande\n",
        "            panel_ancho = 300\n",
        "            panel_alto = 200\n",
        "            overlay = frame_anotado.copy()\n",
        "            cv2.rectangle(overlay, (ancho-panel_ancho, 0), \n",
        "                         (ancho, panel_alto), (0, 0, 0), -1)\n",
        "            frame_anotado = cv2.addWeighted(overlay, 0.7, frame_anotado, 0.3, 0)\n",
        "            \n",
        "            # Información del contador\n",
        "            y = 40\n",
        "            cv2.putText(frame_anotado, \"CONTADOR\", (ancho-panel_ancho+10, y),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "            \n",
        "            y += 50\n",
        "            # Número actual de personas (grande)\n",
        "            texto_personas = f\"{num_personas}\"\n",
        "            cv2.putText(frame_anotado, texto_personas, (ancho-panel_ancho+80, y),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 2.5, (0, 255, 0), 4)\n",
        "            \n",
        "            y += 50\n",
        "            cv2.putText(frame_anotado, \"Personas ahora\", (ancho-panel_ancho+40, y),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n",
        "            \n",
        "            y += 35\n",
        "            cv2.putText(frame_anotado, f\"Maximo: {personas_maximas}\", \n",
        "                       (ancho-panel_ancho+10, y),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1)\n",
        "            \n",
        "            y += 25\n",
        "            promedio = np.mean(historial_personas) if historial_personas else 0\n",
        "            cv2.putText(frame_anotado, f\"Promedio: {promedio:.1f}\", \n",
        "                       (ancho-panel_ancho+10, y),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 1)\n",
        "            \n",
        "            # Tiempo restante\n",
        "            tiempo_restante = duracion_segundos - tiempo_transcurrido\n",
        "            cv2.putText(frame_anotado, f\"Tiempo: {tiempo_restante:.0f}s\", \n",
        "                       (10, alto-10),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "            \n",
        "            # Mostrar\n",
        "            cv2.imshow('Contador de Personas - YOLOv11', frame_anotado)\n",
        "            \n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "    \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nInterrumpido\")\n",
        "    \n",
        "    finally:\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        \n",
        "        # Reporte final\n",
        "        tiempo_total = time.time() - tiempo_inicio\n",
        "        promedio_final = np.mean(historial_personas) if historial_personas else 0\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"REPORTE FINAL - CONTADOR DE PERSONAS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Duración del monitoreo: {tiempo_total:.2f} segundos\")\n",
        "        print(f\"Personas detectadas (máximo simultáneo): {personas_maximas}\")\n",
        "        print(f\"Promedio de personas: {promedio_final:.2f}\")\n",
        "        print(f\"Total de detecciones: {total_detecciones}\")\n",
        "        print(f\"Frames analizados: {len(historial_personas)}\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "        # Gráfico del historial\n",
        "        if historial_personas:\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.plot(historial_personas, linewidth=2, color='blue', label='Personas')\n",
        "            plt.axhline(y=promedio_final, color='red', linestyle='--', \n",
        "                       label=f'Promedio ({promedio_final:.2f})')\n",
        "            plt.axhline(y=personas_maximas, color='green', linestyle='--', \n",
        "                       label=f'Máximo ({personas_maximas})')\n",
        "            plt.xlabel('Frame')\n",
        "            plt.ylabel('Número de personas')\n",
        "            plt.title('Historial de Detección de Personas')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# Ejecutar el contador\n",
        "contador_personas_tiempo_real(duracion_segundos=30, conf_threshold=0.6)\n",
        "\n",
        "print(\"✓ Aplicación de contador de personas creada\")\n",
        "print(\"\\nPara ejecutar, descomenta la última línea\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Proyecto: Detector de Objetos Específicos\n",
        "\n",
        "Crea tu propio detector personalizado.\n",
        "para este ejemplo vamos a contar coches "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa la librería OpenCV para procesamiento de imágenes y video\n",
        "import cv2\n",
        "\n",
        "# Importa las soluciones de Ultralytics (como ObjectCounter)\n",
        "from ultralytics import solutions\n",
        "\n",
        "# Abre el archivo de video indicado\n",
        "cap = cv2.VideoCapture('./video/coches.mp4')\n",
        "\n",
        "# Verifica que el video se haya abierto correctamente\n",
        "assert cap.isOpened(), \"Error reading video file\"\n",
        "\n",
        "# cap = cv2.VideoCapture(0)  # Descomenta esta línea si deseas usar la cámara en lugar de un archivo de video\n",
        "\n",
        "# Define los puntos que forman la región donde se realizará el conteo\n",
        "# En este caso, una línea formada por dos puntos\n",
        "region_points = [(218, 264), (554, 222)]\n",
        "\n",
        "# Ejemplos alternativos para usar una región rectangular o poligonal:\n",
        "# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]  # rectángulo\n",
        "# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]  # polígono\n",
        "\n",
        "# Obtener propiedades del video: ancho, alto y FPS\n",
        "w, h, fps = (int(cap.get(x)) for x in (\n",
        "    cv2.CAP_PROP_FRAME_WIDTH,\n",
        "    cv2.CAP_PROP_FRAME_HEIGHT,\n",
        "    cv2.CAP_PROP_FPS\n",
        "))\n",
        "\n",
        "# Crear un escritor de video para guardar el resultado procesado\n",
        "video_writer = cv2.VideoWriter(\n",
        "    \"object_counting_output.avi\",\n",
        "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "    fps,\n",
        "    (w, h)\n",
        ")\n",
        "\n",
        "# Inicializar el contador de objetos\n",
        "counter = solutions.ObjectCounter(\n",
        "    show=True,              # Mostrar la salida procesada en pantalla\n",
        "    region=region_points,   # Puntos de la región donde se contarán los objetos\n",
        "    model=\"yolo11n.pt\",     # Modelo de detección (OBB disponible opcionalmente)\n",
        "    # classes=[0, 2],        # (Opcional) Contar solo clases específicas, ej.: persona y coche\n",
        "    # tracker=\"botsort.yaml\" # (Opcional) Elegir un tracker como ByteTrack o BoT-SORT\n",
        ")\n",
        "\n",
        "# Procesar el video cuadro por cuadro\n",
        "while cap.isOpened():\n",
        "\n",
        "    # Leer un fotograma del video\n",
        "    success, im0 = cap.read()\n",
        "\n",
        "    # Si no se obtuvo un fotograma, se terminó el video\n",
        "    if not success:\n",
        "        print(\"Video frame is empty or processing is complete.\")\n",
        "        break\n",
        "\n",
        "    # Procesar el fotograma con el contador (detección + tracking + conteo)\n",
        "    results = counter(im0)\n",
        "\n",
        "    # print(results)  # (Opcional) Ver el resultado detallado en consola\n",
        "\n",
        "    # Guardar el fotograma procesado en el archivo de salida\n",
        "    video_writer.write(results.plot_im)\n",
        "\n",
        "# Liberar el video original\n",
        "cap.release()\n",
        "\n",
        "# Cerrar el archivo de salida\n",
        "video_writer.release()\n",
        "\n",
        "# Cerrar todas las ventanas abiertas por OpenCV\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pero.. ¿como hago la linea?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importa la librería OpenCV para trabajar con imágenes y video\n",
        "import cv2\n",
        "\n",
        "# Ruta del video que se usará para seleccionar puntos\n",
        "video_path = './video/coches.mp4'\n",
        "\n",
        "# Escala para mostrar el video más pequeño en pantalla (útil para pantallas pequeñas)\n",
        "scale = 0.5  # Ajusta a 0.3 o 0.7 según lo necesites\n",
        "\n",
        "# Lista donde se guardarán los puntos seleccionados\n",
        "points = []\n",
        "\n",
        "# Función que se ejecuta cuando el usuario hace clic en la ventana\n",
        "def click_event(event, x, y, flags, params):\n",
        "    if event == cv2.EVENT_LBUTTONDOWN:  # Detecta clic izquierdo\n",
        "        # Convertir coordenadas de la ventana escalada a las coordenadas reales del video\n",
        "        x_orig = int(x / scale)\n",
        "        y_orig = int(y / scale)\n",
        "        points.append((x_orig, y_orig))\n",
        "\n",
        "        # Imprime la coordenada real seleccionada\n",
        "        print(f\"Coordenada original: ({x_orig}, {y_orig})\")\n",
        "\n",
        "        # Si ya hay dos puntos, muestra la línea final\n",
        "        if len(points) == 2:\n",
        "            print(f\"\\nLínea lista: {points}\\n\")\n",
        "\n",
        "# Abre el video para lectura\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Bucle para mostrar el video y permitir seleccionar puntos\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:  # Si no hay más fotogramas, termina\n",
        "        break\n",
        "\n",
        "    # Reduce el tamaño del fotograma según el factor de escala\n",
        "    frame_resized = cv2.resize(frame, (0, 0), fx=scale, fy=scale)\n",
        "\n",
        "    # Copia del frame donde se dibujará la línea\n",
        "    clone = frame_resized.copy()\n",
        "\n",
        "    # Si ya se seleccionaron dos puntos, dibujar la línea en la ventana escalada\n",
        "    if len(points) == 2:\n",
        "        pt1 = (int(points[0][0] * scale), int(points[0][1] * scale))\n",
        "        pt2 = (int(points[1][0] * scale), int(points[1][1] * scale))\n",
        "        cv2.line(clone, pt1, pt2, (0, 255, 0), 2)  # Línea verde\n",
        "\n",
        "    # Mostrar la ventana con el video\n",
        "    cv2.imshow(\"Haz clic en dos puntos para definir la línea\", clone)\n",
        "\n",
        "    # Registrar el callback para capturar clics del usuario\n",
        "    cv2.setMouseCallback(\"Haz clic en dos puntos para definir la línea\", click_event)\n",
        "\n",
        "    # Salir si el usuario presiona 'q' o ya seleccionó 2 puntos\n",
        "    key = cv2.waitKey(20)\n",
        "    if key == ord('q') or len(points) == 2:\n",
        "        break\n",
        "\n",
        "# Liberar el video\n",
        "cap.release()\n",
        "\n",
        "# Cerrar ventanas abiertas\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Mostrar el resultado final en consola\n",
        "if len(points) == 2:\n",
        "    print(f\"Usa esta línea en tu código:\\nregion_points = {points}\")\n",
        "else:\n",
        "    print(\"No seleccionaste dos puntos.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Optimización y Consejos\n",
        "\n",
        "### Para mejorar el rendimiento:\n",
        "\n",
        "1. **Usa el modelo más pequeño necesario**\n",
        "   - YOLOv11n para velocidad\n",
        "   - YOLOv11s o m para balance\n",
        "   - YOLOv11l o x solo si necesitas máxima precisión\n",
        "\n",
        "2. **Reduce la resolución de la webcam**\n",
        "   ```python\n",
        "   cap.set(cv2.CAP_PROP_FRAME_WIDTH, 416)\n",
        "   cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 416)\n",
        "   ```\n",
        "\n",
        "3. **Ajusta el umbral de confianza**\n",
        "   - Valores más altos = menos detecciones pero más precisas\n",
        "   - Experimenta con valores entre 0.3 y 0.7\n",
        "\n",
        "4. **Procesa cada N frames**\n",
        "   - No es necesario detectar en TODOS los frames\n",
        "   - Procesa cada 2 o 3 frames para ahorrar procesamiento\n",
        "\n",
        "5. **Usa GPU si está disponible**\n",
        "   - YOLO detectará automáticamente CUDA si tienes una GPU NVIDIA\n",
        "   - Puede aumentar la velocidad 5-10x\n",
        "\n",
        "### Solución de problemas comunes:\n",
        "\n",
        "| Problema | Solución |\n",
        "|----------|----------|\n",
        "| FPS muy bajos | Usa modelo más pequeño o reduce resolución |\n",
        "| Muchos falsos positivos | Aumenta el umbral de confianza |\n",
        "| No detecta objetos pequeños | Aumenta la resolución o usa modelo más grande |\n",
        "| Webcam no se abre | Verifica que no esté en uso por otra app |\n",
        "| Detecciones inestables | Implementa tracking o suavizado temporal |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Proyecto Final: Sistema Completo de Vigilancia\n",
        "\n",
        "Combina todo lo aprendido en un sistema completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importamos el módulo datetime, que permite manejar fechas y horas (útil para registrar eventos)\n",
        "import datetime\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# FUNCIÓN PRINCIPAL DEL SISTEMA DE VIGILANCIA\n",
        "# --------------------------------------------------------\n",
        "def sistema_vigilancia_completo(\n",
        "    duracion_minutos=5,            # Duración total del sistema antes de apagarse automáticamente\n",
        "    guardar_eventos=True,          # Si TRUE, se guardarán videos cuando haya detecciones\n",
        "    clases_vigilar=['person'],     # Lista de clases a detectar (según el modelo YOLO)\n",
        "    conf_threshold=0.6             # Nivel mínimo de confianza para considerar válida una detección\n",
        "):\n",
        "    \"\"\"\n",
        "    Sistema completo de vigilancia conectado a YOLOv11.\n",
        "\n",
        "    Este sistema realiza:\n",
        "    - Captura continua desde webcam\n",
        "    - Detección en tiempo real\n",
        "    - Dibujo de cuadros y etiquetas sobre la imagen\n",
        "    - Registro automático de eventos con hora\n",
        "    - Grabación cuando aparece un objeto vigilado\n",
        "    - Estadísticas en vivo en la pantalla\n",
        "    - Reporte final de todo lo detectado\n",
        "    \"\"\"\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # MUESTRA INFORMACIÓN INICIAL ANTES DE EMPEZAR\n",
        "    # --------------------------------------------------------\n",
        "    print(\"=\"*70)\n",
        "    print(\"SISTEMA DE VIGILANCIA INTELIGENTE - YOLOv11\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Duración: {duracion_minutos} minutos\")                 # Tiempo total de monitoreo\n",
        "    print(f\"Vigilando: {', '.join(clases_vigilar)}\")               # Clases de objetos que se detectarán\n",
        "    print(f\"Umbral de confianza: {conf_threshold}\")                # Nivel requerido para aceptación\n",
        "    print(\"\\nPresiona 'q' para detener\\n\")\n",
        "    \n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # CONFIGURACIÓN DE LA WEBCAM\n",
        "    # --------------------------------------------------------\n",
        "    cap = cv2.VideoCapture(0)   # 0 = webcam principal\n",
        "    if not cap.isOpened():      # Verifica que la cámara esté disponible\n",
        "        print(\"❌ Error: No se pudo abrir la webcam\")\n",
        "        return\n",
        "    \n",
        "    # Establecemos una resolución estándar (puedes modificar si quieres calidad HD)\n",
        "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)  \n",
        "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
        "\n",
        "    # Guardamos los valores reales (en caso el sistema no acepte la resolución pedida)\n",
        "    ancho = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    alto = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    \n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # VARIABLES PARA CONTROL INTERNO DEL SISTEMA\n",
        "    # --------------------------------------------------------\n",
        "    tiempo_inicio = time.time()                 # Momento en que inicia el sistema\n",
        "    duracion_segundos = duracion_minutos * 60   # Convertimos minutos → segundos\n",
        "\n",
        "    eventos = []                                # Lista para guardar todos los eventos detectados\n",
        "    grabando = False                            # Indica si estamos grabando video o no\n",
        "    writer = None                                # Objeto VideoWriter (cuando se graba)\n",
        "    contador_eventos = 0                         # Cantidad total de eventos detectados\n",
        "\n",
        "    print(\"✓ Sistema de vigilancia activado\")\n",
        "    print(\"Monitoreando...\\n\")\n",
        "    \n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # BUCLE PRINCIPAL DE VIGILANCIA\n",
        "    # --------------------------------------------------------\n",
        "    try:\n",
        "        while True:\n",
        "\n",
        "            # Calculamos cuánto tiempo ha transcurrido desde el inicio\n",
        "            tiempo_actual = time.time()\n",
        "            tiempo_transcurrido = tiempo_actual - tiempo_inicio\n",
        "\n",
        "            # Si ya se cumplió el tiempo total → terminar\n",
        "            if tiempo_transcurrido > duracion_segundos:\n",
        "                break\n",
        "\n",
        "            # Capturamos un fotograma de la webcam\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break   # Si falla la captura, se detiene el sistema\n",
        "\n",
        "            \n",
        "            # --------------------------------------------------------\n",
        "            # PROCESAMOS EL FRAME CON YOLO\n",
        "            # --------------------------------------------------------\n",
        "            resultados = modelo.predict(\n",
        "                source=frame,       # Imagen a procesar\n",
        "                conf=conf_threshold,# Probabilidad mínima\n",
        "                verbose=False       # Oculta logs\n",
        "            )\n",
        "            \n",
        "            resultado = resultados[0]       # YOLO devuelve varias cosas, aquí tomamos el primer frame\n",
        "            frame_anotado = frame.copy()    # Duplicamos el frame para dibujar sobre él\n",
        "\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # ANALIZAMOS LAS DETECCIONES REALIZADAS POR YOLO\n",
        "            # --------------------------------------------------------\n",
        "            objetos_detectados = []         # Lista para este frame\n",
        "\n",
        "            for box in resultado.boxes:     # Recorremos cada detección encontrada\n",
        "                \n",
        "                clase_id = int(box.cls[0])          # ID numérico de la clase detectada\n",
        "                clase_nombre = nombres_clases[clase_id]  # Convertimos ID → nombre (ej. 'person')\n",
        "                \n",
        "                # Solo nos interesa si coincide con la lista de vigilancia\n",
        "                if clase_nombre in clases_vigilar:\n",
        "\n",
        "                    objetos_detectados.append(clase_nombre)\n",
        "\n",
        "                    # Confianza de detección (entre 0 y 1)\n",
        "                    confianza = float(box.conf[0])\n",
        "\n",
        "                    # Coordenadas del cuadro delimitador\n",
        "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
        "                    \n",
        "                    # Dibujar el cuadro ROJO alrededor del objeto\n",
        "                    cv2.rectangle(frame_anotado, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "\n",
        "                    # Dibujar etiqueta con nombre y porcentaje de confianza\n",
        "                    cv2.putText(\n",
        "                        frame_anotado,\n",
        "                        f\"{clase_nombre} {confianza*100:.0f}%\",\n",
        "                        (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                        0.6,\n",
        "                        (0, 0, 255),\n",
        "                        2\n",
        "                    )\n",
        "\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # MANEJO DE EVENTOS (INICIO Y FIN DE GRABACIÓN)\n",
        "            # --------------------------------------------------------\n",
        "\n",
        "            # Si detectamos objetos Y no estamos grabando todavía → iniciamos grabación\n",
        "            if objetos_detectados and not grabando:\n",
        "\n",
        "                contador_eventos += 1   # Aumentamos el número de evento\n",
        "                \n",
        "                timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                nombre_archivo = f\"evento_{contador_eventos}_{timestamp}.mp4\"\n",
        "\n",
        "                # Si guardamos los eventos → abrir archivo de video\n",
        "                if guardar_eventos:\n",
        "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Códec de video\n",
        "                    writer = cv2.VideoWriter(nombre_archivo, fourcc, 20, (ancho, alto))\n",
        "\n",
        "                # Guardamos la información del evento\n",
        "                evento = {\n",
        "                    'numero': contador_eventos,\n",
        "                    'timestamp': datetime.datetime.now(),\n",
        "                    'objetos': list(set(objetos_detectados)),\n",
        "                    'archivo': nombre_archivo if guardar_eventos else None\n",
        "                }\n",
        "                eventos.append(evento)\n",
        "\n",
        "                grabando = True\n",
        "\n",
        "                # Mostrar alerta en consola\n",
        "                print(f\"⚠️  EVENTO {contador_eventos}: {', '.join(set(objetos_detectados))} detectado a las {evento['timestamp'].strftime('%H:%M:%S')}\")\n",
        "\n",
        "\n",
        "            # Si NO hay detecciones y SÍ está grabando → detener grabación\n",
        "            elif not objetos_detectados and grabando:\n",
        "                if writer:\n",
        "                    writer.release()   # Cerramos archivo de video\n",
        "                    writer = None\n",
        "                grabando = False\n",
        "\n",
        "\n",
        "            # Si grabando, guardar el frame actual en el video\n",
        "            if grabando and writer:\n",
        "                writer.write(frame_anotado)\n",
        "\n",
        "\n",
        "            # --------------------------------------------------------\n",
        "            # INTERFAZ GRÁFICA (HUD EN LA PARTE SUPERIOR)\n",
        "            # --------------------------------------------------------\n",
        "\n",
        "            # Creamos fondo oscuro semitransparente\n",
        "            overlay = frame_anotado.copy()\n",
        "            cv2.rectangle(overlay, (0, 0), (ancho, 100), (0, 0, 0), -1)\n",
        "            frame_anotado = cv2.addWeighted(overlay, 0.7, frame_anotado, 0.3, 0)\n",
        "            \n",
        "            # Texto del estado actual (MONITOREANDO / GRABANDO)\n",
        "            estado = \"GRABANDO\" if grabando else \"MONITOREANDO\"\n",
        "            color_estado = (0, 0, 255) if grabando else (0, 255, 0)\n",
        "            cv2.putText(frame_anotado, estado, (10, 35),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 1, color_estado, 2)\n",
        "            \n",
        "            # Hora actual del sistema\n",
        "            hora_actual = datetime.datetime.now().strftime(\"%H:%M:%S\")\n",
        "            cv2.putText(frame_anotado, hora_actual, (10, 70),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            \n",
        "            # Cantidad de eventos detectados hasta el momento\n",
        "            cv2.putText(frame_anotado, f\"Eventos: {contador_eventos}\", (ancho-180, 35),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            \n",
        "            # Número de objetos detectados en este frame\n",
        "            if objetos_detectados:\n",
        "                cv2.putText(frame_anotado, f\"Detectados: {len(objetos_detectados)}\",\n",
        "                            (ancho-180, 70), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
        "            \n",
        "            # Mostrar frame con HUD y anotaciones\n",
        "            cv2.imshow('Sistema de Vigilancia - YOLOv11', frame_anotado)\n",
        "            \n",
        "            # Permitir salir manualmente\n",
        "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "                break\n",
        "    \n",
        "\n",
        "    # Capturamos interrupción manual (Ctrl + C)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSistema detenido manualmente.\")\n",
        "    \n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # LIBERAR RECURSOS + REPORTE FINAL\n",
        "    # --------------------------------------------------------\n",
        "    finally:\n",
        "        if writer:\n",
        "            writer.release()\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        \n",
        "\n",
        "        # ------------------------\n",
        "        # MOSTRAR REPORTE FINAL\n",
        "        # ------------------------\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"REPORTE DE VIGILANCIA\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Duración del monitoreo: {tiempo_transcurrido/60:.2f} minutos\")\n",
        "        print(f\"Total de eventos detectados: {contador_eventos}\")\n",
        "        \n",
        "        # Mostrar detalles de cada evento encontrado\n",
        "        if eventos:\n",
        "            print(\"\\nDetalle de eventos:\")\n",
        "            for evento in eventos:\n",
        "                print(f\"\\n  Evento #{evento['numero']}\")\n",
        "                print(f\"    Hora: {evento['timestamp'].strftime('%H:%M:%S')}\")\n",
        "                print(f\"    Objetos: {', '.join(evento['objetos'])}\")\n",
        "                if evento['archivo']:\n",
        "                    print(f\"    Archivo: {evento['archivo']}\")\n",
        "        else:\n",
        "            print(\"\\nNo se detectaron eventos durante el monitoreo.\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"Monitoreo finalizado\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "\n",
        "\n",
        "# --------------------------------------------------------\n",
        "# EJECUCIÓN DEL SISTEMA\n",
        "# --------------------------------------------------------\n",
        "sistema_vigilancia_completo(\n",
        "     duracion_minutos=5,             # Tiempo de vigilancia\n",
        "     guardar_eventos=True,           # Guardar videos cuando haya detecciones\n",
        "     clases_vigilar=['person'],      # Detectar personas\n",
        "     conf_threshold=0.6              # Nivel de confianza mínimo\n",
        ")\n",
        "\n",
        "print(\"✓ Sistema de vigilancia completo creado\")\n",
        "print(\"\\nPara ejecutar, descomenta el código de ejemplo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiones y Próximos Pasos\n",
        "\n",
        "\n",
        "Has aprendido a:\n",
        "-  Instalar y configurar YOLOv11\n",
        "-  Realizar detección en imágenes estáticas\n",
        "-  Implementar detección en tiempo real con webcam\n",
        "-  Crear aplicaciones personalizadas\n",
        "-  Construir un sistema de vigilancia completo\n",
        "-  Optimizar el rendimiento\n",
        "-  Analizar y visualizar resultados\n",
        "\n",
        "### Proyectos que puedes crear:\n",
        "\n",
        "1. **Sistema de Asistencia Automática**\n",
        "   - Detecta personas en un aula\n",
        "   - Registra asistencia automáticamente\n",
        "\n",
        "2. **Monitor de Distanciamiento Social**\n",
        "   - Detecta personas\n",
        "   - Calcula distancias entre ellas\n",
        "   - Alerta si están muy cerca\n",
        "\n",
        "3. **Contador de Aforo**\n",
        "   - Cuenta personas que entran/salen\n",
        "   - Controla capacidad máxima\n",
        "\n",
        "4. **Sistema Anti-Robo**\n",
        "   - Detecta movimiento\n",
        "   - Envía alertas\n",
        "   - Graba automáticamente\n",
        "\n",
        "5. **Asistente para Personas con Discapacidad Visual**\n",
        "   - Detecta objetos\n",
        "   - Proporciona retroalimentación por voz\n",
        "\n",
        "### Recursos para seguir aprendiendo:\n",
        "\n",
        "-  [Documentación Ultralytics YOLOv11](https://docs.ultralytics.com/)\n",
        "-  [Canal de YouTube Ultralytics](https://www.youtube.com/@Ultralytics)\n",
        "-  [Repositorio GitHub](https://github.com/ultralytics/ultralytics)\n",
        "-  [Roboflow Universe](https://universe.roboflow.com/) - Datasets públicos\n",
        "\n",
        "\n",
        "### Siguientes niveles:\n",
        "\n",
        "1. **Entrenar tu propio modelo**\n",
        "   - Crear tu dataset personalizado\n",
        "   - Fine-tuning de YOLO\n",
        "   - Detectar objetos específicos de tu proyecto\n",
        "\n",
        "2. **Tracking de objetos**\n",
        "   - Seguir objetos a través de frames\n",
        "   - Contar objetos que cruzan líneas\n",
        "   - Análisis de trayectorias\n",
        "\n",
        "3. **Segmentación de instancias**\n",
        "   - YOLOv11 también soporta segmentación\n",
        "   - Obtener máscaras precisas de objetos\n",
        "\n",
        "4. **Despliegue en producción**\n",
        "   - Optimización para edge devices\n",
        "   - Conversión a TensorRT, ONNX\n",
        "   - Implementación en Raspberry Pi, Jetson Nano\n",
        "\n",
        "---\n",
        "\n",
        "**Ahora es tu turno de crear proyectos increíbles con YOLOv11!**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "YOLO",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
