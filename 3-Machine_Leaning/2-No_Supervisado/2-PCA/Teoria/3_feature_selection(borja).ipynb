{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Why would it even be necessary to select features? To some, this idea may seem counterintuitive, but there are at least two important reasons to get rid of unimportant features. The first is clear to every engineer: the more data, the higher the computational complexity. As long as we work with toy datasets, the size of the data is not a problem, but, for real loaded production systems, hundreds of extra features will be quite tangible. The second reason is that some algorithms take noise (non-informative features) as a signal and overfit.\n",
    "\n",
    "### Statistical approaches\n",
    "\n",
    "The most obvious candidate for removal is a feature whose value remains unchanged, i.e., it contains no information at all. If we build on this thought, it is reasonable to say that features with low variance are worse than those with high variance. So, one can consider cutting features with variance below a certain threshold."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basado en estadísticos no supervisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importamos la librería NumPy para operaciones numéricas\nimport numpy as np"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:17.058096Z",
     "start_time": "2020-11-19T10:10:13.843481Z"
    }
   },
   "outputs": [],
   "source": "# Importamos la función make_classification de sklearn para generar un dataset sintético\nfrom sklearn.datasets import make_classification\n\n# Generamos un dataset de clasificación con parámetros por defecto\n# x_data_generated: matriz de características (features)\n# y_data_generated: vector de etiquetas (target)\nx_data_generated, y_data_generated = make_classification(random_state=2)\n\n# Verificamos las dimensiones del dataset generado\n# Resultado: (100 muestras, 20 características)\nx_data_generated.shape"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VarianceThreshold\n",
    "\n",
    "- **¿Qué es?**: Una técnica que elimina características cuya varianza no alcanza un umbral especificado.\n",
    "- **Funcionamiento**: Calcula la varianza de cada característica y elimina aquellas que no superan el umbral establecido. Esto significa que las características que apenas cambian entre muestras (es decir, tienen baja varianza) se descartarán.\n",
    "- **Ventajas**: Rápida y fácil de usar, útil cuando las características con baja varianza no son informativas.\n",
    "- **Limitaciones**: No considera la relación con la variable objetivo, puede eliminar características útiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:17.825065Z",
     "start_time": "2020-11-19T10:10:17.818065Z"
    }
   },
   "outputs": [],
   "source": "# Importamos VarianceThreshold para filtrar características por varianza\nfrom sklearn.feature_selection import VarianceThreshold\n\n# Aplicamos VarianceThreshold con umbral de 0.7\n# Esto elimina características cuya varianza sea menor a 0.7\nprimer_filtro = VarianceThreshold(.7).fit_transform(x_data_generated)\n\n# Verificamos cuántas características quedaron después del filtro\nprimer_filtro.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:18.837626Z",
     "start_time": "2020-11-19T10:10:18.833589Z"
    }
   },
   "outputs": [],
   "source": "# Probamos con un umbral más alto de 0.8\n# A mayor umbral, se eliminan más características (las de menor varianza)\nprimer_filtro = VarianceThreshold(.8).fit_transform(x_data_generated)\n\n# Ahora tenemos 15 características (se eliminaron 5)\nprimer_filtro.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Probamos con umbral de 0.9\n# Umbral aún más restrictivo\nprimer_filtro = VarianceThreshold(.9).fit_transform(x_data_generated)\n\n# Nos quedamos con 14 características\nprimer_filtro.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:20.212419Z",
     "start_time": "2020-11-19T10:10:20.207432Z"
    }
   },
   "outputs": [],
   "source": "# Probamos con umbral muy alto de 0.99\n# Solo mantenemos características con muy alta varianza\nprimer_filtro = VarianceThreshold(.99).fit_transform(x_data_generated)\n\n# Resultado: solo 11 características cumplen con este criterio estricto\nprimer_filtro.shape"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basado en estadísticos supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectKBest\n",
    "\n",
    "- **¿Qué es?**: Selecciona las K mejores características basadas en algún criterio de puntuación.\n",
    "- **Funcionamiento**: Calcula un puntaje para cada característica en función de algún criterio (como la prueba F para la regresión lineal o la puntuación chi-cuadrado para la clasificación). Luego, selecciona las K características con los puntajes más altos.\n",
    "- **Ventajas**: Permite seleccionar características en función de su relevancia con respecto a la variable objetivo.\n",
    "- **Limitaciones**: Puede ser costoso para conjuntos de datos grandes, elección subjetiva de K.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:22.132032Z",
     "start_time": "2020-11-19T10:10:22.121062Z"
    }
   },
   "outputs": [],
   "source": "# Importamos SelectKBest para selección supervisada de características\n# f_classif: función de puntuación F-ANOVA para problemas de clasificación\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Aplicamos SelectKBest para seleccionar las 5 mejores características\n# Este método es SUPERVISADO: considera la relación con la variable objetivo (y)\n# Por defecto usa f_classif como función de scoring\nx_data_kbest = SelectKBest(k=5).fit_transform(x_data_generated, y_data_generated)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos que ahora tenemos solo 5 características\n# Pasamos de 20 características originales a solo las 5 más relevantes\nx_data_kbest.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creamos un modelo de Regresión Logística para evaluar el rendimiento\n# random_state=42 asegura reproducibilidad de resultados\nlogit = LogisticRegression(random_state=42)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:25.414124Z",
     "start_time": "2020-11-19T10:10:25.386181Z"
    }
   },
   "outputs": [],
   "source": "# Evaluamos el modelo con TODAS las características originales (20)\n# cross_val_score: validación cruzada con 5 folds\n# Esto nos da una medida del rendimiento base\ncoss_val_res = cross_val_score(logit, x_data_generated, y_data_generated, cv=5)\n\n# Calculamos el promedio de accuracy en los 5 folds\n# Resultado: 88% de accuracy con todas las características\nnp.mean(coss_val_res)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:25.923503Z",
     "start_time": "2020-11-19T10:10:25.907511Z"
    }
   },
   "outputs": [],
   "source": "# Evaluamos el modelo con las 5 mejores características seleccionadas por SelectKBest\n# Comparamos el rendimiento con el modelo que usa todas las características\ncoss_val_res = cross_val_score(logit, x_data_kbest, y_data_generated, cv=5)\n\n# Resultado: 86% de accuracy\n# Ligera disminución, pero usamos 75% menos características\nnp.mean(coss_val_res)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:10:26.516993Z",
     "start_time": "2020-11-19T10:10:26.500038Z"
    }
   },
   "outputs": [],
   "source": "# Evaluamos el modelo con características filtradas por VarianceThreshold (umbral 0.9)\n# Esto nos permite comparar el método NO supervisado vs el modelo base\nx_data_varth = VarianceThreshold(.9).fit_transform(x_data_generated)\ncoss_val_res = cross_val_score(logit, x_data_varth, y_data_generated, cv=5)\n\n# Resultado: 89% de accuracy - ¡mejor que el modelo original!\n# VarianceThreshold mantuvo 14 características y obtuvo mejor rendimiento\nnp.mean(coss_val_res)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our selected features have improved the quality of the classifier. Of course, this example is purely artificial; however, it is worth using for real problems."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervisado wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE (Recursive Feature Elimination)\n",
    "\n",
    "- **¿Qué es?**: Elimina recursivamente las características menos importantes hasta alcanzar el número deseado.\n",
    "- **Funcionamiento**: Entrena un modelo (generalmente un modelo de aprendizaje automático) en todas las características y luego elimina las características con menor importancia. Este proceso se repite hasta que se alcanza el número deseado de características.\n",
    "- **Ventajas**: Considera la interacción entre características, puede ser más robusto.\n",
    "- **Limitaciones**: Puede ser costoso computacionalmente, elección del modelo y número de características.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:36:46.595134Z",
     "start_time": "2020-11-19T10:36:36.919545Z"
    }
   },
   "outputs": [],
   "source": "# Importamos las librerías necesarias para RFE (Recursive Feature Elimination)\nfrom sklearn.svm import SVC  # Support Vector Classifier\nfrom sklearn.feature_selection import RFE  # Eliminación recursiva de características\nfrom sklearn.datasets import load_digits  # Dataset de dígitos escritos a mano\nimport matplotlib.pyplot as plt  # Para visualización\n\n# Cargamos el dataset de dígitos (0-9)\n# Es un conjunto de imágenes de 8x8 píxeles\ndigits = load_digits()\n\n# Transformamos las imágenes 2D en vectores 1D\n# reshape(-1) calcula automáticamente el tamaño\nX = digits.images.reshape((len(digits.images)), -1)\n\n# Etiquetas: los dígitos del 0 al 9\ny = digits.target"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verificamos las dimensiones del dataset\n# 1797 muestras (imágenes)\n# 64 características (8x8 = 64 píxeles por imagen)\nX.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T10:42:05.336475Z",
     "start_time": "2020-11-19T10:41:59.409990Z"
    }
   },
   "outputs": [],
   "source": "# Creamos un clasificador SVC (Support Vector Classifier) con kernel lineal\nsvc = SVC(kernel=\"linear\", C=1)\n\n# Configuramos RFE para seleccionar las 10 características más importantes\n# RFE funciona de manera RECURSIVA:\n# 1. Entrena el modelo con todas las características\n# 2. Elimina la característica menos importante (step=1)\n# 3. Repite hasta tener solo 10 características\nrfe = RFE(estimator=svc, n_features_to_select=10, step=1)\n\n# Ajustamos el modelo RFE con nuestros datos\nrfe.fit(X, y)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# rfe.ranking_ nos da el ranking de importancia de cada característica\n# 1 = las más importantes (seleccionadas)\n# Números mayores = menos importantes (eliminadas primero)\n\n# Reformateamos el ranking para tener la forma original de la imagen (8x8)\nranking = rfe.ranking_.reshape(digits.images[0].shape)\n\n# Visualizamos el ranking de características en forma de mapa de calor\n# Los píxeles más oscuros (ranking=1) son los más importantes para clasificar dígitos\n# Los píxeles claros fueron eliminados antes (menos importantes)\nplt.matshow(ranking, cmap='binary')\nplt.colorbar();"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}